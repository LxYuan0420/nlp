{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d3d700-80a4-4e69-a548-ca39cb68ce9e",
   "metadata": {},
   "source": [
    "### Transformer Model\n",
    "\n",
    "This notebook is designed to show the steps involved in fine-tuning transformer models on a multi-label dataset. To keep things simple and to ensure a fair comparison with the scikit-learn model that we've previously trained, both models will be directly fitted on the training set and evaluated on the test set. If you're interested in learning stratified splitting on multi-label datasets or hyperparameter search using transformers, please refer to the next notebook: reuters_hyperparameter_search_using_trainer_api notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab640f9f-e39a-4484-ade3-b865ea413010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97850683-313b-4aac-b673-70c15f04aa93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Remove numbers, newlines, and special characters from text.\"\"\"\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Find Single Appearance Labels\n",
    "def find_single_appearance_labels(y):\n",
    "    \"\"\"Find labels that appear only once in the dataset.\"\"\"\n",
    "    all_labels = list(chain.from_iterable(y))\n",
    "    label_count = Counter(all_labels)\n",
    "    single_appearance_labels = [label for label, count in label_count.items() if count == 1]\n",
    "    return single_appearance_labels\n",
    "\n",
    "# Remove Single Appearance Labels from Dataset\n",
    "def remove_single_appearance_labels(dataset, single_appearance_labels):\n",
    "    \"\"\"Remove samples with single-appearance labels from both train and test sets.\"\"\"\n",
    "    for split in ['train', 'test']:\n",
    "        dataset[split] = dataset[split].filter(lambda x: all(label not in single_appearance_labels for label in x['topics']))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e10cde9-d5fc-4a45-847e-4da1818698c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    \n",
    "    y_true = labels\n",
    "    \n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea2626-5e64-449c-bc8b-f27efcad281c",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Note that we are using `ModApte` split in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6e06bc-254b-4ada-beb9-0ac922dcc924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\"reuters21578\", \"ModApte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007268b-4fe5-4400-b078-c2724a151c9a",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "- Find out single appearance labels and remove them from train and test split\n",
    "- Combine title and text together as `text` column\n",
    "- Transform topics into multihot encoding as `labels` column\n",
    "- Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db6f5f3-6177-45d9-ada9-bd16674f07a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding single appearance labels...\n",
      "Single appearance labels: ['lin-oil', 'rye', 'red-bean', 'groundnut-oil', 'citruspulp', 'rape-meal', 'corn-oil', 'peseta', 'cotton-oil', 'ringgit', 'castorseed', 'castor-oil', 'lit', 'rupiah', 'skr', 'nkr', 'dkr', 'sun-meal', 'lin-meal', 'cruzado']\n",
      "Removing samples with single-appearance labels...\n"
     ]
    }
   ],
   "source": [
    "# Find and Remove Single Appearance Labels\n",
    "print(\"Finding single appearance labels...\")\n",
    "y_train = [item['topics'] for item in dataset['train']]\n",
    "single_appearance_labels = find_single_appearance_labels(y_train)\n",
    "print(f\"Single appearance labels: {single_appearance_labels}\")\n",
    "\n",
    "print(\"Removing samples with single-appearance labels...\")\n",
    "dataset = remove_single_appearance_labels(dataset, single_appearance_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b386af6-017c-4fdf-8ca5-9ed25736fbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine title and text together\n"
     ]
    }
   ],
   "source": [
    "print(\"Combine title and text together\")\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text\": x[\"title\"] + \" \" + x[\"text\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af3b06ad-a4a5-414f-a410-7e033a015eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 95 unique labels:\n",
      "{'veg-oil', 'gold', 'platinum', 'ipi', 'acq', 'carcass', 'wool', 'coconut-oil', 'linseed', 'copper', 'soy-meal', 'jet', 'dlr', 'copra-cake', 'hog', 'rand', 'strategic-metal', 'can', 'tea', 'sorghum', 'livestock', 'barley', 'lumber', 'earn', 'wheat', 'trade', 'soy-oil', 'cocoa', 'inventories', 'income', 'rubber', 'tin', 'iron-steel', 'ship', 'rapeseed', 'wpi', 'sun-oil', 'pet-chem', 'palmkernel', 'nat-gas', 'gnp', 'l-cattle', 'propane', 'rice', 'lead', 'alum', 'instal-debt', 'saudriyal', 'cpu', 'jobs', 'meal-feed', 'oilseed', 'dmk', 'plywood', 'zinc', 'retail', 'dfl', 'cpi', 'crude', 'pork-belly', 'gas', 'money-fx', 'corn', 'tapioca', 'palladium', 'lei', 'cornglutenfeed', 'sunseed', 'potato', 'silver', 'sugar', 'grain', 'groundnut', 'naphtha', 'orange', 'soybean', 'coconut', 'stg', 'cotton', 'yen', 'rape-oil', 'palm-oil', 'oat', 'reserves', 'housing', 'interest', 'coffee', 'fuel', 'austdlr', 'money-supply', 'heat', 'fishmeal', 'bop', 'nickel', 'nzdlr'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9588/9588 [00:00<00:00, 60341.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Check number of unique labels \n",
    "unique_labels = set(chain.from_iterable(dataset['train'][\"topics\"]))\n",
    "print(f\"We have {len(unique_labels)} unique labels:\\n{unique_labels}\")\n",
    "\n",
    "# Transform topics into multi-hot encoding format\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(dataset['train']['topics'])\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"labels\": torch.from_numpy(mlb.transform(x[\"topics\"])).float()}, batched=True)\n",
    "\n",
    "labels = mlb.classes_\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "num_labels = len(id2label)\n",
    "\n",
    "assert num_labels == len(unique_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04250da0-b8a9-4a24-bb73-5ebeb6aa6901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: acq\n",
      "1: alum\n",
      "2: austdlr\n",
      "3: barley\n",
      "4: bop\n",
      "5: can\n",
      "6: carcass\n",
      "7: cocoa\n",
      "8: coconut\n",
      "9: coconut-oil\n"
     ]
    }
   ],
   "source": [
    "# sanity check:\n",
    "for idx, label in id2label.items():\n",
    "    if idx>=10:\n",
    "        break\n",
    "    \n",
    "    print(f\"{idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a97865f-6270-4d86-aa89-c0b9b45135b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9588/9588 [00:02<00:00, 3850.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "# Tokenize and remove unwanted columns\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "columns = dataset[\"train\"].column_names\n",
    "columns.remove(\"text\")\n",
    "columns.remove(\"labels\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbbdebb6-518f-4e44-a821-45c1397c6579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3292\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9588\n",
       "    })\n",
       "    unused: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 722\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638074ca-e405-4b39-81a6-3e945407a889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'labels', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d09099-3c4a-4c7b-8c6e-a3024b27fd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] BAHIA COCOA REVIEW Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review. The dry period means the temporao will be late this year. Arrivals for the week ended February 22 were 155, 221 bags of 60 kilos making a cumulative total for the season of 5. 93 mln against 5. 81 at the same stage last year. Again it seems that cocoa delivered earlier on consignment was included in the arrivals figures. Comissaria Smith said there is still some doubt as to how much old crop cocoa is still available as harvesting has practically come to an end. With total Bahia crop estimates around 6. 4 mln bags and sales standing at almost 6. 2 mln there are a few hundred thousand bags still in the hands of farmers, middlemen, exporters and processors. There are doubts as to how much of this cocoa would be fit for export as shippers are now experiencing dificulties in obtaining + Bahia superior + certificates. In view of the lower quality over recent weeks farmers have sold a good part of their cocoa held on consignment. Comissaria Smith said spot bean prices rose to 340 to 350 cruzados per arroba of 15 kilos. Bean shippers were reluctant to offer nearby shipment and only limited sales were booked for March shipment at 1, 750 to 1, 780 dlrs per tonne to ports to be named. New crop sales were also light and all to open ports with June / July going at 1, 850 and 1, 880 dlrs and at 35 and 45 dlrs under New York july, Aug / Sept at 1, 870, 1, 875 and 1, 880 dlrs per tonne FOB. Routine sales of butter were made. March / April sold at 4, 340, 4, 345 and 4, 350 dlrs. April / May butter went at 2. 27 times New York May, June / July at 4, 400 and 4, 415 dlrs, Aug / Sept at 4, 351 to 4, 450 dlrs and at 2. 27 and 2. 28 times New York Sept and Oct / Dec at 4, 480 d [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a95e1f-9b3d-4be6-be80-d94c1f5c6e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(example['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa77d4b-f64b-4572-b4b2-c9fd065197c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cocoa']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1f33873-4453-4523-9f6e-cb465ad8c58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32db0a68-4877-4b08-b4f1-a499103f72c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-cased\", \n",
    "    num_labels=num_labels, \n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e9ba2f-d40d-44f7-87da-fcb8815fc007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test forward pass\n",
    "tokenized_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab0f811-6b99-47f4-a6d5-cc83a2c61415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 12465,  3048,  9984, 18732, 15678,  1592,   155,  2036, 23314,\n",
       "         2036,  2924,  3237,  1468,  1598,  2032,  1103,  1989,  1107,  1103,\n",
       "        18757, 10652,  1884, 20535,  4834,   117,  1155,  6348, 25148,  1103,\n",
       "        16076,  1290,  1346,  1356,  1105,  9248, 19743,  1111,  1103,  1909,\n",
       "        16655,  1611,  1186,   117,  1780,  2999, 20641,  3001,  1138,  1136,\n",
       "         1151,  5219,   117,  3291, 15394,  9724,  1465,  2159,  1163,  1107,\n",
       "         1157,  5392,  3189,   119,  1109,  3712,  1669,  2086,  1103, 16655,\n",
       "         1611,  1186,  1209,  1129,  1523,  1142,  1214,   119,   138, 14791,\n",
       "         7501,  1116,  1111,  1103,  1989,  2207,  1428,  1659,  1127, 14691,\n",
       "          117, 21319,  8483,  1104,  2539,   180, 24755,  1116,  1543,   170,\n",
       "        27574,  1703,  1111,  1103,  1265,  1104,   126,   119,  5429,   182,\n",
       "        21615,  1222,   126,   119,  5615,  1120,  1103,  1269,  2016,  1314,\n",
       "         1214,   119,  5630,  1122,  3093,  1115,  1884, 20535,  4653,  2206,\n",
       "         1113, 14255, 19638,  1880,  1108,  1529,  1107,  1103,  4870,  1116,\n",
       "         3736,   119,  3291, 15394,  9724,  1465,  2159,  1163,  1175,  1110,\n",
       "         1253,  1199,  4095,  1112,  1106,  1293,  1277,  1385, 10809,  1884,\n",
       "        20535,  1110,  1253,  1907,  1112, 25039,  1144,  7892,  1435,  1106,\n",
       "         1126,  1322,   119,  1556,  1703, 18757, 10652, 10809, 10777,  1213,\n",
       "          127,   119,   125,   182, 21615,  8483,  1105,  3813,  2288,  1120,\n",
       "         1593,   127,   119,   123,   182, 21615,  1175,  1132,   170,  1374,\n",
       "         2937,  4032,  8483,  1253,  1107,  1103,  1493,  1104,  6915,   117,\n",
       "         2243,  2354,   117, 10107,  1468,  1105, 20026,   119,  1247,  1132,\n",
       "        14351,  1112,  1106,  1293,  1277,  1104,  1142,  1884, 20535,  1156,\n",
       "         1129,  4218,  1111, 10107,  1112,  2062,  6206,  1132,  1208, 13992,\n",
       "         4267, 21361,  7067,  1905,  1107, 11621,   116, 18757, 10652,  7298,\n",
       "          116, 21487,   119,  1130,  2458,  1104,  1103,  2211,  3068,  1166,\n",
       "         2793,  2277,  6915,  1138,  1962,   170,  1363,  1226,  1104,  1147,\n",
       "         1884, 20535,  1316,  1113, 14255, 19638,  1880,   119,  3291, 15394,\n",
       "         9724,  1465,  2159,  1163,  3205, 27528,  7352,  3152,  1106, 16984,\n",
       "         1106,  8301,   172,  5082, 24981,  2155,  1679,   170, 13656,  2822,\n",
       "         1104,  1405,   180, 24755,  1116,   119, 21561,  2062,  6206,  1127,\n",
       "        12061,  1106,  2906,  2721, 25464,  1105,  1178,  2609,  3813,  1127,\n",
       "        18951,  1111,  1345, 25464,  1120,   122,   117,  9416,  1106,   122,\n",
       "          117,  5603,  1568,   173,  1233,  1733,  1679, 11371,  1673,  1106,\n",
       "         9267,  1106,  1129,  1417,   119,  1203, 10809,  3813,  1127,  1145,\n",
       "         1609,  1105,  1155,  1106,  1501,  9267,  1114,  1340,   120,  1351,\n",
       "         1280,  1120,   122,   117, 16577,  1105,   122,   117,  5385,  1568,\n",
       "          173,  1233,  1733,  1105,  1120,  2588,  1105,  2532,   173,  1233,\n",
       "         1733,  1223,  1203,  1365,   179,  4654,  1183,   117, 16892,   120,\n",
       "        20456,  1120,   122,   117,  5966,  1568,   117,   122,   117,  5966,\n",
       "         1571,  1105,   122,   117,  5385,  1568,   173,  1233,  1733,  1679,\n",
       "        11371,  1673,   143,  2346,  2064,   119,   155,  3554,  2042,  3813,\n",
       "         1104, 13742,  1127,  1189,   119,  1345,   120,  1364,  1962,  1120,\n",
       "          125,   117, 16984,   117,   125,   117, 26625,  1105,   125,   117,\n",
       "         8301,   173,  1233,  1733,   119,  1364,   120,  1318, 13742,  1355,\n",
       "         1120,   123,   119,  1765,  1551,  1203,  1365,  1318,   117,  1340,\n",
       "          120,  1351,  1120,   125,   117,  3434,  1105,   125,   117, 27813,\n",
       "          173,  1233,  1733,   117, 16892,   120, 20456,  1120,   125,   117,\n",
       "         2588,  1475,  1106,   125,   117, 10181,   173,  1233,  1733,  1105,\n",
       "         1120,   123,   119,  1765,  1105,   123,   119,  1743,  1551,  1203,\n",
       "         1365, 20456,  1105, 14125,   120, 13063,  1120,   125,   117, 18478,\n",
       "          173,   102])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "585352b6-7e8d-45df-a950-efe6e725593e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6990, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.0081, -0.0012, -0.1407, -0.0754,  0.0880, -0.0083, -0.1369,  0.0325,\n",
       "          0.1409, -0.0466, -0.1064, -0.0094,  0.0234,  0.0217, -0.2389,  0.0915,\n",
       "          0.0148,  0.0769, -0.0678, -0.0427, -0.0168,  0.0680,  0.0972,  0.0718,\n",
       "          0.1725,  0.0523,  0.1037, -0.0177,  0.0143,  0.0510, -0.0428, -0.1005,\n",
       "         -0.1053, -0.0996, -0.0407,  0.0608, -0.1763,  0.0490,  0.1373,  0.1419,\n",
       "          0.0867, -0.0576, -0.1596,  0.0084, -0.0396, -0.1620,  0.0884,  0.0631,\n",
       "         -0.0276,  0.0112, -0.1778, -0.0582, -0.0645, -0.0952, -0.0962, -0.2435,\n",
       "          0.1074,  0.0811, -0.1186,  0.0730,  0.1431,  0.1783, -0.0022, -0.0878,\n",
       "          0.0458, -0.1022,  0.0841,  0.0173,  0.1825,  0.1341, -0.1104,  0.0081,\n",
       "         -0.0668,  0.0023,  0.3571,  0.0953, -0.0483,  0.1300,  0.0370,  0.0782,\n",
       "         -0.0569,  0.1126, -0.1065,  0.2615,  0.0463,  0.0785, -0.0053,  0.0632,\n",
       "          0.0497,  0.1027, -0.1393,  0.0934,  0.0801, -0.0427,  0.0532]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=tokenized_dataset['train']['input_ids'][0].unsqueeze(0), labels=tokenized_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae188986-ba6e-4f66-b8ab-69949d8c4f86",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18d525c6-5265-416f-a302-9cdf20880e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"distilbert-finetuned-reuters21578-multilabel\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    push_to_hub=True,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3afa72a4-758c-4e0e-a05b-352451cbc077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4ade980-e443-410e-be89-a1501ba62ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 1:04:26, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>0.043942</td>\n",
       "      <td>0.389627</td>\n",
       "      <td>0.621005</td>\n",
       "      <td>0.356622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.028706</td>\n",
       "      <td>0.628855</td>\n",
       "      <td>0.731796</td>\n",
       "      <td>0.595383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.021945</td>\n",
       "      <td>0.672073</td>\n",
       "      <td>0.757869</td>\n",
       "      <td>0.608445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.017743</td>\n",
       "      <td>0.750487</td>\n",
       "      <td>0.812821</td>\n",
       "      <td>0.690765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.790499</td>\n",
       "      <td>0.837643</td>\n",
       "      <td>0.727825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.013540</td>\n",
       "      <td>0.813237</td>\n",
       "      <td>0.858942</td>\n",
       "      <td>0.755468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.829122</td>\n",
       "      <td>0.872717</td>\n",
       "      <td>0.772479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>0.833483</td>\n",
       "      <td>0.875701</td>\n",
       "      <td>0.782199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011853</td>\n",
       "      <td>0.839233</td>\n",
       "      <td>0.884694</td>\n",
       "      <td>0.788275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.833851</td>\n",
       "      <td>0.881030</td>\n",
       "      <td>0.782807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.853803</td>\n",
       "      <td>0.899931</td>\n",
       "      <td>0.804678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.011255</td>\n",
       "      <td>0.852535</td>\n",
       "      <td>0.896718</td>\n",
       "      <td>0.804374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>0.852018</td>\n",
       "      <td>0.898176</td>\n",
       "      <td>0.802855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>0.896238</td>\n",
       "      <td>0.810450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.010955</td>\n",
       "      <td>0.860952</td>\n",
       "      <td>0.906019</td>\n",
       "      <td>0.816525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.858297</td>\n",
       "      <td>0.902121</td>\n",
       "      <td>0.813791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.011047</td>\n",
       "      <td>0.862029</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>0.819563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.010995</td>\n",
       "      <td>0.862886</td>\n",
       "      <td>0.906310</td>\n",
       "      <td>0.819563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.862351</td>\n",
       "      <td>0.906170</td>\n",
       "      <td>0.818044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.862649</td>\n",
       "      <td>0.905508</td>\n",
       "      <td>0.817740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.017809471408526102, metrics={'train_runtime': 3867.6489, 'train_samples_per_second': 49.581, 'train_steps_per_second': 1.551, 'total_flos': 2.54440780812288e+16, 'train_loss': 0.017809471408526102, 'epoch': 20.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ee98566-a8e6-46fd-8dce-e235f2623629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.010994679294526577,\n",
       " 'eval_f1': 0.8628858578607322,\n",
       " 'eval_roc_auc': 0.906310303884654,\n",
       " 'eval_accuracy': 0.8195625759416768,\n",
       " 'eval_runtime': 26.409,\n",
       " 'eval_samples_per_second': 124.654,\n",
       " 'eval_steps_per_second': 3.9,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43311b-2282-44d0-bef4-63eb66b51712",
   "metadata": {},
   "source": [
    "### Push Model to HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b983558c-b29e-4742-afc6-7d4f65b2835a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/lxyuan/distilbert-finetuned-reuters21578-multilabel/tree/main/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82b4f1eb-d11b-41dc-91b9-ebb05de8fc23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lxyuan/distilbert-finetuned-reuters21578-multilabel/commit/d1f274a43ed66a57b8317f8b785b064425b2414b', commit_message='Upload tokenizer', commit_description='', oid='d1f274a43ed66a57b8317f8b785b064425b2414b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"distilbert-finetuned-reuters21578-multilabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c06927-67c7-48d0-9e38-e8e3f875ba88",
   "metadata": {},
   "source": [
    "### Load pushed model from HF hub for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6247f86f-0d8d-47e4-b980-312f2bc6c01c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 4.30k/4.30k [00:00<00:00, 1.85MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 263M/263M [00:16<00:00, 16.3MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 321/321 [00:00<00:00, 152kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 1.99MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 669k/669k [00:00<00:00, 8.61MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 55.9kB/s]\n",
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"lxyuan/distilbert-finetuned-reuters21578-multilabel\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8118dbc6-8f85-423f-b0d8-4abb34c78878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWARDS The Ministry of International Trade and\\nIndustry (MITI) will revise its long-term energy supply/demand\\noutlook by August to meet a forecast downtrend in Japanese\\nenergy demand, ministry officials said.\\n    MITI is expected to lower the projection for primary energy\\nsupplies in the year 2000 to 550 mln kilolitres (kl) from 600\\nmln, they said.\\n    The decision follows the emergence of structural changes in\\nJapanese industry following the rise in the value of the yen\\nand a decline in domestic electric power demand.\\n    MITI is planning to work out a revised energy supply/demand\\noutlook through deliberations of committee meetings of the\\nAgency of Natural Resources and Energy, the officials said.\\n    They said MITI will also review the breakdown of energy\\nsupply sources, including oil, nuclear, coal and natural gas.\\n    Nuclear energy provided the bulk of Japan's electric power\\nin the fiscal year ended March 31, supplying an estimated 27\\npct on a kilowatt/hour basis, followed by oil (23 pct) and\\nliquefied natural gas (21 pct), they noted.\\n REUTER\\n\",\n",
       " ['crude', 'nat-gas'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"test\"][\"text\"][2]\n",
    "target_topics = dataset[\"test\"][\"topics\"][2]\n",
    "\n",
    "example, target_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8eefeb58-0df4-4371-8f73-69bfe3e164db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_kwargs={\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512}\n",
    "\n",
    "output = pipe(example, function_to_apply=\"sigmoid\", **fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab7696f9-c793-4f06-9858-b5c23caa5ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crude 0.7355073690414429\n",
      "nat-gas 0.8600426316261292\n"
     ]
    }
   ],
   "source": [
    "for item in output[0]:\n",
    "    if item[\"score\"]>=0.5:\n",
    "        print(item[\"label\"], item[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "124d13ed-c080-4068-ad11-10761965abcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'acq', 'score': 0.0020191946532577276},\n",
       "  {'label': 'alum', 'score': 0.006228976417332888},\n",
       "  {'label': 'austdlr', 'score': 0.0015945922350510955},\n",
       "  {'label': 'barley', 'score': 0.0026575943920761347},\n",
       "  {'label': 'bop', 'score': 0.04313690587878227},\n",
       "  {'label': 'can', 'score': 0.0008254407439380884},\n",
       "  {'label': 'carcass', 'score': 0.0011239182204008102},\n",
       "  {'label': 'cocoa', 'score': 0.001983838388696313},\n",
       "  {'label': 'coconut', 'score': 0.0005582965677604079},\n",
       "  {'label': 'coconut-oil', 'score': 0.0015241571236401796},\n",
       "  {'label': 'coffee', 'score': 0.0027940908912569284},\n",
       "  {'label': 'copper', 'score': 0.015326935797929764},\n",
       "  {'label': 'copra-cake', 'score': 0.0008283186471089721},\n",
       "  {'label': 'corn', 'score': 0.0037029897794127464},\n",
       "  {'label': 'cornglutenfeed', 'score': 0.0005428714212030172},\n",
       "  {'label': 'cotton', 'score': 0.002034077188000083},\n",
       "  {'label': 'cpi', 'score': 0.06535973399877548},\n",
       "  {'label': 'cpu', 'score': 0.0021881815046072006},\n",
       "  {'label': 'crude', 'score': 0.7355073690414429},\n",
       "  {'label': 'dfl', 'score': 0.0013635228388011456},\n",
       "  {'label': 'dlr', 'score': 0.0006174662848934531},\n",
       "  {'label': 'dmk', 'score': 0.0008339431951753795},\n",
       "  {'label': 'earn', 'score': 0.011476946994662285},\n",
       "  {'label': 'fishmeal', 'score': 0.0022318316623568535},\n",
       "  {'label': 'fuel', 'score': 0.033368177711963654},\n",
       "  {'label': 'gas', 'score': 0.04613899812102318},\n",
       "  {'label': 'gnp', 'score': 0.016808224841952324},\n",
       "  {'label': 'gold', 'score': 0.002687695436179638},\n",
       "  {'label': 'grain', 'score': 0.013697398826479912},\n",
       "  {'label': 'groundnut', 'score': 0.0015494064427912235},\n",
       "  {'label': 'heat', 'score': 0.02759167179465294},\n",
       "  {'label': 'hog', 'score': 0.0019160741940140724},\n",
       "  {'label': 'housing', 'score': 0.00441614119336009},\n",
       "  {'label': 'income', 'score': 0.004025637172162533},\n",
       "  {'label': 'instal-debt', 'score': 0.0016264691948890686},\n",
       "  {'label': 'interest', 'score': 0.013458047062158585},\n",
       "  {'label': 'inventories', 'score': 0.0035903449170291424},\n",
       "  {'label': 'ipi', 'score': 0.024754367768764496},\n",
       "  {'label': 'iron-steel', 'score': 0.058403585106134415},\n",
       "  {'label': 'jet', 'score': 0.006504293996840715},\n",
       "  {'label': 'jobs', 'score': 0.0058925398625433445},\n",
       "  {'label': 'l-cattle', 'score': 0.001757090794853866},\n",
       "  {'label': 'lead', 'score': 0.0032556664664298296},\n",
       "  {'label': 'lei', 'score': 0.00707882409915328},\n",
       "  {'label': 'linseed', 'score': 0.000720182666555047},\n",
       "  {'label': 'livestock', 'score': 0.0014596197288483381},\n",
       "  {'label': 'lumber', 'score': 0.0036452319473028183},\n",
       "  {'label': 'meal-feed', 'score': 0.002192002022638917},\n",
       "  {'label': 'money-fx', 'score': 0.001551983063109219},\n",
       "  {'label': 'money-supply', 'score': 0.001651902450248599},\n",
       "  {'label': 'naphtha', 'score': 0.0014346915995702147},\n",
       "  {'label': 'nat-gas', 'score': 0.8600426316261292},\n",
       "  {'label': 'nickel', 'score': 0.005618440452963114},\n",
       "  {'label': 'nzdlr', 'score': 0.0006484535988420248},\n",
       "  {'label': 'oat', 'score': 0.0013083372032269835},\n",
       "  {'label': 'oilseed', 'score': 0.002226397395133972},\n",
       "  {'label': 'orange', 'score': 0.0033350789453834295},\n",
       "  {'label': 'palladium', 'score': 0.001478377147577703},\n",
       "  {'label': 'palm-oil', 'score': 0.004945281893014908},\n",
       "  {'label': 'palmkernel', 'score': 0.0012238806812092662},\n",
       "  {'label': 'pet-chem', 'score': 0.035592615604400635},\n",
       "  {'label': 'platinum', 'score': 0.0017588965129107237},\n",
       "  {'label': 'plywood', 'score': 0.001716948812827468},\n",
       "  {'label': 'pork-belly', 'score': 0.0013114786706864834},\n",
       "  {'label': 'potato', 'score': 0.0010726690525189042},\n",
       "  {'label': 'propane', 'score': 0.0060063996352255344},\n",
       "  {'label': 'rand', 'score': 0.0019263230497017503},\n",
       "  {'label': 'rape-oil', 'score': 0.0016102393856272101},\n",
       "  {'label': 'rapeseed', 'score': 0.002418149495497346},\n",
       "  {'label': 'reserves', 'score': 0.017363285645842552},\n",
       "  {'label': 'retail', 'score': 0.00311975902877748},\n",
       "  {'label': 'rice', 'score': 0.00245090969838202},\n",
       "  {'label': 'rubber', 'score': 0.005399945192039013},\n",
       "  {'label': 'saudriyal', 'score': 0.001152348704636097},\n",
       "  {'label': 'ship', 'score': 0.017765341326594353},\n",
       "  {'label': 'silver', 'score': 0.003991504665464163},\n",
       "  {'label': 'sorghum', 'score': 0.003409914206713438},\n",
       "  {'label': 'soy-meal', 'score': 0.0027011337224394083},\n",
       "  {'label': 'soy-oil', 'score': 0.006485546939074993},\n",
       "  {'label': 'soybean', 'score': 0.003187927883118391},\n",
       "  {'label': 'stg', 'score': 0.0020826603285968304},\n",
       "  {'label': 'strategic-metal', 'score': 0.010356053709983826},\n",
       "  {'label': 'sugar', 'score': 0.0015613195719197392},\n",
       "  {'label': 'sun-oil', 'score': 0.0017360737547278404},\n",
       "  {'label': 'sunseed', 'score': 0.0019364079926162958},\n",
       "  {'label': 'tapioca', 'score': 0.000769609643612057},\n",
       "  {'label': 'tea', 'score': 0.0034493855200707912},\n",
       "  {'label': 'tin', 'score': 0.003201316110789776},\n",
       "  {'label': 'trade', 'score': 0.007313435897231102},\n",
       "  {'label': 'veg-oil', 'score': 0.008714810945093632},\n",
       "  {'label': 'wheat', 'score': 0.0029099180828779936},\n",
       "  {'label': 'wool', 'score': 0.0011728241806849837},\n",
       "  {'label': 'wpi', 'score': 0.061525627970695496},\n",
       "  {'label': 'yen', 'score': 0.0013777504209429026},\n",
       "  {'label': 'zinc', 'score': 0.004084727726876736}]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a8128-1a3e-42e6-8b08-fddd03d39af3",
   "metadata": {},
   "source": [
    "### Generate classification report using pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e509145-e3b8-451a-a551-a876a6a75d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = dataset[\"test\"][\"text\"]\n",
    "y_test = tokenized_dataset[\"test\"][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48009592-ede6-44ed-adda-e7f0ae5aa4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"lxyuan/distilbert-finetuned-reuters21578-multilabel\", return_all_scores=True, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56ddb75a-dc98-480f-a9da-5cec58b3a491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = pipe(X_test, function_to_apply=\"sigmoid\", **fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "875576f2-1991-44e7-bd36-e8866b4cbfce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract scores using list comprehension\n",
    "scores = [[prediction[\"score\"] for prediction in sample] for sample in y_pred]\n",
    "\n",
    "# Convert list of scores to a tensor\n",
    "y_pred_float = torch.tensor(scores, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46ac39dd-30e7-435a-b336-5eed757567a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3292, 95])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_float.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b0f1f7b-410f-43d3-8dba-7a2dafe0dec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "            acq       0.97      0.93      0.95       719\n",
      "           alum       1.00      0.70      0.82        23\n",
      "        austdlr       0.00      0.00      0.00         0\n",
      "         barley       1.00      0.50      0.67        12\n",
      "            bop       0.79      0.50      0.61        30\n",
      "            can       0.00      0.00      0.00         0\n",
      "        carcass       0.67      0.67      0.67        18\n",
      "          cocoa       1.00      1.00      1.00        18\n",
      "        coconut       0.00      0.00      0.00         2\n",
      "    coconut-oil       0.00      0.00      0.00         2\n",
      "         coffee       0.86      0.89      0.87        27\n",
      "         copper       1.00      0.78      0.88        18\n",
      "     copra-cake       0.00      0.00      0.00         1\n",
      "           corn       0.84      0.87      0.86        55\n",
      " cornglutenfeed       0.00      0.00      0.00         0\n",
      "         cotton       0.92      0.67      0.77        18\n",
      "            cpi       0.86      0.43      0.57        28\n",
      "            cpu       0.00      0.00      0.00         1\n",
      "          crude       0.87      0.93      0.90       189\n",
      "            dfl       0.00      0.00      0.00         1\n",
      "            dlr       0.72      0.64      0.67        44\n",
      "            dmk       0.00      0.00      0.00         4\n",
      "           earn       0.98      0.99      0.98      1087\n",
      "       fishmeal       0.00      0.00      0.00         0\n",
      "           fuel       0.00      0.00      0.00        10\n",
      "            gas       0.80      0.71      0.75        17\n",
      "            gnp       0.79      0.66      0.72        35\n",
      "           gold       0.95      0.67      0.78        30\n",
      "          grain       0.94      0.92      0.93       146\n",
      "      groundnut       0.00      0.00      0.00         4\n",
      "           heat       0.00      0.00      0.00         5\n",
      "            hog       1.00      0.33      0.50         6\n",
      "        housing       0.00      0.00      0.00         4\n",
      "         income       0.00      0.00      0.00         7\n",
      "    instal-debt       0.00      0.00      0.00         1\n",
      "       interest       0.89      0.67      0.77       131\n",
      "    inventories       0.00      0.00      0.00         0\n",
      "            ipi       1.00      0.58      0.74        12\n",
      "     iron-steel       0.90      0.64      0.75        14\n",
      "            jet       0.00      0.00      0.00         1\n",
      "           jobs       0.92      0.57      0.71        21\n",
      "       l-cattle       0.00      0.00      0.00         2\n",
      "           lead       0.00      0.00      0.00        14\n",
      "            lei       0.00      0.00      0.00         3\n",
      "        linseed       0.00      0.00      0.00         0\n",
      "      livestock       0.63      0.79      0.70        24\n",
      "         lumber       0.00      0.00      0.00         6\n",
      "      meal-feed       0.00      0.00      0.00        17\n",
      "       money-fx       0.78      0.81      0.80       177\n",
      "   money-supply       0.80      0.71      0.75        34\n",
      "        naphtha       0.00      0.00      0.00         4\n",
      "        nat-gas       0.82      0.60      0.69        30\n",
      "         nickel       0.00      0.00      0.00         1\n",
      "          nzdlr       0.00      0.00      0.00         2\n",
      "            oat       0.00      0.00      0.00         4\n",
      "        oilseed       0.64      0.61      0.63        44\n",
      "         orange       1.00      0.36      0.53        11\n",
      "      palladium       0.00      0.00      0.00         1\n",
      "       palm-oil       1.00      0.56      0.71         9\n",
      "     palmkernel       0.00      0.00      0.00         1\n",
      "       pet-chem       0.00      0.00      0.00        12\n",
      "       platinum       0.00      0.00      0.00         7\n",
      "        plywood       0.00      0.00      0.00         0\n",
      "     pork-belly       0.00      0.00      0.00         0\n",
      "         potato       0.00      0.00      0.00         3\n",
      "        propane       0.00      0.00      0.00         3\n",
      "           rand       0.00      0.00      0.00         1\n",
      "       rape-oil       0.00      0.00      0.00         1\n",
      "       rapeseed       0.00      0.00      0.00         8\n",
      "       reserves       0.83      0.56      0.67        18\n",
      "         retail       0.00      0.00      0.00         2\n",
      "           rice       1.00      0.57      0.72        23\n",
      "         rubber       0.82      0.75      0.78        12\n",
      "      saudriyal       0.00      0.00      0.00         0\n",
      "           ship       0.95      0.81      0.87        89\n",
      "         silver       1.00      0.12      0.22         8\n",
      "        sorghum       1.00      0.12      0.22         8\n",
      "       soy-meal       0.00      0.00      0.00        12\n",
      "        soy-oil       0.00      0.00      0.00         8\n",
      "        soybean       0.72      0.56      0.63        32\n",
      "            stg       0.00      0.00      0.00         0\n",
      "strategic-metal       0.00      0.00      0.00        11\n",
      "          sugar       1.00      0.80      0.89        35\n",
      "        sun-oil       0.00      0.00      0.00         0\n",
      "        sunseed       0.00      0.00      0.00         5\n",
      "        tapioca       0.00      0.00      0.00         0\n",
      "            tea       0.00      0.00      0.00         3\n",
      "            tin       1.00      0.42      0.59        12\n",
      "          trade       0.78      0.79      0.79       116\n",
      "        veg-oil       0.91      0.59      0.71        34\n",
      "          wheat       0.83      0.83      0.83        69\n",
      "           wool       0.00      0.00      0.00         0\n",
      "            wpi       0.00      0.00      0.00        10\n",
      "            yen       0.57      0.29      0.38        14\n",
      "           zinc       1.00      0.69      0.82        13\n",
      "\n",
      "      micro avg       0.92      0.81      0.86      3694\n",
      "      macro avg       0.41      0.30      0.33      3694\n",
      "   weighted avg       0.87      0.81      0.84      3694\n",
      "    samples avg       0.81      0.80      0.80      3694\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold=0.5\n",
    "report = classification_report(y_test, torch.ge(y_pred_float, threshold), target_names=labels)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2668b7e-02d0-4ff2-9aa1-345c23997f73",
   "metadata": {},
   "source": [
    "---\n",
    "Insight:\n",
    "\n",
    "\n",
    "**Precision vs Recall Trade-off**\n",
    "\n",
    "The model exhibits a high level of precision for many topics, such as acq (0.97), alum (1.00), barley (1.00), and cocoa (1.00). However, recall for some of these same categories tends to be lower. For example, alum has a recall of 0.70, and barley a recall of 0.50. This can be likened to the previous baseline model, where high precision was a design objective to avoid false positives, which are considered more problematic than false negatives in client-facing applications. While the high precision scores ensure fewer false positives, the cost is lower recall, meaning the model may miss some articles that should have been classified under certain labels.\n",
    "\n",
    "**Discrepancy in Micro and Macro Averages**\n",
    "\n",
    "The model shows a micro-averaged F1-score of 0.86 and a macro-averaged F1-score of 0.33. The large discrepancy indicates similar behavior to the baseline model: the model performs exceptionally well on common labels but struggles with minority classes. The gap is even more pronounced in this model compared to the baseline, which could be a significant issue if the application requires comprehensive coverage across a variety of topics.\n",
    "\n",
    "**Labels with Zero Support**\n",
    "\n",
    "In this classification report, there are several labels like austdlr, can, cornglutenfeed, and others with zero support. These are classes that didn't appear in the test set. This issue also occurred in the baseline model. The zero-support issue indicates a lack of diversity in the test data for these labels and could mean the model is untested for these minority classes, a point of concern for real-world deployment where these classes may appear.\n",
    "\n",
    "**Topic-Specific Observations**\n",
    "\n",
    "For some labels, there's a good balance between precision and recall (acq, coffee, corn). However, for others, the model seems to struggle. For example, gnp and interest have decent precision (0.79 and 0.89, respectively) but lower recall (0.66 and 0.67, respectively). This suggests that while the model is confident in its predictions for these labels, it is missing out on several true positives, a behavior similar to the baseline model.\n",
    "Conclusion\n",
    "\n",
    "In summary, while this model seems to have higher precision values than the baseline for several labels, it faces similar challenges: low recall, poor performance on minority classes, and untested categories due to zero support. These are critical points to consider for future model iterations, especially if comprehensive and balanced performance across all topics is a key objective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
