{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConllPP NER Classification using DistilBERT HF Pytorch Script.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O21tI9CjAXIc"
      },
      "source": [
        "### 0. Install and load library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIcCT1VkqhlO"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_HQ2yqlCDA"
      },
      "source": [
        "### 1. Fine-tuning DistillBERT model on CONLLPP NER dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB_DGmdeTFDE"
      },
      "source": [
        "\"\"\"\n",
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"conllpp\")\n",
        "dataset['train'].features['ner_tags']\n",
        "\n",
        ">> Sequence(feature=ClassLabel(\n",
        "    num_classes=9, \n",
        "    names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], \n",
        "    names_file=None, id=None),\n",
        "    length=-1, \n",
        "    id=None)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cASTBYFpVRNw"
      },
      "source": [
        "%cd \"/content/transformers/examples/pytorch/token-classification\"\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okxmBTUYVJ-3",
        "outputId": "26c15379-0fce-48d8-bd49-ee8a98688eb2"
      },
      "source": [
        "!python run_ner.py --help"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 06:27:13.398862: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "usage: run_ner.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                  [--config_name CONFIG_NAME]\n",
            "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                  [--model_revision MODEL_REVISION]\n",
            "                  [--use_auth_token [USE_AUTH_TOKEN]] [--task_name TASK_NAME]\n",
            "                  [--dataset_name DATASET_NAME]\n",
            "                  [--dataset_config_name DATASET_CONFIG_NAME]\n",
            "                  [--train_file TRAIN_FILE]\n",
            "                  [--validation_file VALIDATION_FILE] [--test_file TEST_FILE]\n",
            "                  [--text_column_name TEXT_COLUMN_NAME]\n",
            "                  [--label_column_name LABEL_COLUMN_NAME]\n",
            "                  [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
            "                  [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
            "                  [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "                  [--max_eval_samples MAX_EVAL_SAMPLES]\n",
            "                  [--max_predict_samples MAX_PREDICT_SAMPLES]\n",
            "                  [--label_all_tokens [LABEL_ALL_TOKENS]]\n",
            "                  [--return_entity_level_metrics [RETURN_ENTITY_LEVEL_METRICS]]\n",
            "                  --output_dir OUTPUT_DIR\n",
            "                  [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                  [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
            "                  [--do_predict [DO_PREDICT]]\n",
            "                  [--evaluation_strategy {no,steps,epoch}]\n",
            "                  [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                  [--learning_rate LEARNING_RATE]\n",
            "                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                  [--max_grad_norm MAX_GRAD_NORM]\n",
            "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                  [--max_steps MAX_STEPS]\n",
            "                  [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                  [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
            "                  [--log_level {debug,info,warning,error,critical,passive}]\n",
            "                  [--log_level_replica {debug,info,warning,error,critical,passive}]\n",
            "                  [--no_log_on_each_node]\n",
            "                  [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
            "                  [--logging_dir LOGGING_DIR]\n",
            "                  [--logging_strategy {no,steps,epoch}]\n",
            "                  [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                  [--logging_steps LOGGING_STEPS]\n",
            "                  [--save_strategy {no,steps,epoch}] [--save_steps SAVE_STEPS]\n",
            "                  [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "                  [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
            "                  [--no_cuda [NO_CUDA]] [--seed SEED] [--fp16 [FP16]]\n",
            "                  [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                  [--fp16_backend {auto,amp,apex}]\n",
            "                  [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "                  [--local_rank LOCAL_RANK] [--tpu_num_cores TPU_NUM_CORES]\n",
            "                  [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]\n",
            "                  [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                  [--eval_steps EVAL_STEPS]\n",
            "                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                  [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                  [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
            "                  [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                  [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                  [--greater_is_better GREATER_IS_BETTER]\n",
            "                  [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                  [--sharded_ddp SHARDED_DDP] [--deepspeed DEEPSPEED]\n",
            "                  [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                  [--adafactor [ADAFACTOR]]\n",
            "                  [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                  [--length_column_name LENGTH_COLUMN_NAME]\n",
            "                  [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                  [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                  [--no_dataloader_pin_memory]\n",
            "                  [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                  [--no_skip_memory_metrics]\n",
            "                  [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "                  [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
            "                  [--push_to_hub [PUSH_TO_HUB]]\n",
            "                  [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "                  [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
            "                  [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
            "                  [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
            "                  [--mp_parameters MP_PARAMETERS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        Path to pretrained model or model identifier from\n",
            "                        huggingface.co/models\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from huggingface.co\n",
            "  --model_revision MODEL_REVISION\n",
            "                        The specific model version to use (can be a branch\n",
            "                        name, tag name or commit id).\n",
            "  --use_auth_token [USE_AUTH_TOKEN]\n",
            "                        Will use the token generated when running\n",
            "                        `transformers-cli login` (necessary to use this script\n",
            "                        with private models).\n",
            "  --task_name TASK_NAME\n",
            "                        The name of the task (ner, pos...).\n",
            "  --dataset_name DATASET_NAME\n",
            "                        The name of the dataset to use (via the datasets\n",
            "                        library).\n",
            "  --dataset_config_name DATASET_CONFIG_NAME\n",
            "                        The configuration name of the dataset to use (via the\n",
            "                        datasets library).\n",
            "  --train_file TRAIN_FILE\n",
            "                        The input training data file (a csv or JSON file).\n",
            "  --validation_file VALIDATION_FILE\n",
            "                        An optional input evaluation data file to evaluate on\n",
            "                        (a csv or JSON file).\n",
            "  --test_file TEST_FILE\n",
            "                        An optional input test data file to predict on (a csv\n",
            "                        or JSON file).\n",
            "  --text_column_name TEXT_COLUMN_NAME\n",
            "                        The column name of text to input in the file (a csv or\n",
            "                        JSON file).\n",
            "  --label_column_name LABEL_COLUMN_NAME\n",
            "                        The column name of label to input in the file (a csv\n",
            "                        or JSON file).\n",
            "  --overwrite_cache [OVERWRITE_CACHE]\n",
            "                        Overwrite the cached training and evaluation sets\n",
            "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\n",
            "                        The number of processes to use for the preprocessing.\n",
            "  --pad_to_max_length [PAD_TO_MAX_LENGTH]\n",
            "                        Whether to pad all samples to model maximum sentence\n",
            "                        length. If False, will pad the samples dynamically\n",
            "                        when batching to the maximum length in the batch. More\n",
            "                        efficient on GPU but very bad for TPU.\n",
            "  --max_train_samples MAX_TRAIN_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of training examples to this value if set.\n",
            "  --max_eval_samples MAX_EVAL_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of evaluation examples to this value if\n",
            "                        set.\n",
            "  --max_predict_samples MAX_PREDICT_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of prediction examples to this value if\n",
            "                        set.\n",
            "  --label_all_tokens [LABEL_ALL_TOKENS]\n",
            "                        Whether to put the label for one word on all tokens of\n",
            "                        generated by that word or just on the one (in which\n",
            "                        case the other tokens will have a padding index).\n",
            "  --return_entity_level_metrics [RETURN_ENTITY_LEVEL_METRICS]\n",
            "                        Whether to return all the entity levels during\n",
            "                        evaluation or just the overall ones.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "                        Overwrite the content of the output directory.Use this\n",
            "                        to continue training if output_dir points to a\n",
            "                        checkpoint directory.\n",
            "  --do_train [DO_TRAIN]\n",
            "                        Whether to run training.\n",
            "  --do_eval [DO_EVAL]   Whether to run eval on the dev set.\n",
            "  --do_predict [DO_PREDICT]\n",
            "                        Whether to run predictions on the test set.\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "                        The evaluation strategy to use.\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss.\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for AdamW.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay for AdamW if we apply some.\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for AdamW optimizer\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for AdamW optimizer\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for AdamW optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
            "                        The scheduler type to use.\n",
            "  --warmup_ratio WARMUP_RATIO\n",
            "                        Linear warmup over warmup_ratio fraction of total\n",
            "                        steps.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --log_level {debug,info,warning,error,critical,passive}\n",
            "                        Logger log level to use on the main node. Possible\n",
            "                        choices are the log levels as strings: 'debug',\n",
            "                        'info', 'warning', 'error' and 'critical', plus a\n",
            "                        'passive' level which doesn't set anything and lets\n",
            "                        the application set the level. Defaults to 'passive'.\n",
            "  --log_level_replica {debug,info,warning,error,critical,passive}\n",
            "                        Logger log level to use on replica nodes. Same choices\n",
            "                        and defaults as ``log_level``\n",
            "  --no_log_on_each_node\n",
            "                        When doing a multinode distributed training, whether\n",
            "                        to log once per node or just once on the main node.\n",
            "  --log_on_each_node [LOG_ON_EACH_NODE]\n",
            "                        When doing a multinode distributed training, whether\n",
            "                        to log once per node or just once on the main node.\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir.\n",
            "  --logging_strategy {no,steps,epoch}\n",
            "                        The logging strategy to use.\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "                        Log the first global_step\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_strategy {no,steps,epoch}\n",
            "                        The checkpoint save strategy to use.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints.Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints\n",
            "  --save_on_each_node [SAVE_ON_EACH_NODE]\n",
            "                        When doing multi-node distributed training, whether to\n",
            "                        save models and checkpoints on each node, or only on\n",
            "                        the main one\n",
            "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available\n",
            "  --seed SEED           Random seed that will be set at the beginning of\n",
            "                        training.\n",
            "  --fp16 [FP16]         Whether to use 16-bit (mixed) precision instead of\n",
            "                        32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --fp16_backend {auto,amp,apex}\n",
            "                        The backend to be used for mixed precision.\n",
            "  --fp16_full_eval [FP16_FULL_EVAL]\n",
            "                        Whether to use full 16-bit precision evaluation\n",
            "                        instead of 32-bit\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "                        Deprecated, the use of `--debug tpu_metrics_debug` is\n",
            "                        preferred. TPU: Whether to print debug metrics\n",
            "  --debug DEBUG         Whether or not to enable debug mode. Current options:\n",
            "                        `underflow_overflow` (Detect underflow and overflow in\n",
            "                        activations and weights), `tpu_metrics_debug` (print\n",
            "                        debug metrics on TPU).\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size.\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps.\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process.\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step.\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging.\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "  --no_remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels.\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training.\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not.\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "                        When resuming training, whether or not to skip the\n",
            "                        first epochs and batches to get to the same training\n",
            "                        data.\n",
            "  --sharded_ddp SHARDED_DDP\n",
            "                        Whether or not to use sharded DDP training (in\n",
            "                        distributed training only). The base option should be\n",
            "                        `simple`, `zero_dp_2` or `zero_dp_3` and you can add\n",
            "                        CPU-offload to `zero_dp_2` or `zero_dp_3` like this:\n",
            "                        zero_dp_2 offload` or `zero_dp_3 offload`. You can add\n",
            "                        auto-wrap to `zero_dp_2` or with the same syntax:\n",
            "                        zero_dp_2 auto_wrap` or `zero_dp_3 auto_wrap`.\n",
            "  --deepspeed DEEPSPEED\n",
            "                        Enable deepspeed and pass the path to deepspeed json\n",
            "                        config file (e.g. ds_config.json) or an already loaded\n",
            "                        json file as a dict\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "                        The label smoothing epsilon to apply (zero means no\n",
            "                        label smoothing).\n",
            "  --adafactor [ADAFACTOR]\n",
            "                        Whether or not to replace AdamW by Adafactor.\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "                        Whether or not to group samples of roughly the same\n",
            "                        length together when batching.\n",
            "  --length_column_name LENGTH_COLUMN_NAME\n",
            "                        Column name with precomputed lengths to use when\n",
            "                        grouping by length.\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "                        The list of integrations to report the results and\n",
            "                        logs to.\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `find_unused_parameters` passed to\n",
            "                        `DistributedDataParallel`.\n",
            "  --no_dataloader_pin_memory\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --no_skip_memory_metrics\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics.\n",
            "  --skip_memory_metrics [SKIP_MEMORY_METRICS]\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics.\n",
            "  --use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]\n",
            "                        Whether or not to use the legacy prediction_loop in\n",
            "                        the Trainer.\n",
            "  --push_to_hub [PUSH_TO_HUB]\n",
            "                        Whether or not to upload the trained model to the\n",
            "                        model hub after training.\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        The path to a folder with a valid checkpoint for your\n",
            "                        model.\n",
            "  --push_to_hub_model_id PUSH_TO_HUB_MODEL_ID\n",
            "                        The name of the repository to which push the\n",
            "                        `Trainer`.\n",
            "  --push_to_hub_organization PUSH_TO_HUB_ORGANIZATION\n",
            "                        The name of the organization in with to which push the\n",
            "                        `Trainer`.\n",
            "  --push_to_hub_token PUSH_TO_HUB_TOKEN\n",
            "                        The token to use to push to the Model Hub.\n",
            "  --mp_parameters MP_PARAMETERS\n",
            "                        Used by the SageMaker launcher to send mp-specific\n",
            "                        args. Ignored in Trainer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP_E2Z9FDkcR",
        "outputId": "d3da9ea2-686e-4a09-9fed-d3d57c58513a"
      },
      "source": [
        "!python run_ner.py \\\n",
        "  --task_name ner \\\n",
        "  --model_name_or_path distilbert-base-cased \\\n",
        "  --label_all_tokens True \\\n",
        "  --return_entity_level_metrics True \\\n",
        "  --dataset_name conllpp \\\n",
        "  --output_dir /tmp/distilbert-base-cased-finetuned-conllpp-english_pt \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --save_strategy epoch \\\n",
        "  --logging_strategy epoch \\\n",
        "  --num_train_epochs 10"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 06:29:48.213244: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/25/2021 06:29:49 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/25/2021 06:29:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/distilbert-base-cased-finetuned-conllpp-english_pt/runs/Jul25_06-29-49_41665f6ef460,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.EPOCH,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "output_dir=/tmp/distilbert-base-cased-finetuned-conllpp-english_pt,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=distilbert-base-cased-finetuned-conllpp-english_pt,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/distilbert-base-cased-finetuned-conllpp-english_pt,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/25/2021 06:29:50 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/conllpp.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpspvt579u\n",
            "Downloading: 8.73kB [00:00, 10.5MB/s]       \n",
            "07/25/2021 06:29:50 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/conllpp.py in cache at /root/.cache/huggingface/datasets/downloads/a0f02b0b31073e39d0df13883a33bff6f9d48db6ed5d5a2d2b5d558bcfbe5e08.d04f9130411af1435d5ada33ff90b848beab957a57c602e90d8ab92cee4dc96a.py\n",
            "07/25/2021 06:29:50 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a0f02b0b31073e39d0df13883a33bff6f9d48db6ed5d5a2d2b5d558bcfbe5e08.d04f9130411af1435d5ada33ff90b848beab957a57c602e90d8ab92cee4dc96a.py\n",
            "07/25/2021 06:29:50 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpex6r2v7o\n",
            "Downloading: 3.35kB [00:00, 4.60MB/s]       \n",
            "07/25/2021 06:29:50 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/6abc468f7ebe2666c26e6d1c4fb6e6df9ee28eee3f19cbf3d3da5184ebe5948b.a02de846064644641c065cc276fe79f4d7dc46997a2ad799bef80bd2310fd7cb\n",
            "07/25/2021 06:29:50 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6abc468f7ebe2666c26e6d1c4fb6e6df9ee28eee3f19cbf3d3da5184ebe5948b.a02de846064644641c065cc276fe79f4d7dc46997a2ad799bef80bd2310fd7cb\n",
            "07/25/2021 06:29:50 - INFO - datasets.load - Creating main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/conllpp.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conllpp\n",
            "07/25/2021 06:29:50 - INFO - datasets.load - Creating specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/conllpp.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conllpp/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2\n",
            "07/25/2021 06:29:50 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/conllpp.py to /root/.cache/huggingface/modules/datasets_modules/datasets/conllpp/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/conllpp.py\n",
            "07/25/2021 06:29:50 - INFO - datasets.load - Copying dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/conllpp/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/dataset_infos.json\n",
            "07/25/2021 06:29:50 - INFO - datasets.load - Creating metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.10.2/datasets/conllpp/conllpp.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conllpp/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/conllpp.json\n",
            "07/25/2021 06:29:50 - INFO - datasets.builder - No config specified, defaulting to first: conllpp/conllpp\n",
            "07/25/2021 06:29:50 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/conllpp/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2\n",
            "07/25/2021 06:29:50 - INFO - datasets.builder - Generating dataset conllpp (/root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n",
            "Downloading and preparing dataset conllpp/conllpp (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2...\n",
            "07/25/2021 06:29:50 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "  0% 0/3 [00:00<?, ?it/s]07/25/2021 06:29:52 - INFO - datasets.utils.file_utils - https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_train.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpkq_b8l0w\n",
            "\n",
            "Downloading: 3.28MB [00:00, 71.3MB/s]      \n",
            "07/25/2021 06:29:52 - INFO - datasets.utils.file_utils - storing https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_train.txt in cache at /root/.cache/huggingface/datasets/downloads/f18ba4c74f5fa9565cd13ecb66699f4e78189b1f2df7830b788a223a68d28360\n",
            "07/25/2021 06:29:52 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/f18ba4c74f5fa9565cd13ecb66699f4e78189b1f2df7830b788a223a68d28360\n",
            " 33% 1/3 [00:01<00:02,  1.31s/it]07/25/2021 06:29:52 - INFO - datasets.utils.file_utils - https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_dev.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpgogz_g36\n",
            "\n",
            "Downloading: 827kB [00:00, 51.2MB/s]       \n",
            "07/25/2021 06:29:53 - INFO - datasets.utils.file_utils - storing https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_dev.txt in cache at /root/.cache/huggingface/datasets/downloads/48ddcf83197caef607ebcaf6d34acc9fd6c86880fda0b9c3b8f7615c79ff14c3\n",
            "07/25/2021 06:29:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/48ddcf83197caef607ebcaf6d34acc9fd6c86880fda0b9c3b8f7615c79ff14c3\n",
            " 67% 2/3 [00:02<00:01,  1.05s/it]07/25/2021 06:29:54 - INFO - datasets.utils.file_utils - https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_test.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpki60cshi\n",
            "\n",
            "Downloading: 749kB [00:00, 48.2MB/s]       \n",
            "07/25/2021 06:29:54 - INFO - datasets.utils.file_utils - storing https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_test.txt in cache at /root/.cache/huggingface/datasets/downloads/35e9a9514e0a7910a2ec2c0ce3e9f5e5f095bc44ac760e2104732c4b9284c893\n",
            "07/25/2021 06:29:54 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/35e9a9514e0a7910a2ec2c0ce3e9f5e5f095bc44ac760e2104732c4b9284c893\n",
            "100% 3/3 [00:03<00:00,  1.14s/it]\n",
            "07/25/2021 06:29:54 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "07/25/2021 06:29:54 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 3/3 [00:00<00:00, 1667.94it/s]\n",
            "07/25/2021 06:29:54 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "07/25/2021 06:29:54 - INFO - datasets.builder - Generating split train\n",
            "07/25/2021 06:29:56 - INFO - datasets.builder - Generating split validation\n",
            "07/25/2021 06:29:57 - INFO - datasets.builder - Generating split test\n",
            "07/25/2021 06:29:58 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset conllpp downloaded and prepared to /root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 337.81it/s]\n",
            "[INFO|file_utils.py:1624] 2021-07-25 06:29:58,379 >> https://huggingface.co/distilbert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9mb5bcxj\n",
            "Downloading: 100% 411/411 [00:00<00:00, 348kB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-25 06:29:58,674 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|file_utils.py:1636] 2021-07-25 06:29:58,674 >> creating metadata file for /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:545] 2021-07-25 06:29:58,675 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:581] 2021-07-25 06:29:58,675 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2,\n",
            "    \"3\": 3,\n",
            "    \"4\": 4,\n",
            "    \"5\": 5,\n",
            "    \"6\": 6,\n",
            "    \"7\": 7,\n",
            "    \"8\": 8\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2,\n",
            "    \"3\": 3,\n",
            "    \"4\": 4,\n",
            "    \"5\": 5,\n",
            "    \"6\": 6,\n",
            "    \"7\": 7,\n",
            "    \"8\": 8\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1624] 2021-07-25 06:29:58,977 >> https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmvtqrdch\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 25.6kB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-25 06:29:59,273 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1636] 2021-07-25 06:29:59,274 >> creating metadata file for /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:545] 2021-07-25 06:29:59,577 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:581] 2021-07-25 06:29:59,577 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1624] 2021-07-25 06:30:00,165 >> https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcz4nkd93\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 1.59MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-25 06:30:00,769 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1636] 2021-07-25 06:30:00,769 >> creating metadata file for /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1624] 2021-07-25 06:30:01,061 >> https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6lrv_g4d\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 2.51MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-25 06:30:01,529 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1636] 2021-07-25 06:30:01,529 >> creating metadata file for /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-25 06:30:02,422 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-25 06:30:02,422 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-25 06:30:02,422 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-25 06:30:02,422 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1730] 2021-07-25 06:30:02,422 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:545] 2021-07-25 06:30:02,716 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:581] 2021-07-25 06:30:02,717 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1624] 2021-07-25 06:30:03,034 >> https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp31_j1fuv\n",
            "Downloading: 100% 263M/263M [00:04<00:00, 58.8MB/s]\n",
            "[INFO|file_utils.py:1628] 2021-07-25 06:30:07,733 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[INFO|file_utils.py:1636] 2021-07-25 06:30:07,734 >> creating metadata file for /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[INFO|modeling_utils.py:1271] 2021-07-25 06:30:07,734 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[WARNING|modeling_utils.py:1502] 2021-07-25 06:30:08,543 >> Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1513] 2021-07-25 06:30:08,543 >> Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/15 [00:00<?, ?ba/s]07/25/2021 06:30:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-23b353aed6b31132.arrow\n",
            "Running tokenizer on train dataset: 100% 15/15 [00:01<00:00,  8.56ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]07/25/2021 06:30:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-a7cd2eb48cb0fd67.arrow\n",
            "Running tokenizer on validation dataset: 100% 4/4 [00:00<00:00,  9.78ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/4 [00:00<?, ?ba/s]07/25/2021 06:30:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-f6baa8180cff5369.arrow\n",
            "Running tokenizer on prediction dataset: 100% 4/4 [00:00<00:00,  8.45ba/s]\n",
            "07/25/2021 06:30:11 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/seqeval/seqeval.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp84aox99l\n",
            "Downloading: 6.34kB [00:00, 6.27MB/s]       \n",
            "07/25/2021 06:30:11 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/seqeval/seqeval.py in cache at /root/.cache/huggingface/datasets/downloads/68395a83ec996bac33c81d9e90cbfb0cdc7af7c2b4582c06158458d1a7de3856.a09051c6235c8b054e473ffcc08da34f95b4a1800e1818bce66eed11f4e3833d.py\n",
            "07/25/2021 06:30:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/68395a83ec996bac33c81d9e90cbfb0cdc7af7c2b4582c06158458d1a7de3856.a09051c6235c8b054e473ffcc08da34f95b4a1800e1818bce66eed11f4e3833d.py\n",
            "07/25/2021 06:30:11 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval\n",
            "07/25/2021 06:30:11 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e\n",
            "07/25/2021 06:30:11 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/seqeval/seqeval.py to /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.py\n",
            "07/25/2021 06:30:11 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/seqeval/dataset_infos.json\n",
            "07/25/2021 06:30:11 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.10.2/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.json\n",
            "[INFO|trainer.py:522] 2021-07-25 06:30:23,616 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:1164] 2021-07-25 06:30:23,632 >> ***** Running training *****\n",
            "[INFO|trainer.py:1165] 2021-07-25 06:30:23,633 >>   Num examples = 14041\n",
            "[INFO|trainer.py:1166] 2021-07-25 06:30:23,633 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1167] 2021-07-25 06:30:23,633 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1168] 2021-07-25 06:30:23,633 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1169] 2021-07-25 06:30:23,633 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1170] 2021-07-25 06:30:23,633 >>   Total optimization steps = 17560\n",
            "{'loss': 0.1477, 'learning_rate': 4.5e-05, 'epoch': 1.0}\n",
            " 10% 1756/17560 [01:35<15:40, 16.80it/s][INFO|trainer.py:522] 2021-07-25 06:31:59,056 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:31:59,058 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:31:59,058 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:31:59,058 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 11/407 [00:00<00:03, 99.89it/s]\u001b[A\n",
            "  5% 21/407 [00:00<00:04, 93.53it/s]\u001b[A\n",
            "  8% 31/407 [00:00<00:04, 92.82it/s]\u001b[A\n",
            " 10% 41/407 [00:00<00:04, 89.92it/s]\u001b[A\n",
            " 13% 51/407 [00:00<00:04, 86.46it/s]\u001b[A\n",
            " 15% 63/407 [00:00<00:03, 95.52it/s]\u001b[A\n",
            " 18% 73/407 [00:00<00:03, 96.24it/s]\u001b[A\n",
            " 20% 83/407 [00:00<00:03, 93.70it/s]\u001b[A\n",
            " 23% 93/407 [00:01<00:03, 84.76it/s]\u001b[A\n",
            " 25% 102/407 [00:01<00:03, 78.57it/s]\u001b[A\n",
            " 27% 111/407 [00:01<00:03, 75.59it/s]\u001b[A\n",
            " 29% 119/407 [00:01<00:03, 73.84it/s]\u001b[A\n",
            " 31% 127/407 [00:01<00:03, 70.53it/s]\u001b[A\n",
            " 33% 135/407 [00:01<00:04, 67.94it/s]\u001b[A\n",
            " 35% 142/407 [00:01<00:04, 65.46it/s]\u001b[A\n",
            " 37% 149/407 [00:01<00:03, 64.85it/s]\u001b[A\n",
            " 38% 156/407 [00:02<00:03, 65.72it/s]\u001b[A\n",
            " 41% 166/407 [00:02<00:03, 73.08it/s]\u001b[A\n",
            " 43% 175/407 [00:02<00:03, 76.44it/s]\u001b[A\n",
            " 45% 183/407 [00:02<00:02, 75.35it/s]\u001b[A\n",
            " 47% 191/407 [00:02<00:02, 72.09it/s]\u001b[A\n",
            " 49% 199/407 [00:02<00:02, 71.83it/s]\u001b[A\n",
            " 51% 207/407 [00:02<00:02, 73.62it/s]\u001b[A\n",
            " 53% 215/407 [00:02<00:02, 73.86it/s]\u001b[A\n",
            " 55% 225/407 [00:02<00:02, 79.31it/s]\u001b[A\n",
            " 57% 233/407 [00:03<00:02, 74.59it/s]\u001b[A\n",
            " 60% 243/407 [00:03<00:02, 79.95it/s]\u001b[A\n",
            " 63% 257/407 [00:03<00:01, 95.55it/s]\u001b[A\n",
            " 66% 269/407 [00:03<00:01, 102.18it/s]\u001b[A\n",
            " 69% 280/407 [00:03<00:01, 89.10it/s] \u001b[A\n",
            " 71% 290/407 [00:03<00:01, 88.57it/s]\u001b[A\n",
            " 74% 300/407 [00:03<00:01, 81.45it/s]\u001b[A\n",
            " 77% 312/407 [00:03<00:01, 89.09it/s]\u001b[A\n",
            " 79% 322/407 [00:04<00:01, 81.04it/s]\u001b[A\n",
            " 81% 331/407 [00:04<00:01, 67.43it/s]\u001b[A\n",
            " 83% 339/407 [00:04<00:00, 69.38it/s]\u001b[A\n",
            " 86% 348/407 [00:04<00:00, 72.82it/s]\u001b[A\n",
            " 87% 356/407 [00:04<00:00, 72.82it/s]\u001b[A\n",
            " 89% 364/407 [00:04<00:00, 68.75it/s]\u001b[A\n",
            " 91% 372/407 [00:04<00:00, 68.81it/s]\u001b[A\n",
            " 93% 380/407 [00:04<00:00, 69.15it/s]\u001b[A\n",
            " 95% 388/407 [00:05<00:00, 67.77it/s]\u001b[A\n",
            " 97% 395/407 [00:05<00:00, 66.53it/s]\u001b[A\n",
            " 99% 402/407 [00:05<00:00, 65.98it/s]\u001b[A07/25/2021 06:32:05 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.09074925631284714, 'eval_LOC_precision': 0.9628623188405797, 'eval_LOC_recall': 0.8773039889958735, 'eval_LOC_f1': 0.918094141355981, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.7811912225705329, 'eval_MISC_recall': 0.8418918918918918, 'eval_MISC_f1': 0.8104065040650406, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.8929640718562875, 'eval_ORG_recall': 0.8830495928941525, 'eval_ORG_f1': 0.8879791589132863, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9165222414789139, 'eval_PER_recall': 0.9531390808050466, 'eval_PER_f1': 0.9344720954204093, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9051716330042568, 'eval_overall_recall': 0.8966445361564687, 'eval_overall_f1': 0.900887907333123, 'eval_overall_accuracy': 0.9759080473303114, 'eval_runtime': 6.9038, 'eval_samples_per_second': 470.754, 'eval_steps_per_second': 58.953, 'epoch': 1.0}\n",
            " 10% 1756/17560 [01:42<15:40, 16.80it/s]\n",
            "100% 407/407 [00:06<00:00, 65.98it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:32:05,965 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-1756\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:32:05,966 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-1756/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:32:06,494 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-1756/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:32:06,494 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-1756/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:32:06,494 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-1756/special_tokens_map.json\n",
            "{'loss': 0.0541, 'learning_rate': 4e-05, 'epoch': 2.0}\n",
            " 20% 3512/17560 [03:22<12:16, 19.07it/s][INFO|trainer.py:522] 2021-07-25 06:33:46,188 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:33:46,190 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:33:46,191 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:33:46,191 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 10/407 [00:00<00:03, 99.39it/s]\u001b[A\n",
            "  5% 20/407 [00:00<00:04, 89.99it/s]\u001b[A\n",
            "  7% 30/407 [00:00<00:04, 91.04it/s]\u001b[A\n",
            " 10% 40/407 [00:00<00:04, 91.11it/s]\u001b[A\n",
            " 12% 50/407 [00:00<00:04, 85.56it/s]\u001b[A\n",
            " 15% 62/407 [00:00<00:03, 95.08it/s]\u001b[A\n",
            " 18% 73/407 [00:00<00:03, 96.29it/s]\u001b[A\n",
            " 20% 83/407 [00:00<00:03, 95.35it/s]\u001b[A\n",
            " 23% 93/407 [00:01<00:03, 85.76it/s]\u001b[A\n",
            " 25% 102/407 [00:01<00:03, 79.06it/s]\u001b[A\n",
            " 27% 111/407 [00:01<00:03, 76.42it/s]\u001b[A\n",
            " 29% 119/407 [00:01<00:03, 74.45it/s]\u001b[A\n",
            " 31% 127/407 [00:01<00:04, 69.16it/s]\u001b[A\n",
            " 33% 135/407 [00:01<00:04, 66.32it/s]\u001b[A\n",
            " 35% 142/407 [00:01<00:04, 64.84it/s]\u001b[A\n",
            " 37% 149/407 [00:01<00:04, 64.35it/s]\u001b[A\n",
            " 38% 156/407 [00:02<00:03, 65.35it/s]\u001b[A\n",
            " 41% 166/407 [00:02<00:03, 72.77it/s]\u001b[A\n",
            " 43% 175/407 [00:02<00:03, 76.28it/s]\u001b[A\n",
            " 45% 183/407 [00:02<00:02, 75.24it/s]\u001b[A\n",
            " 47% 191/407 [00:02<00:03, 71.69it/s]\u001b[A\n",
            " 49% 199/407 [00:02<00:02, 71.83it/s]\u001b[A\n",
            " 51% 207/407 [00:02<00:02, 73.81it/s]\u001b[A\n",
            " 53% 215/407 [00:02<00:02, 73.72it/s]\u001b[A\n",
            " 55% 225/407 [00:02<00:02, 79.18it/s]\u001b[A\n",
            " 57% 233/407 [00:03<00:02, 75.08it/s]\u001b[A\n",
            " 60% 243/407 [00:03<00:02, 79.38it/s]\u001b[A\n",
            " 63% 257/407 [00:03<00:01, 95.23it/s]\u001b[A\n",
            " 66% 270/407 [00:03<00:01, 101.98it/s]\u001b[A\n",
            " 69% 281/407 [00:03<00:01, 91.25it/s] \u001b[A\n",
            " 71% 291/407 [00:03<00:01, 89.49it/s]\u001b[A\n",
            " 74% 301/407 [00:03<00:01, 82.36it/s]\u001b[A\n",
            " 77% 312/407 [00:03<00:01, 88.76it/s]\u001b[A\n",
            " 79% 322/407 [00:04<00:01, 81.49it/s]\u001b[A\n",
            " 81% 331/407 [00:04<00:01, 66.96it/s]\u001b[A\n",
            " 83% 339/407 [00:04<00:00, 68.81it/s]\u001b[A\n",
            " 86% 348/407 [00:04<00:00, 72.22it/s]\u001b[A\n",
            " 87% 356/407 [00:04<00:00, 72.05it/s]\u001b[A\n",
            " 89% 364/407 [00:04<00:00, 68.96it/s]\u001b[A\n",
            " 91% 372/407 [00:04<00:00, 69.08it/s]\u001b[A\n",
            " 93% 380/407 [00:04<00:00, 70.04it/s]\u001b[A\n",
            " 95% 388/407 [00:05<00:00, 67.72it/s]\u001b[A\n",
            " 97% 395/407 [00:05<00:00, 65.98it/s]\u001b[A\n",
            " 99% 402/407 [00:05<00:00, 66.32it/s]\u001b[A07/25/2021 06:33:53 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.08257891237735748, 'eval_LOC_precision': 0.9646118721461188, 'eval_LOC_recall': 0.9298486932599724, 'eval_LOC_f1': 0.9469113321193444, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.865424430641822, 'eval_MISC_recall': 0.8472972972972973, 'eval_MISC_f1': 0.8562649368385113, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.8690601900739177, 'eval_ORG_recall': 0.9137675795706883, 'eval_ORG_f1': 0.8908533285224607, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9204875217643644, 'eval_PER_recall': 0.9528386902973865, 'eval_PER_f1': 0.9363837638376383, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9141459074733096, 'eval_overall_recall': 0.9218553741252468, 'eval_overall_f1': 0.9179844545698204, 'eval_overall_accuracy': 0.979675634308589, 'eval_runtime': 6.9028, 'eval_samples_per_second': 470.826, 'eval_steps_per_second': 58.962, 'epoch': 2.0}\n",
            " 20% 3512/17560 [03:29<12:16, 19.07it/s]\n",
            "100% 407/407 [00:06<00:00, 66.32it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:33:53,096 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-3512\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:33:53,097 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-3512/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:33:53,612 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-3512/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:33:53,613 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-3512/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:33:53,613 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-3512/special_tokens_map.json\n",
            "{'loss': 0.0294, 'learning_rate': 3.5e-05, 'epoch': 3.0}\n",
            " 30% 5268/17560 [05:09<11:07, 18.42it/s][INFO|trainer.py:522] 2021-07-25 06:35:32,816 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:35:32,818 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:35:32,819 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:35:32,819 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 10/407 [00:00<00:03, 99.51it/s]\u001b[A\n",
            "  5% 20/407 [00:00<00:04, 92.10it/s]\u001b[A\n",
            "  7% 30/407 [00:00<00:04, 91.69it/s]\u001b[A\n",
            " 10% 40/407 [00:00<00:04, 90.82it/s]\u001b[A\n",
            " 12% 50/407 [00:00<00:04, 85.97it/s]\u001b[A\n",
            " 15% 62/407 [00:00<00:03, 94.92it/s]\u001b[A\n",
            " 18% 73/407 [00:00<00:03, 96.41it/s]\u001b[A\n",
            " 20% 83/407 [00:00<00:03, 94.70it/s]\u001b[A\n",
            " 23% 93/407 [00:01<00:03, 86.13it/s]\u001b[A\n",
            " 25% 102/407 [00:01<00:03, 79.59it/s]\u001b[A\n",
            " 27% 111/407 [00:01<00:03, 76.48it/s]\u001b[A\n",
            " 29% 119/407 [00:01<00:03, 74.52it/s]\u001b[A\n",
            " 31% 127/407 [00:01<00:04, 69.95it/s]\u001b[A\n",
            " 33% 135/407 [00:01<00:04, 67.11it/s]\u001b[A\n",
            " 35% 142/407 [00:01<00:04, 65.64it/s]\u001b[A\n",
            " 37% 149/407 [00:01<00:03, 65.32it/s]\u001b[A\n",
            " 38% 156/407 [00:01<00:03, 66.30it/s]\u001b[A\n",
            " 41% 166/407 [00:02<00:03, 73.99it/s]\u001b[A\n",
            " 43% 175/407 [00:02<00:02, 77.75it/s]\u001b[A\n",
            " 45% 183/407 [00:02<00:02, 76.39it/s]\u001b[A\n",
            " 47% 191/407 [00:02<00:02, 72.72it/s]\u001b[A\n",
            " 49% 199/407 [00:02<00:02, 72.27it/s]\u001b[A\n",
            " 51% 207/407 [00:02<00:02, 73.98it/s]\u001b[A\n",
            " 53% 215/407 [00:02<00:02, 74.38it/s]\u001b[A\n",
            " 55% 225/407 [00:02<00:02, 79.90it/s]\u001b[A\n",
            " 57% 234/407 [00:03<00:02, 75.34it/s]\u001b[A\n",
            " 60% 244/407 [00:03<00:02, 81.36it/s]\u001b[A\n",
            " 63% 258/407 [00:03<00:01, 96.71it/s]\u001b[A\n",
            " 66% 270/407 [00:03<00:01, 102.55it/s]\u001b[A\n",
            " 69% 281/407 [00:03<00:01, 91.67it/s] \u001b[A\n",
            " 71% 291/407 [00:03<00:01, 89.68it/s]\u001b[A\n",
            " 74% 301/407 [00:03<00:01, 83.19it/s]\u001b[A\n",
            " 77% 312/407 [00:03<00:01, 89.24it/s]\u001b[A\n",
            " 79% 322/407 [00:03<00:01, 81.56it/s]\u001b[A\n",
            " 81% 331/407 [00:04<00:01, 66.88it/s]\u001b[A\n",
            " 83% 339/407 [00:04<00:00, 68.57it/s]\u001b[A\n",
            " 86% 348/407 [00:04<00:00, 72.45it/s]\u001b[A\n",
            " 87% 356/407 [00:04<00:00, 72.37it/s]\u001b[A\n",
            " 89% 364/407 [00:04<00:00, 68.88it/s]\u001b[A\n",
            " 91% 372/407 [00:04<00:00, 69.64it/s]\u001b[A\n",
            " 93% 380/407 [00:04<00:00, 70.21it/s]\u001b[A\n",
            " 95% 388/407 [00:04<00:00, 68.47it/s]\u001b[A\n",
            " 97% 395/407 [00:05<00:00, 67.32it/s]\u001b[A\n",
            " 99% 402/407 [00:05<00:00, 66.59it/s]\u001b[A07/25/2021 06:35:39 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.10716952383518219, 'eval_LOC_precision': 0.9642248722316865, 'eval_LOC_recall': 0.9342503438789546, 'eval_LOC_f1': 0.9490009780634344, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.8794835007173601, 'eval_MISC_recall': 0.8283783783783784, 'eval_MISC_f1': 0.8531663187195546, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.9239904988123515, 'eval_ORG_recall': 0.8638045891931903, 'eval_ORG_f1': 0.8928844682478959, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.8837653478854025, 'eval_PER_recall': 0.9729648543106038, 'eval_PER_f1': 0.92622247640835, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9178896191590888, 'eval_overall_recall': 0.9146779113583349, 'eval_overall_f1': 0.916280950883027, 'eval_overall_accuracy': 0.979660917171955, 'eval_runtime': 7.0057, 'eval_samples_per_second': 463.911, 'eval_steps_per_second': 58.096, 'epoch': 3.0}\n",
            " 30% 5268/17560 [05:16<11:07, 18.42it/s]\n",
            "100% 407/407 [00:06<00:00, 66.59it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:35:39,827 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-5268\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:35:39,828 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-5268/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:35:40,347 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-5268/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:35:40,348 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-5268/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:35:40,348 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-5268/special_tokens_map.json\n",
            "{'loss': 0.0173, 'learning_rate': 3e-05, 'epoch': 4.0}\n",
            " 40% 7024/17560 [06:55<09:35, 18.31it/s][INFO|trainer.py:522] 2021-07-25 06:37:18,830 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:37:18,833 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:37:18,833 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:37:18,833 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 10/407 [00:00<00:03, 99.81it/s]\u001b[A\n",
            "  5% 20/407 [00:00<00:04, 91.91it/s]\u001b[A\n",
            "  7% 30/407 [00:00<00:04, 91.67it/s]\u001b[A\n",
            " 10% 40/407 [00:00<00:04, 91.35it/s]\u001b[A\n",
            " 12% 50/407 [00:00<00:04, 86.00it/s]\u001b[A\n",
            " 15% 62/407 [00:00<00:03, 95.41it/s]\u001b[A\n",
            " 18% 73/407 [00:00<00:03, 96.16it/s]\u001b[A\n",
            " 20% 83/407 [00:00<00:03, 94.48it/s]\u001b[A\n",
            " 23% 93/407 [00:01<00:03, 85.29it/s]\u001b[A\n",
            " 25% 102/407 [00:01<00:03, 78.43it/s]\u001b[A\n",
            " 27% 111/407 [00:01<00:03, 75.49it/s]\u001b[A\n",
            " 29% 119/407 [00:01<00:03, 73.68it/s]\u001b[A\n",
            " 31% 127/407 [00:01<00:04, 68.98it/s]\u001b[A\n",
            " 33% 134/407 [00:01<00:04, 66.30it/s]\u001b[A\n",
            " 35% 141/407 [00:01<00:04, 65.65it/s]\u001b[A\n",
            " 36% 148/407 [00:01<00:03, 64.94it/s]\u001b[A\n",
            " 38% 155/407 [00:01<00:03, 65.11it/s]\u001b[A\n",
            " 40% 164/407 [00:02<00:03, 71.54it/s]\u001b[A\n",
            " 42% 172/407 [00:02<00:03, 73.78it/s]\u001b[A\n",
            " 44% 181/407 [00:02<00:02, 76.78it/s]\u001b[A\n",
            " 46% 189/407 [00:02<00:03, 72.42it/s]\u001b[A\n",
            " 48% 197/407 [00:02<00:02, 71.88it/s]\u001b[A\n",
            " 50% 205/407 [00:02<00:02, 73.78it/s]\u001b[A\n",
            " 52% 213/407 [00:02<00:02, 72.46it/s]\u001b[A\n",
            " 55% 223/407 [00:02<00:02, 77.64it/s]\u001b[A\n",
            " 57% 231/407 [00:02<00:02, 77.47it/s]\u001b[A\n",
            " 59% 239/407 [00:03<00:02, 77.73it/s]\u001b[A\n",
            " 61% 250/407 [00:03<00:01, 86.68it/s]\u001b[A\n",
            " 65% 263/407 [00:03<00:01, 98.97it/s]\u001b[A\n",
            " 67% 273/407 [00:03<00:01, 95.54it/s]\u001b[A\n",
            " 70% 283/407 [00:03<00:01, 88.44it/s]\u001b[A\n",
            " 72% 292/407 [00:03<00:01, 85.21it/s]\u001b[A\n",
            " 74% 301/407 [00:03<00:01, 81.13it/s]\u001b[A\n",
            " 77% 312/407 [00:03<00:01, 88.18it/s]\u001b[A\n",
            " 79% 321/407 [00:04<00:01, 81.01it/s]\u001b[A\n",
            " 81% 330/407 [00:04<00:01, 66.97it/s]\u001b[A\n",
            " 83% 338/407 [00:04<00:01, 68.72it/s]\u001b[A\n",
            " 85% 347/407 [00:04<00:00, 71.91it/s]\u001b[A\n",
            " 87% 355/407 [00:04<00:00, 72.56it/s]\u001b[A\n",
            " 89% 363/407 [00:04<00:00, 68.22it/s]\u001b[A\n",
            " 91% 371/407 [00:04<00:00, 69.46it/s]\u001b[A\n",
            " 93% 379/407 [00:04<00:00, 69.61it/s]\u001b[A\n",
            " 95% 387/407 [00:05<00:00, 68.01it/s]\u001b[A\n",
            " 97% 394/407 [00:05<00:00, 67.23it/s]\u001b[A\n",
            " 99% 401/407 [00:05<00:00, 65.57it/s]\u001b[A07/25/2021 06:37:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.0869370773434639, 'eval_LOC_precision': 0.971063829787234, 'eval_LOC_recall': 0.9416781292984869, 'eval_LOC_f1': 0.956145251396648, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.885073580939033, 'eval_MISC_recall': 0.8533783783783784, 'eval_MISC_f1': 0.8689370485036121, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.8784783374427615, 'eval_ORG_recall': 0.923019985196151, 'eval_ORG_f1': 0.9001985201227215, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9339130434782609, 'eval_PER_recall': 0.9678582156803845, 'eval_PER_f1': 0.9505826818114768, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9253625122320078, 'eval_overall_recall': 0.9332495962677193, 'eval_overall_f1': 0.9292893196944655, 'eval_overall_accuracy': 0.9826632130452699, 'eval_runtime': 7.0655, 'eval_samples_per_second': 459.981, 'eval_steps_per_second': 57.604, 'epoch': 4.0}\n",
            " 40% 7024/17560 [07:02<09:35, 18.31it/s]\n",
            "100% 407/407 [00:07<00:00, 65.57it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:37:25,902 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-7024\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:37:25,903 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-7024/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:37:26,539 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-7024/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:37:26,540 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-7024/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:37:26,540 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-7024/special_tokens_map.json\n",
            "{'loss': 0.0104, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n",
            " 50% 8780/17560 [08:41<07:55, 18.45it/s][INFO|trainer.py:522] 2021-07-25 06:39:04,928 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:39:04,931 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:39:04,931 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:39:04,931 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 10/407 [00:00<00:03, 99.54it/s]\u001b[A\n",
            "  5% 20/407 [00:00<00:04, 90.16it/s]\u001b[A\n",
            "  7% 30/407 [00:00<00:04, 90.42it/s]\u001b[A\n",
            " 10% 40/407 [00:00<00:04, 89.71it/s]\u001b[A\n",
            " 12% 49/407 [00:00<00:04, 84.79it/s]\u001b[A\n",
            " 15% 61/407 [00:00<00:03, 93.87it/s]\u001b[A\n",
            " 18% 72/407 [00:00<00:03, 98.22it/s]\u001b[A\n",
            " 20% 82/407 [00:00<00:03, 93.43it/s]\u001b[A\n",
            " 23% 92/407 [00:01<00:03, 84.84it/s]\u001b[A\n",
            " 25% 101/407 [00:01<00:03, 77.25it/s]\u001b[A\n",
            " 27% 109/407 [00:01<00:03, 74.68it/s]\u001b[A\n",
            " 29% 117/407 [00:01<00:03, 73.71it/s]\u001b[A\n",
            " 31% 125/407 [00:01<00:04, 69.47it/s]\u001b[A\n",
            " 33% 133/407 [00:01<00:04, 66.60it/s]\u001b[A\n",
            " 34% 140/407 [00:01<00:04, 65.78it/s]\u001b[A\n",
            " 36% 147/407 [00:01<00:04, 64.51it/s]\u001b[A\n",
            " 38% 154/407 [00:01<00:03, 65.46it/s]\u001b[A\n",
            " 40% 163/407 [00:02<00:03, 70.91it/s]\u001b[A\n",
            " 42% 172/407 [00:02<00:03, 74.02it/s]\u001b[A\n",
            " 44% 181/407 [00:02<00:02, 78.21it/s]\u001b[A\n",
            " 46% 189/407 [00:02<00:03, 72.35it/s]\u001b[A\n",
            " 48% 197/407 [00:02<00:02, 72.19it/s]\u001b[A\n",
            " 51% 206/407 [00:02<00:02, 73.98it/s]\u001b[A\n",
            " 53% 214/407 [00:02<00:02, 72.85it/s]\u001b[A\n",
            " 55% 224/407 [00:02<00:02, 78.88it/s]\u001b[A\n",
            " 57% 232/407 [00:03<00:02, 75.93it/s]\u001b[A\n",
            " 59% 242/407 [00:03<00:02, 80.32it/s]\u001b[A\n",
            " 62% 253/407 [00:03<00:01, 88.50it/s]\u001b[A\n",
            " 65% 266/407 [00:03<00:01, 99.41it/s]\u001b[A\n",
            " 68% 277/407 [00:03<00:01, 86.41it/s]\u001b[A\n",
            " 71% 287/407 [00:03<00:01, 87.93it/s]\u001b[A\n",
            " 73% 297/407 [00:03<00:01, 82.86it/s]\u001b[A\n",
            " 75% 307/407 [00:03<00:01, 86.33it/s]\u001b[A\n",
            " 78% 316/407 [00:03<00:01, 85.73it/s]\u001b[A\n",
            " 80% 325/407 [00:04<00:01, 73.90it/s]\u001b[A\n",
            " 82% 333/407 [00:04<00:01, 67.46it/s]\u001b[A\n",
            " 84% 341/407 [00:04<00:00, 70.28it/s]\u001b[A\n",
            " 86% 349/407 [00:04<00:00, 72.65it/s]\u001b[A\n",
            " 88% 357/407 [00:04<00:00, 72.56it/s]\u001b[A\n",
            " 90% 365/407 [00:04<00:00, 69.41it/s]\u001b[A\n",
            " 92% 373/407 [00:04<00:00, 70.00it/s]\u001b[A\n",
            " 94% 381/407 [00:04<00:00, 70.09it/s]\u001b[A\n",
            " 96% 389/407 [00:05<00:00, 67.43it/s]\u001b[A\n",
            " 97% 396/407 [00:05<00:00, 66.01it/s]\u001b[A\n",
            " 99% 403/407 [00:05<00:00, 66.05it/s]\u001b[A07/25/2021 06:39:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.10247855633497238, 'eval_LOC_precision': 0.9560530679933665, 'eval_LOC_recall': 0.9515818431911967, 'eval_LOC_f1': 0.9538122156349097, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.8355471220746363, 'eval_MISC_recall': 0.8925675675675676, 'eval_MISC_f1': 0.8631166285527606, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.9359375, 'eval_ORG_recall': 0.8867505551443375, 'eval_ORG_f1': 0.910680349676929, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9490312965722801, 'eval_PER_recall': 0.9564433763893061, 'eval_PER_f1': 0.9527229204069418, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9321576390138564, 'eval_overall_recall': 0.9294814283150906, 'eval_overall_f1': 0.930817610062893, 'eval_overall_accuracy': 0.9824718902690293, 'eval_runtime': 7.0673, 'eval_samples_per_second': 459.864, 'eval_steps_per_second': 57.589, 'epoch': 5.0}\n",
            " 50% 8780/17560 [08:48<07:55, 18.45it/s]\n",
            "100% 407/407 [00:07<00:00, 66.05it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:39:12,001 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-8780\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:39:12,001 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-8780/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:39:12,658 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-8780/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:39:12,659 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-8780/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:39:12,659 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-8780/special_tokens_map.json\n",
            "{'loss': 0.0062, 'learning_rate': 2e-05, 'epoch': 6.0}\n",
            " 60% 10536/17560 [10:26<06:28, 18.07it/s][INFO|trainer.py:522] 2021-07-25 06:40:50,610 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:40:50,613 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:40:50,613 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:40:50,613 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 10/407 [00:00<00:03, 99.60it/s]\u001b[A\n",
            "  5% 20/407 [00:00<00:04, 90.91it/s]\u001b[A\n",
            "  7% 30/407 [00:00<00:04, 91.67it/s]\u001b[A\n",
            " 10% 40/407 [00:00<00:04, 91.14it/s]\u001b[A\n",
            " 12% 50/407 [00:00<00:04, 85.61it/s]\u001b[A\n",
            " 15% 62/407 [00:00<00:03, 94.54it/s]\u001b[A\n",
            " 18% 73/407 [00:00<00:03, 95.13it/s]\u001b[A\n",
            " 20% 83/407 [00:00<00:03, 93.17it/s]\u001b[A\n",
            " 23% 93/407 [00:01<00:03, 85.32it/s]\u001b[A\n",
            " 25% 102/407 [00:01<00:03, 78.72it/s]\u001b[A\n",
            " 27% 111/407 [00:01<00:03, 75.72it/s]\u001b[A\n",
            " 29% 119/407 [00:01<00:03, 74.07it/s]\u001b[A\n",
            " 31% 127/407 [00:01<00:03, 70.32it/s]\u001b[A\n",
            " 33% 135/407 [00:01<00:04, 66.85it/s]\u001b[A\n",
            " 35% 142/407 [00:01<00:04, 64.85it/s]\u001b[A\n",
            " 37% 149/407 [00:01<00:03, 65.07it/s]\u001b[A\n",
            " 38% 156/407 [00:02<00:03, 65.70it/s]\u001b[A\n",
            " 41% 166/407 [00:02<00:03, 73.16it/s]\u001b[A\n",
            " 43% 175/407 [00:02<00:03, 76.46it/s]\u001b[A\n",
            " 45% 183/407 [00:02<00:02, 75.68it/s]\u001b[A\n",
            " 47% 191/407 [00:02<00:03, 71.91it/s]\u001b[A\n",
            " 49% 199/407 [00:02<00:02, 72.49it/s]\u001b[A\n",
            " 51% 207/407 [00:02<00:02, 74.53it/s]\u001b[A\n",
            " 53% 215/407 [00:02<00:02, 74.83it/s]\u001b[A\n",
            " 55% 225/407 [00:02<00:02, 80.52it/s]\u001b[A\n",
            " 57% 234/407 [00:03<00:02, 75.81it/s]\u001b[A\n",
            " 60% 244/407 [00:03<00:02, 81.06it/s]\u001b[A\n",
            " 63% 258/407 [00:03<00:01, 96.69it/s]\u001b[A\n",
            " 66% 270/407 [00:03<00:01, 102.34it/s]\u001b[A\n",
            " 69% 281/407 [00:03<00:01, 90.98it/s] \u001b[A\n",
            " 71% 291/407 [00:03<00:01, 88.11it/s]\u001b[A\n",
            " 74% 301/407 [00:03<00:01, 82.30it/s]\u001b[A\n",
            " 77% 313/407 [00:03<00:01, 88.87it/s]\u001b[A\n",
            " 79% 323/407 [00:04<00:01, 80.40it/s]\u001b[A\n",
            " 82% 332/407 [00:04<00:01, 67.66it/s]\u001b[A\n",
            " 84% 340/407 [00:04<00:00, 70.40it/s]\u001b[A\n",
            " 86% 349/407 [00:04<00:00, 73.01it/s]\u001b[A\n",
            " 88% 357/407 [00:04<00:00, 72.89it/s]\u001b[A\n",
            " 90% 365/407 [00:04<00:00, 69.79it/s]\u001b[A\n",
            " 92% 373/407 [00:04<00:00, 70.81it/s]\u001b[A\n",
            " 94% 381/407 [00:04<00:00, 69.32it/s]\u001b[A\n",
            " 96% 389/407 [00:05<00:00, 67.92it/s]\u001b[A\n",
            " 97% 396/407 [00:05<00:00, 66.82it/s]\u001b[A\n",
            " 99% 403/407 [00:05<00:00, 66.65it/s]\u001b[A07/25/2021 06:40:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.11985049396753311, 'eval_LOC_precision': 0.9703365658870508, 'eval_LOC_recall': 0.9359009628610729, 'eval_LOC_f1': 0.9528077300098026, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.8505452212957024, 'eval_MISC_recall': 0.8959459459459459, 'eval_MISC_f1': 0.8726554787759131, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.8923132704858593, 'eval_ORG_recall': 0.9108068097705403, 'eval_ORG_f1': 0.9014652014652014, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9406952965235174, 'eval_PER_recall': 0.9672574346650645, 'eval_PER_f1': 0.9537914691943128, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9255735372576916, 'eval_overall_recall': 0.9338776242598241, 'eval_overall_f1': 0.9297070382279385, 'eval_overall_accuracy': 0.9823541531759581, 'eval_runtime': 6.9078, 'eval_samples_per_second': 470.482, 'eval_steps_per_second': 58.919, 'epoch': 6.0}\n",
            " 60% 10536/17560 [10:33<06:28, 18.07it/s]\n",
            "100% 407/407 [00:06<00:00, 66.65it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:40:57,524 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-10536\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:40:57,525 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-10536/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:40:58,206 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-10536/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:40:58,206 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-10536/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:40:58,207 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-10536/special_tokens_map.json\n",
            "{'loss': 0.0042, 'learning_rate': 1.5e-05, 'epoch': 7.0}\n",
            " 70% 12292/17560 [12:12<04:57, 17.70it/s][INFO|trainer.py:522] 2021-07-25 06:42:36,123 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:42:36,125 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:42:36,125 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:42:36,126 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 10/407 [00:00<00:04, 97.63it/s]\u001b[A\n",
            "  5% 20/407 [00:00<00:04, 91.30it/s]\u001b[A\n",
            "  7% 30/407 [00:00<00:04, 88.73it/s]\u001b[A\n",
            " 10% 40/407 [00:00<00:04, 89.47it/s]\u001b[A\n",
            " 12% 49/407 [00:00<00:04, 84.78it/s]\u001b[A\n",
            " 15% 60/407 [00:00<00:03, 91.09it/s]\u001b[A\n",
            " 17% 71/407 [00:00<00:03, 96.71it/s]\u001b[A\n",
            " 20% 81/407 [00:00<00:03, 93.64it/s]\u001b[A\n",
            " 22% 91/407 [00:01<00:03, 85.82it/s]\u001b[A\n",
            " 25% 100/407 [00:01<00:03, 77.64it/s]\u001b[A\n",
            " 27% 108/407 [00:01<00:03, 75.75it/s]\u001b[A\n",
            " 29% 116/407 [00:01<00:03, 74.56it/s]\u001b[A\n",
            " 30% 124/407 [00:01<00:04, 70.19it/s]\u001b[A\n",
            " 32% 132/407 [00:01<00:04, 65.25it/s]\u001b[A\n",
            " 34% 139/407 [00:01<00:04, 64.67it/s]\u001b[A\n",
            " 36% 146/407 [00:01<00:04, 64.31it/s]\u001b[A\n",
            " 38% 153/407 [00:01<00:03, 64.36it/s]\u001b[A\n",
            " 40% 161/407 [00:02<00:03, 67.71it/s]\u001b[A\n",
            " 42% 170/407 [00:02<00:03, 73.76it/s]\u001b[A\n",
            " 44% 179/407 [00:02<00:02, 77.80it/s]\u001b[A\n",
            " 46% 187/407 [00:02<00:03, 73.25it/s]\u001b[A\n",
            " 48% 195/407 [00:02<00:02, 72.70it/s]\u001b[A\n",
            " 50% 204/407 [00:02<00:02, 74.48it/s]\u001b[A\n",
            " 52% 212/407 [00:02<00:02, 71.71it/s]\u001b[A\n",
            " 55% 222/407 [00:02<00:02, 78.35it/s]\u001b[A\n",
            " 57% 230/407 [00:02<00:02, 77.69it/s]\u001b[A\n",
            " 58% 238/407 [00:03<00:02, 77.58it/s]\u001b[A\n",
            " 61% 249/407 [00:03<00:01, 84.29it/s]\u001b[A\n",
            " 65% 263/407 [00:03<00:01, 98.56it/s]\u001b[A\n",
            " 67% 273/407 [00:03<00:01, 96.14it/s]\u001b[A\n",
            " 70% 283/407 [00:03<00:01, 89.19it/s]\u001b[A\n",
            " 72% 293/407 [00:03<00:01, 85.05it/s]\u001b[A\n",
            " 74% 302/407 [00:03<00:01, 82.03it/s]\u001b[A\n",
            " 77% 313/407 [00:03<00:01, 89.25it/s]\u001b[A\n",
            " 79% 323/407 [00:04<00:01, 80.26it/s]\u001b[A\n",
            " 82% 332/407 [00:04<00:01, 67.24it/s]\u001b[A\n",
            " 84% 340/407 [00:04<00:00, 69.64it/s]\u001b[A\n",
            " 86% 349/407 [00:04<00:00, 72.87it/s]\u001b[A\n",
            " 88% 357/407 [00:04<00:00, 72.85it/s]\u001b[A\n",
            " 90% 365/407 [00:04<00:00, 69.38it/s]\u001b[A\n",
            " 92% 373/407 [00:04<00:00, 69.70it/s]\u001b[A\n",
            " 94% 381/407 [00:04<00:00, 69.98it/s]\u001b[A\n",
            " 96% 389/407 [00:05<00:00, 68.93it/s]\u001b[A\n",
            " 97% 396/407 [00:05<00:00, 67.49it/s]\u001b[A\n",
            " 99% 403/407 [00:05<00:00, 67.17it/s]\u001b[A07/25/2021 06:42:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.12243547290563583, 'eval_LOC_precision': 0.9424266881894, 'eval_LOC_recall': 0.9636863823933975, 'eval_LOC_f1': 0.9529379760609358, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.8313253012048193, 'eval_MISC_recall': 0.8858108108108108, 'eval_MISC_f1': 0.8577036310107948, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.9363354037267081, 'eval_ORG_recall': 0.8926720947446336, 'eval_ORG_f1': 0.913982569154983, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9368635437881874, 'eval_PER_recall': 0.9672574346650645, 'eval_PER_f1': 0.9518179130948861, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9238524807641284, 'eval_overall_recall': 0.937197200789521, 'eval_overall_f1': 0.9304769963924642, 'eval_overall_accuracy': 0.9823541531759581, 'eval_runtime': 7.057, 'eval_samples_per_second': 460.539, 'eval_steps_per_second': 57.674, 'epoch': 7.0}\n",
            " 70% 12292/17560 [12:19<04:57, 17.70it/s]\n",
            "100% 407/407 [00:07<00:00, 67.17it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:42:43,185 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-12292\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:42:43,186 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-12292/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:42:43,795 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-12292/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:42:43,795 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-12292/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:42:43,796 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-12292/special_tokens_map.json\n",
            "{'loss': 0.0027, 'learning_rate': 1e-05, 'epoch': 8.0}\n",
            " 80% 14048/17560 [13:57<03:16, 17.89it/s][INFO|trainer.py:522] 2021-07-25 06:44:21,339 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:44:21,342 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:44:21,342 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:44:21,342 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 11/407 [00:00<00:04, 98.24it/s]\u001b[A\n",
            "  5% 21/407 [00:00<00:04, 93.66it/s]\u001b[A\n",
            "  8% 31/407 [00:00<00:04, 92.46it/s]\u001b[A\n",
            " 10% 41/407 [00:00<00:04, 89.80it/s]\u001b[A\n",
            " 12% 50/407 [00:00<00:04, 86.26it/s]\u001b[A\n",
            " 15% 62/407 [00:00<00:03, 96.03it/s]\u001b[A\n",
            " 18% 72/407 [00:00<00:03, 97.20it/s]\u001b[A\n",
            " 20% 82/407 [00:00<00:03, 94.32it/s]\u001b[A\n",
            " 23% 92/407 [00:01<00:03, 85.79it/s]\u001b[A\n",
            " 25% 101/407 [00:01<00:03, 79.57it/s]\u001b[A\n",
            " 27% 110/407 [00:01<00:03, 76.65it/s]\u001b[A\n",
            " 29% 118/407 [00:01<00:03, 74.70it/s]\u001b[A\n",
            " 31% 126/407 [00:01<00:03, 70.43it/s]\u001b[A\n",
            " 33% 134/407 [00:01<00:04, 66.67it/s]\u001b[A\n",
            " 35% 141/407 [00:01<00:04, 65.44it/s]\u001b[A\n",
            " 36% 148/407 [00:01<00:04, 64.56it/s]\u001b[A\n",
            " 38% 155/407 [00:01<00:03, 65.10it/s]\u001b[A\n",
            " 41% 165/407 [00:02<00:03, 72.80it/s]\u001b[A\n",
            " 43% 174/407 [00:02<00:03, 76.68it/s]\u001b[A\n",
            " 45% 182/407 [00:02<00:02, 75.40it/s]\u001b[A\n",
            " 47% 190/407 [00:02<00:03, 71.95it/s]\u001b[A\n",
            " 49% 198/407 [00:02<00:02, 73.00it/s]\u001b[A\n",
            " 51% 206/407 [00:02<00:02, 74.62it/s]\u001b[A\n",
            " 53% 214/407 [00:02<00:02, 74.10it/s]\u001b[A\n",
            " 55% 224/407 [00:02<00:02, 79.73it/s]\u001b[A\n",
            " 57% 232/407 [00:02<00:02, 76.55it/s]\u001b[A\n",
            " 59% 242/407 [00:03<00:02, 81.16it/s]\u001b[A\n",
            " 63% 255/407 [00:03<00:01, 94.52it/s]\u001b[A\n",
            " 66% 268/407 [00:03<00:01, 102.92it/s]\u001b[A\n",
            " 69% 279/407 [00:03<00:01, 90.70it/s] \u001b[A\n",
            " 71% 289/407 [00:03<00:01, 90.54it/s]\u001b[A\n",
            " 73% 299/407 [00:03<00:01, 81.82it/s]\u001b[A\n",
            " 76% 311/407 [00:03<00:01, 89.43it/s]\u001b[A\n",
            " 79% 321/407 [00:03<00:01, 81.91it/s]\u001b[A\n",
            " 81% 330/407 [00:04<00:01, 68.02it/s]\u001b[A\n",
            " 83% 338/407 [00:04<00:00, 69.45it/s]\u001b[A\n",
            " 85% 347/407 [00:04<00:00, 73.00it/s]\u001b[A\n",
            " 87% 355/407 [00:04<00:00, 73.09it/s]\u001b[A\n",
            " 89% 363/407 [00:04<00:00, 68.74it/s]\u001b[A\n",
            " 91% 371/407 [00:04<00:00, 69.90it/s]\u001b[A\n",
            " 93% 379/407 [00:04<00:00, 70.27it/s]\u001b[A\n",
            " 95% 387/407 [00:04<00:00, 68.49it/s]\u001b[A\n",
            " 97% 394/407 [00:05<00:00, 67.23it/s]\u001b[A\n",
            " 99% 401/407 [00:05<00:00, 66.88it/s]\u001b[A07/25/2021 06:44:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.11870164424180984, 'eval_LOC_precision': 0.9678411633109619, 'eval_LOC_recall': 0.9521320495185694, 'eval_LOC_f1': 0.959922340868118, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.8670595939751146, 'eval_MISC_recall': 0.8945945945945946, 'eval_MISC_f1': 0.8806119055537079, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.9072506440927494, 'eval_ORG_recall': 0.9122871946706144, 'eval_ORG_f1': 0.909761948699022, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9425219941348973, 'eval_PER_recall': 0.9654550916191048, 'eval_PER_f1': 0.9538507196913488, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9317898486197684, 'eval_overall_recall': 0.9388121299120761, 'eval_overall_f1': 0.9352878083661064, 'eval_overall_accuracy': 0.9833107670571614, 'eval_runtime': 6.8683, 'eval_samples_per_second': 473.186, 'eval_steps_per_second': 59.257, 'epoch': 8.0}\n",
            " 80% 14048/17560 [14:04<03:16, 17.89it/s]\n",
            "100% 407/407 [00:06<00:00, 66.88it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:44:28,213 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-14048\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:44:28,214 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-14048/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:44:28,787 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-14048/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:44:28,787 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-14048/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:44:28,787 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-14048/special_tokens_map.json\n",
            "{'loss': 0.0009, 'learning_rate': 5e-06, 'epoch': 9.0}\n",
            " 90% 15804/17560 [15:42<01:39, 17.58it/s][INFO|trainer.py:522] 2021-07-25 06:46:06,387 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:46:06,390 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:46:06,390 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:46:06,390 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 11/407 [00:00<00:04, 98.11it/s]\u001b[A\n",
            "  5% 21/407 [00:00<00:04, 92.66it/s]\u001b[A\n",
            "  8% 31/407 [00:00<00:04, 91.77it/s]\u001b[A\n",
            " 10% 41/407 [00:00<00:04, 89.77it/s]\u001b[A\n",
            " 12% 50/407 [00:00<00:04, 84.98it/s]\u001b[A\n",
            " 15% 62/407 [00:00<00:03, 94.90it/s]\u001b[A\n",
            " 18% 73/407 [00:00<00:03, 96.14it/s]\u001b[A\n",
            " 20% 83/407 [00:00<00:03, 94.08it/s]\u001b[A\n",
            " 23% 93/407 [00:01<00:03, 84.71it/s]\u001b[A\n",
            " 25% 102/407 [00:01<00:03, 78.82it/s]\u001b[A\n",
            " 27% 111/407 [00:01<00:03, 75.22it/s]\u001b[A\n",
            " 29% 119/407 [00:01<00:03, 73.56it/s]\u001b[A\n",
            " 31% 127/407 [00:01<00:04, 69.22it/s]\u001b[A\n",
            " 33% 134/407 [00:01<00:04, 66.44it/s]\u001b[A\n",
            " 35% 141/407 [00:01<00:04, 65.67it/s]\u001b[A\n",
            " 36% 148/407 [00:01<00:04, 64.63it/s]\u001b[A\n",
            " 38% 155/407 [00:02<00:03, 64.92it/s]\u001b[A\n",
            " 41% 165/407 [00:02<00:03, 72.53it/s]\u001b[A\n",
            " 43% 174/407 [00:02<00:03, 75.96it/s]\u001b[A\n",
            " 45% 182/407 [00:02<00:02, 75.55it/s]\u001b[A\n",
            " 47% 190/407 [00:02<00:03, 71.61it/s]\u001b[A\n",
            " 49% 198/407 [00:02<00:02, 71.61it/s]\u001b[A\n",
            " 51% 206/407 [00:02<00:02, 72.95it/s]\u001b[A\n",
            " 53% 214/407 [00:02<00:02, 72.40it/s]\u001b[A\n",
            " 55% 224/407 [00:02<00:02, 78.14it/s]\u001b[A\n",
            " 57% 232/407 [00:03<00:02, 75.21it/s]\u001b[A\n",
            " 59% 242/407 [00:03<00:02, 79.77it/s]\u001b[A\n",
            " 63% 255/407 [00:03<00:01, 92.64it/s]\u001b[A\n",
            " 66% 268/407 [00:03<00:01, 101.93it/s]\u001b[A\n",
            " 69% 279/407 [00:03<00:01, 88.95it/s] \u001b[A\n",
            " 71% 289/407 [00:03<00:01, 88.92it/s]\u001b[A\n",
            " 73% 299/407 [00:03<00:01, 80.68it/s]\u001b[A\n",
            " 76% 310/407 [00:03<00:01, 87.23it/s]\u001b[A\n",
            " 79% 320/407 [00:03<00:01, 81.37it/s]\u001b[A\n",
            " 81% 329/407 [00:04<00:01, 67.07it/s]\u001b[A\n",
            " 83% 337/407 [00:04<00:01, 68.68it/s]\u001b[A\n",
            " 85% 346/407 [00:04<00:00, 71.85it/s]\u001b[A\n",
            " 87% 354/407 [00:04<00:00, 72.59it/s]\u001b[A\n",
            " 89% 362/407 [00:04<00:00, 69.50it/s]\u001b[A\n",
            " 91% 370/407 [00:04<00:00, 68.59it/s]\u001b[A\n",
            " 93% 378/407 [00:04<00:00, 69.46it/s]\u001b[A\n",
            " 95% 386/407 [00:05<00:00, 67.37it/s]\u001b[A\n",
            " 97% 393/407 [00:05<00:00, 67.39it/s]\u001b[A\n",
            " 98% 400/407 [00:05<00:00, 64.47it/s]\u001b[A07/25/2021 06:46:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.12460020929574966, 'eval_LOC_precision': 0.9648101967303963, 'eval_LOC_recall': 0.9579092159559834, 'eval_LOC_f1': 0.9613473219215903, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.8809840425531915, 'eval_MISC_recall': 0.8952702702702703, 'eval_MISC_f1': 0.8880697050938339, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.905074844833881, 'eval_ORG_recall': 0.9174685418208735, 'eval_ORG_f1': 0.9112295533909208, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9452338661930136, 'eval_PER_recall': 0.9591468909582457, 'eval_PER_f1': 0.9521395556880872, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9331255565449689, 'eval_overall_recall': 0.9401579041808721, 'eval_overall_f1': 0.9366285305684664, 'eval_overall_accuracy': 0.9834137870135986, 'eval_runtime': 7.0786, 'eval_samples_per_second': 459.133, 'eval_steps_per_second': 57.498, 'epoch': 9.0}\n",
            " 90% 15804/17560 [15:49<01:39, 17.58it/s]\n",
            "100% 407/407 [00:07<00:00, 64.47it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:46:13,475 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-15804\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:46:13,476 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-15804/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:46:14,131 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-15804/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:46:14,132 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-15804/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:46:14,132 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-15804/special_tokens_map.json\n",
            "{'loss': 0.0007, 'learning_rate': 0.0, 'epoch': 10.0}\n",
            "100% 17560/17560 [17:27<00:00, 17.89it/s][INFO|trainer.py:522] 2021-07-25 06:47:51,613 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:47:51,615 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:47:51,616 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:47:51,616 >>   Batch size = 8\n",
            "\n",
            "  0% 0/407 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 11/407 [00:00<00:04, 98.85it/s]\u001b[A\n",
            "  5% 21/407 [00:00<00:04, 91.38it/s]\u001b[A\n",
            "  8% 31/407 [00:00<00:04, 89.71it/s]\u001b[A\n",
            " 10% 40/407 [00:00<00:04, 89.00it/s]\u001b[A\n",
            " 12% 49/407 [00:00<00:04, 84.57it/s]\u001b[A\n",
            " 15% 61/407 [00:00<00:03, 93.89it/s]\u001b[A\n",
            " 18% 72/407 [00:00<00:03, 98.45it/s]\u001b[A\n",
            " 20% 82/407 [00:00<00:03, 94.75it/s]\u001b[A\n",
            " 23% 92/407 [00:01<00:03, 86.21it/s]\u001b[A\n",
            " 25% 101/407 [00:01<00:03, 79.60it/s]\u001b[A\n",
            " 27% 110/407 [00:01<00:03, 76.28it/s]\u001b[A\n",
            " 29% 118/407 [00:01<00:03, 75.04it/s]\u001b[A\n",
            " 31% 126/407 [00:01<00:03, 70.82it/s]\u001b[A\n",
            " 33% 134/407 [00:01<00:04, 66.38it/s]\u001b[A\n",
            " 35% 141/407 [00:01<00:04, 65.91it/s]\u001b[A\n",
            " 36% 148/407 [00:01<00:03, 65.32it/s]\u001b[A\n",
            " 38% 155/407 [00:01<00:03, 65.88it/s]\u001b[A\n",
            " 40% 164/407 [00:02<00:03, 72.12it/s]\u001b[A\n",
            " 42% 172/407 [00:02<00:03, 74.31it/s]\u001b[A\n",
            " 44% 181/407 [00:02<00:02, 78.18it/s]\u001b[A\n",
            " 46% 189/407 [00:02<00:03, 72.30it/s]\u001b[A\n",
            " 48% 197/407 [00:02<00:02, 72.37it/s]\u001b[A\n",
            " 50% 205/407 [00:02<00:02, 74.27it/s]\u001b[A\n",
            " 52% 213/407 [00:02<00:02, 72.39it/s]\u001b[A\n",
            " 55% 223/407 [00:02<00:02, 78.15it/s]\u001b[A\n",
            " 57% 231/407 [00:02<00:02, 77.01it/s]\u001b[A\n",
            " 59% 240/407 [00:03<00:02, 79.68it/s]\u001b[A\n",
            " 62% 251/407 [00:03<00:01, 88.11it/s]\u001b[A\n",
            " 65% 265/407 [00:03<00:01, 101.52it/s]\u001b[A\n",
            " 68% 276/407 [00:03<00:01, 88.24it/s] \u001b[A\n",
            " 70% 286/407 [00:03<00:01, 90.87it/s]\u001b[A\n",
            " 73% 296/407 [00:03<00:01, 85.42it/s]\u001b[A\n",
            " 75% 305/407 [00:03<00:01, 86.53it/s]\u001b[A\n",
            " 77% 315/407 [00:03<00:01, 88.82it/s]\u001b[A\n",
            " 80% 325/407 [00:04<00:01, 75.14it/s]\u001b[A\n",
            " 82% 333/407 [00:04<00:01, 67.35it/s]\u001b[A\n",
            " 84% 342/407 [00:04<00:00, 71.18it/s]\u001b[A\n",
            " 86% 350/407 [00:04<00:00, 73.09it/s]\u001b[A\n",
            " 88% 358/407 [00:04<00:00, 72.04it/s]\u001b[A\n",
            " 90% 366/407 [00:04<00:00, 69.59it/s]\u001b[A\n",
            " 92% 374/407 [00:04<00:00, 70.31it/s]\u001b[A\n",
            " 94% 382/407 [00:04<00:00, 70.21it/s]\u001b[A\n",
            " 96% 390/407 [00:05<00:00, 67.84it/s]\u001b[A\n",
            " 98% 397/407 [00:05<00:00, 65.39it/s]\u001b[A\n",
            " 99% 404/407 [00:05<00:00, 65.79it/s]\u001b[A07/25/2021 06:47:58 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.12733404338359833, 'eval_LOC_precision': 0.9606277533039648, 'eval_LOC_recall': 0.9598349381017882, 'eval_LOC_f1': 0.9602311820558691, 'eval_LOC_number': 3635, 'eval_MISC_precision': 0.8779840848806366, 'eval_MISC_recall': 0.8945945945945946, 'eval_MISC_f1': 0.8862115127175367, 'eval_MISC_number': 1480, 'eval_ORG_precision': 0.9078610603290677, 'eval_ORG_recall': 0.9189489267209474, 'eval_ORG_f1': 0.9133713444914475, 'eval_ORG_number': 2702, 'eval_PER_precision': 0.9560770156438027, 'eval_PER_recall': 0.9546410333433464, 'eval_PER_f1': 0.9553584848940327, 'eval_PER_number': 3329, 'eval_overall_precision': 0.9352620769711582, 'eval_overall_recall': 0.9397093127579401, 'eval_overall_f1': 0.9374804206757664, 'eval_overall_accuracy': 0.9836639783363749, 'eval_runtime': 6.9149, 'eval_samples_per_second': 469.999, 'eval_steps_per_second': 58.858, 'epoch': 10.0}\n",
            "100% 17560/17560 [17:34<00:00, 17.89it/s]\n",
            "100% 407/407 [00:06<00:00, 65.79it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1919] 2021-07-25 06:47:58,533 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-17560\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:47:58,534 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-17560/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:47:59,190 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-17560/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:47:59,190 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-17560/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:47:59,190 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/checkpoint-17560/special_tokens_map.json\n",
            "[INFO|trainer.py:1360] 2021-07-25 06:48:01,056 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1057.423, 'train_samples_per_second': 132.785, 'train_steps_per_second': 16.606, 'train_loss': 0.027368284195050562, 'epoch': 10.0}\n",
            "100% 17560/17560 [17:37<00:00, 16.61it/s]\n",
            "[INFO|trainer.py:1919] 2021-07-25 06:48:01,058 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conllpp-english_pt\n",
            "[INFO|configuration_utils.py:379] 2021-07-25 06:48:01,059 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/config.json\n",
            "[INFO|modeling_utils.py:997] 2021-07-25 06:48:02,163 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2021-07-25 06:48:02,165 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2021-07-25 06:48:02,166 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_pt/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.0274\n",
            "  train_runtime            = 0:17:37.42\n",
            "  train_samples            =      14041\n",
            "  train_samples_per_second =    132.785\n",
            "  train_steps_per_second   =     16.606\n",
            "07/25/2021 06:48:02 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:522] 2021-07-25 06:48:02,204 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:48:02,206 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:48:02,206 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:48:02,206 >>   Batch size = 8\n",
            " 99% 404/407 [00:05<00:00, 67.14it/s]07/25/2021 06:48:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "100% 407/407 [00:06<00:00, 58.90it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_LOC_f1             =     0.9602\n",
            "  eval_LOC_number         =       3635\n",
            "  eval_LOC_precision      =     0.9606\n",
            "  eval_LOC_recall         =     0.9598\n",
            "  eval_MISC_f1            =     0.8862\n",
            "  eval_MISC_number        =       1480\n",
            "  eval_MISC_precision     =      0.878\n",
            "  eval_MISC_recall        =     0.8946\n",
            "  eval_ORG_f1             =     0.9134\n",
            "  eval_ORG_number         =       2702\n",
            "  eval_ORG_precision      =     0.9079\n",
            "  eval_ORG_recall         =     0.9189\n",
            "  eval_PER_f1             =     0.9554\n",
            "  eval_PER_number         =       3329\n",
            "  eval_PER_precision      =     0.9561\n",
            "  eval_PER_recall         =     0.9546\n",
            "  eval_loss               =     0.1273\n",
            "  eval_overall_accuracy   =     0.9837\n",
            "  eval_overall_f1         =     0.9375\n",
            "  eval_overall_precision  =     0.9353\n",
            "  eval_overall_recall     =     0.9397\n",
            "  eval_runtime            = 0:00:06.92\n",
            "  eval_samples            =       3250\n",
            "  eval_samples_per_second =    468.979\n",
            "  eval_steps_per_second   =     58.731\n",
            "07/25/2021 06:48:09 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:522] 2021-07-25 06:48:09,141 >> The following columns in the test set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
            "[INFO|trainer.py:2165] 2021-07-25 06:48:09,165 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2167] 2021-07-25 06:48:09,165 >>   Num examples = 3453\n",
            "[INFO|trainer.py:2170] 2021-07-25 06:48:09,165 >>   Batch size = 8\n",
            " 99% 426/432 [00:04<00:00, 103.25it/s]07/25/2021 06:48:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "***** predict metrics *****\n",
            "  predict_LOC_f1             =     0.9239\n",
            "  predict_LOC_number         =       2991\n",
            "  predict_LOC_precision      =     0.9167\n",
            "  predict_LOC_recall         =     0.9311\n",
            "  predict_MISC_f1            =     0.7755\n",
            "  predict_MISC_number        =       1319\n",
            "  predict_MISC_precision     =     0.7995\n",
            "  predict_MISC_recall        =     0.7528\n",
            "  predict_ORG_f1             =     0.8992\n",
            "  predict_ORG_number         =       3629\n",
            "  predict_ORG_precision      =     0.8942\n",
            "  predict_ORG_recall         =     0.9041\n",
            "  predict_PER_f1             =      0.937\n",
            "  predict_PER_number         =       3006\n",
            "  predict_PER_precision      =     0.9458\n",
            "  predict_PER_recall         =     0.9285\n",
            "  predict_loss               =     0.2619\n",
            "  predict_overall_accuracy   =     0.9714\n",
            "  predict_overall_f1         =     0.9018\n",
            "  predict_overall_precision  =     0.9037\n",
            "  predict_overall_recall     =        0.9\n",
            "  predict_runtime            = 0:00:06.73\n",
            "  predict_samples_per_second =    512.896\n",
            "  predict_steps_per_second   =     64.168\n",
            "100% 432/432 [00:07<00:00, 61.41it/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kye944GmmZd"
      },
      "source": [
        "\"\"\"\n",
        "***** train metrics *****\n",
        "  epoch                    =       10.0\n",
        "  train_loss               =     0.0274\n",
        "  train_runtime            = 0:17:37.42\n",
        "  train_samples            =      14041\n",
        "  train_samples_per_second =    132.785\n",
        "  train_steps_per_second   =     16.606\n",
        "07/25/2021 06:48:02 - INFO - __main__ - *** Evaluate ***\n",
        "[INFO|trainer.py:522] 2021-07-25 06:48:02,204 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
        "[INFO|trainer.py:2165] 2021-07-25 06:48:02,206 >> ***** Running Evaluation *****\n",
        "[INFO|trainer.py:2167] 2021-07-25 06:48:02,206 >>   Num examples = 3250\n",
        "[INFO|trainer.py:2170] 2021-07-25 06:48:02,206 >>   Batch size = 8\n",
        " 99% 404/407 [00:05<00:00, 67.14it/s]07/25/2021 06:48:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
        "100% 407/407 [00:06<00:00, 58.90it/s]\n",
        "***** eval metrics *****\n",
        "  epoch                   =       10.0\n",
        "  eval_LOC_f1             =     0.9602\n",
        "  eval_LOC_number         =       3635\n",
        "  eval_LOC_precision      =     0.9606\n",
        "  eval_LOC_recall         =     0.9598\n",
        "  eval_MISC_f1            =     0.8862\n",
        "  eval_MISC_number        =       1480\n",
        "  eval_MISC_precision     =      0.878\n",
        "  eval_MISC_recall        =     0.8946\n",
        "  eval_ORG_f1             =     0.9134\n",
        "  eval_ORG_number         =       2702\n",
        "  eval_ORG_precision      =     0.9079\n",
        "  eval_ORG_recall         =     0.9189\n",
        "  eval_PER_f1             =     0.9554\n",
        "  eval_PER_number         =       3329\n",
        "  eval_PER_precision      =     0.9561\n",
        "  eval_PER_recall         =     0.9546\n",
        "  eval_loss               =     0.1273\n",
        "  eval_overall_accuracy   =     0.9837\n",
        "  eval_overall_f1         =     0.9375\n",
        "  eval_overall_precision  =     0.9353\n",
        "  eval_overall_recall     =     0.9397\n",
        "  eval_runtime            = 0:00:06.92\n",
        "  eval_samples            =       3250\n",
        "  eval_samples_per_second =    468.979\n",
        "  eval_steps_per_second   =     58.731\n",
        "07/25/2021 06:48:09 - INFO - __main__ - *** Predict ***\n",
        "[INFO|trainer.py:522] 2021-07-25 06:48:09,141 >> The following columns in the test set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, ner_tags, pos_tags, id.\n",
        "[INFO|trainer.py:2165] 2021-07-25 06:48:09,165 >> ***** Running Prediction *****\n",
        "[INFO|trainer.py:2167] 2021-07-25 06:48:09,165 >>   Num examples = 3453\n",
        "[INFO|trainer.py:2170] 2021-07-25 06:48:09,165 >>   Batch size = 8\n",
        " 99% 426/432 [00:04<00:00, 103.25it/s]07/25/2021 06:48:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
        "***** predict metrics *****\n",
        "  predict_LOC_f1             =     0.9239\n",
        "  predict_LOC_number         =       2991\n",
        "  predict_LOC_precision      =     0.9167\n",
        "  predict_LOC_recall         =     0.9311\n",
        "  predict_MISC_f1            =     0.7755\n",
        "  predict_MISC_number        =       1319\n",
        "  predict_MISC_precision     =     0.7995\n",
        "  predict_MISC_recall        =     0.7528\n",
        "  predict_ORG_f1             =     0.8992\n",
        "  predict_ORG_number         =       3629\n",
        "  predict_ORG_precision      =     0.8942\n",
        "  predict_ORG_recall         =     0.9041\n",
        "  predict_PER_f1             =      0.937\n",
        "  predict_PER_number         =       3006\n",
        "  predict_PER_precision      =     0.9458\n",
        "  predict_PER_recall         =     0.9285\n",
        "  predict_loss               =     0.2619\n",
        "  predict_overall_accuracy   =     0.9714\n",
        "  predict_overall_f1         =     0.9018\n",
        "  predict_overall_precision  =     0.9037\n",
        "  predict_overall_recall     =        0.9\n",
        "  predict_runtime            = 0:00:06.73\n",
        "  predict_samples_per_second =    512.896\n",
        "  predict_steps_per_second   =     64.168\n",
        "100% 432/432 [00:07<00:00, 61.41it/s] \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np6obh3fGfGw"
      },
      "source": [
        "### 2. Inference using pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoNgwOCTV6YL",
        "outputId": "30277779-1673-43bd-dc8c-101b9efa4426"
      },
      "source": [
        "!ls /tmp/distilbert-base-cased-finetuned-conllpp-english_pt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_results.json  checkpoint-5268\truns\n",
            "checkpoint-10536  checkpoint-7024\tspecial_tokens_map.json\n",
            "checkpoint-12292  checkpoint-8780\ttokenizer_config.json\n",
            "checkpoint-14048  config.json\t\ttokenizer.json\n",
            "checkpoint-15804  eval_results.json\ttrainer_state.json\n",
            "checkpoint-1756   predictions.txt\ttraining_args.bin\n",
            "checkpoint-17560  predict_results.json\ttrain_results.json\n",
            "checkpoint-3512   pytorch_model.bin\tvocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kg3EH1Iav3v"
      },
      "source": [
        "# Update NER label2id and id2label in model config file: config.json\n",
        "\"\"\"\n",
        "  \"id2label\": {\n",
        "    \"0\": \"O\",\n",
        "    \"1\": \"B-PER\",\n",
        "    \"2\": \"i-PER\",\n",
        "    \"3\": \"B-ORG\",\n",
        "    \"4\": \"I-ORG\",\n",
        "    \"5\": \"B-LOC\",\n",
        "    \"6\": \"I-LOC\",\n",
        "    \"7\": \"B-MISC\",\n",
        "    \"8\": \"I-MISC\"\n",
        "  },\n",
        "\n",
        "\n",
        "  \"label2id\": {\n",
        "    \"B-LOC\": 5,\n",
        "    \"B-MISC\": 7,\n",
        "    \"B-ORG\": 3,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-MISC\": 8,\n",
        "    \"I-ORG\": 4,\n",
        "    \"I-PER\": 2,\n",
        "    \"O\": 0\n",
        "  },\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUeIUVJGSMc"
      },
      "source": [
        "# restart runtime if having problem importing pipeline\n",
        "from transformers import pipeline\n",
        "distilbert_ner = pipeline('ner', model=\"/tmp/distilbert-base-cased-finetuned-conllpp-english_pt/\", tokenizer=\"distilbert-base-cased\", aggregation_strategy='first')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8mDhsXIWol2"
      },
      "source": [
        "article = \"\"\"\n",
        "KUALA LUMPUR - Malaysian Prime Minister Muhyiddin Yassin's party said on Thursday (July 8) that his government would continue to function despite Umno withdrawing its backing. \n",
        "Amid uncertainty over whether Tan Sri Muhyiddin continues to command majority support without Umno, the largest party in the Perikatan Nasional (PN) ruling pact, \n",
        "his Parti Pribumi Bersatu Malaysia said Umno's decision \"had no effect on the workings of government\".\"\"\"\n",
        "\n",
        "\n",
        "results = distilbert_ner(article)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXb3kWwIXMvw"
      },
      "source": [
        "print(\"Predicted:\")\n",
        "for tag in results:\n",
        "    print(f\"{tag['entity_group']:<5} as {tag['word']}\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Output\n",
        "------\n",
        "Predicted:\n",
        "ORG   as KUALA LUMPUR\n",
        "MISC  as -\n",
        "MISC  as Malaysian\n",
        "PER   as Muhyiddin\n",
        "PER   as Yassin\n",
        "PER   as Umno\n",
        "PER   as Amid\n",
        "PER   as Tan\n",
        "PER   as Sri\n",
        "PER   as Muhyiddin\n",
        "ORG   as Umno\n",
        "ORG   as Perikatan Nasional\n",
        "ORG   as PN\n",
        "ORG   as Parti Pribumi Bersatu Malaysia\n",
        "PER   as Umno\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}