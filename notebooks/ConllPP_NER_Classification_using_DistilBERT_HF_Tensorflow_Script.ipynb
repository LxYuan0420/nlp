{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConllPP NER Classification using DistilBERT HF Tensorflow Script.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O21tI9CjAXIc"
      },
      "source": [
        "### 0. Install and load library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyZGa3XuFX96"
      },
      "source": [
        "#Restart kernel after installation: Runtime -> Restart runtime\n",
        "\n",
        "#!pip install -U sentencepiece datasets\n",
        "#!pip install -U transformers sentencepiece datasets seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIcCT1VkqhlO"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_HQ2yqlCDA"
      },
      "source": [
        "### 1. Fine-tuning DistillBERT model on CONLLPP NER dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYP1WvLnoQAl",
        "outputId": "6f840077-ac13-41d5-c5b8-bd35ee6c0428"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/tensorflow/token-classification/run_ner.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-24 11:19:42--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/tensorflow/token-classification/run_ner.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22263 (22K) [text/plain]\n",
            "Saving to: ‘run_ner.py’\n",
            "\n",
            "run_ner.py          100%[===================>]  21.74K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-07-24 11:19:42 (20.4 MB/s) - ‘run_ner.py’ saved [22263/22263]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB_DGmdeTFDE"
      },
      "source": [
        "\"\"\"\n",
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"conllpp\")\n",
        "dataset['train'].features['ner_tags']\n",
        "\n",
        ">> Sequence(feature=ClassLabel(\n",
        "    num_classes=9, \n",
        "    names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], \n",
        "    names_file=None, id=None),\n",
        "    length=-1, \n",
        "    id=None)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP_E2Z9FDkcR",
        "outputId": "77aee338-d2f4-47a5-e75a-0e81535b2d5c"
      },
      "source": [
        "!python /content/run_ner.py \\\n",
        "  --model_name_or_path distilbert-base-cased \\\n",
        "  --label_all_tokens True \\\n",
        "  --return_entity_level_metrics True \\\n",
        "  --dataset_name conllpp \\\n",
        "  --output_dir /tmp/distilbert-base-cased-finetuned-conllpp-english_tf \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --save_strategy epoch \\\n",
        "  --logging_strategy epoch \\\n",
        "  --num_train_epochs 10"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-24 11:38:30.238428: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Reusing dataset conllpp (/root/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.9.0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.9.0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.9.0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset: 100% 15/15 [00:01<00:00,  8.85ba/s]\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00, 10.97ba/s]\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00, 11.95ba/s]\n",
            "Sample 10476 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [101, 140, 11607, 19747, 2249, 11185, 21669, 13020, 18732, 2162, 9565, 14569, 2346, 102], 'labels': [-100, 3, 3, 3, 3, 3, 3, 0, 5, 5, 5, 5, 5, -100]}.\n",
            "Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [101, 9269, 1114, 2733, 117, 1134, 1110, 1412, 1514, 3547, 117, 1138, 1632, 4495, 117, 107, 23209, 1732, 1918, 1163, 119, 102], 'labels': [-100, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, -100]}.\n",
            "Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [101, 1124, 1896, 131, 107, 1409, 1185, 1141, 1455, 117, 146, 1309, 1533, 1139, 1779, 119, 102], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}.\n",
            "Tensorflow: setting up strategy\n",
            "2021-07-24 11:38:38.061140: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-24 11:38:38.061314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.061974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-07-24 11:38:38.062021: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-24 11:38:38.063999: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-24 11:38:38.064096: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-07-24 11:38:38.065564: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-24 11:38:38.065902: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-24 11:38:38.067515: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-24 11:38:38.068227: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-07-24 11:38:38.068433: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-07-24 11:38:38.068543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.069173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.069703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-24 11:38:38.070372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.070938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-07-24 11:38:38.071019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.071583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.072124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-24 11:38:38.072183: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-24 11:38:38.664163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-24 11:38:38.664211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-07-24 11:38:38.664224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-07-24 11:38:38.664428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.665145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.665949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-24 11:38:38.666504: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-24 11:38:38.666557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/fe773335fbb46b412a9093627b6c3235a69c55bad3bd1deee40813cd0a8d0a82.33c483181ffc4c7cbdd0b733245bcc9b479f14f3b2e892f635fe03f4f3a41495.h5\n",
            "2021-07-24 11:38:38.970416: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "2021-07-24 11:38:39.134046: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-24 11:38:39.638510: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForTokenClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier', 'dropout_19']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 14041\n",
            "  Num Epochs = 10.0\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size = 8\n",
            "2021-07-24 11:38:40.857148: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-07-24 11:38:40.857518: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
            "  return py_builtins.overload_of(f)(*args)\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1755/1755 [==============================] - ETA: 0s - loss: 0.1568 - loss_loss: 0.1568WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "1755/1755 [==============================] - 125s 66ms/step - loss: 0.1568 - loss_loss: 0.1568 - val_loss: 0.0888 - val_loss_loss: 0.0888\n",
            "Epoch 2/10\n",
            "1755/1755 [==============================] - 115s 65ms/step - loss: 0.0577 - loss_loss: 0.0577 - val_loss: 0.0714 - val_loss_loss: 0.0714\n",
            "Epoch 3/10\n",
            "1755/1755 [==============================] - 115s 65ms/step - loss: 0.0295 - loss_loss: 0.0295 - val_loss: 0.0709 - val_loss_loss: 0.0709\n",
            "Epoch 4/10\n",
            "1755/1755 [==============================] - 115s 65ms/step - loss: 0.0174 - loss_loss: 0.0174 - val_loss: 0.0844 - val_loss_loss: 0.0844\n",
            "Epoch 5/10\n",
            "1755/1755 [==============================] - 115s 66ms/step - loss: 0.0110 - loss_loss: 0.0110 - val_loss: 0.0833 - val_loss_loss: 0.0833\n",
            "Epoch 6/10\n",
            "1755/1755 [==============================] - 115s 65ms/step - loss: 0.0069 - loss_loss: 0.0069 - val_loss: 0.0931 - val_loss_loss: 0.0931\n",
            "Epoch 7/10\n",
            "1755/1755 [==============================] - 115s 65ms/step - loss: 0.0049 - loss_loss: 0.0049 - val_loss: 0.0967 - val_loss_loss: 0.0967\n",
            "Epoch 8/10\n",
            "1755/1755 [==============================] - 115s 65ms/step - loss: 0.0028 - loss_loss: 0.0028 - val_loss: 0.1033 - val_loss_loss: 0.1033\n",
            "Epoch 9/10\n",
            "1755/1755 [==============================] - 115s 65ms/step - loss: 0.0017 - loss_loss: 0.0017 - val_loss: 0.0983 - val_loss_loss: 0.0983\n",
            "Epoch 10/10\n",
            "1755/1755 [==============================] - 115s 66ms/step - loss: 9.3707e-04 - loss_loss: 9.3707e-04 - val_loss: 0.1005 - val_loss_loss: 0.1005\n",
            "2021-07-24 11:57:59.196861: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"PrefetchDataset/_8\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_slice_batch_indices_577803\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Evaluation metrics:\n",
            "LOC_precision: 0.9689\n",
            "LOC_recall: 0.9527\n",
            "LOC_f1: 0.9607\n",
            "LOC_number: 3635.0000\n",
            "MISC_precision: 0.8724\n",
            "MISC_recall: 0.8919\n",
            "MISC_f1: 0.8821\n",
            "MISC_number: 1480.0000\n",
            "ORG_precision: 0.9130\n",
            "ORG_recall: 0.9171\n",
            "ORG_f1: 0.9151\n",
            "ORG_number: 2702.0000\n",
            "PER_precision: 0.9438\n",
            "PER_recall: 0.9631\n",
            "PER_f1: 0.9533\n",
            "PER_number: 3329.0000\n",
            "overall_precision: 0.9347\n",
            "overall_recall: 0.9391\n",
            "overall_f1: 0.9369\n",
            "overall_accuracy: 0.9841\n",
            "Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_tf/config.json\n",
            "Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_tf/tf_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kye944GmmZd"
      },
      "source": [
        "\"\"\"\n",
        "Evaluation metrics:\n",
        "LOC_precision: 0.9689\n",
        "LOC_recall: 0.9527\n",
        "LOC_f1: 0.9607\n",
        "LOC_number: 3635.0000\n",
        "MISC_precision: 0.8724\n",
        "MISC_recall: 0.8919\n",
        "MISC_f1: 0.8821\n",
        "MISC_number: 1480.0000\n",
        "ORG_precision: 0.9130\n",
        "ORG_recall: 0.9171\n",
        "ORG_f1: 0.9151\n",
        "ORG_number: 2702.0000\n",
        "PER_precision: 0.9438\n",
        "PER_recall: 0.9631\n",
        "PER_f1: 0.9533\n",
        "PER_number: 3329.0000\n",
        "overall_precision: 0.9347\n",
        "overall_recall: 0.9391\n",
        "overall_f1: 0.9369\n",
        "overall_accuracy: 0.9841\n",
        "Configuration saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_tf/config.json\n",
        "Model weights saved in /tmp/distilbert-base-cased-finetuned-conllpp-english_tf/tf_model.h5\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np6obh3fGfGw"
      },
      "source": [
        "### 2. Inference using pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoNgwOCTV6YL",
        "outputId": "96388ebb-5d0a-4942-9ee2-f523090661de"
      },
      "source": [
        "!ls /tmp/distilbert-base-cased-finetuned-conllpp-english_tf"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config.json  tf_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kg3EH1Iav3v"
      },
      "source": [
        "# Update NER label2id and id2label in model config file: config.json\n",
        "\"\"\"\n",
        "  \"id2label\": {\n",
        "    \"0\": \"O\",\n",
        "    \"1\": \"B-PER\",\n",
        "    \"2\": \"i-PER\",\n",
        "    \"3\": \"B-ORG\",\n",
        "    \"4\": \"I-ORG\",\n",
        "    \"5\": \"B-LOC\",\n",
        "    \"6\": \"I-LOC\",\n",
        "    \"7\": \"B-MISC\",\n",
        "    \"8\": \"I-MISC\"\n",
        "  },\n",
        "\n",
        "\n",
        "  \"label2id\": {\n",
        "    \"B-LOC\": 5,\n",
        "    \"B-MISC\": 7,\n",
        "    \"B-ORG\": 3,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-MISC\": 8,\n",
        "    \"I-ORG\": 4,\n",
        "    \"I-PER\": 2,\n",
        "    \"O\": 0\n",
        "  },\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUeIUVJGSMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f2ae89-cf76-48a3-c711-26ceb6d96c54"
      },
      "source": [
        "from transformers import pipeline\n",
        "distilbert_ner = pipeline('ner', model=\"/tmp/distilbert-base-cased-finetuned-conllpp-english_tf/\", tokenizer=\"distilbert-base-cased\", aggregation_strategy='first')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFDistilBertForTokenClassification.\n",
            "\n",
            "All the layers of TFDistilBertForTokenClassification were initialized from the model checkpoint at /tmp/distilbert-base-cased-finetuned-conllpp-english_tf/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForTokenClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8mDhsXIWol2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b8fcdd-f59e-45a2-da5d-0cd6f1152790"
      },
      "source": [
        "article = \"\"\"\n",
        "KUALA LUMPUR - Malaysian Prime Minister Muhyiddin Yassin's party said on Thursday (July 8) that his government would continue to function despite Umno withdrawing its backing. \n",
        "Amid uncertainty over whether Tan Sri Muhyiddin continues to command majority support without Umno, the largest party in the Perikatan Nasional (PN) ruling pact, \n",
        "his Parti Pribumi Bersatu Malaysia said Umno's decision \"had no effect on the workings of government\".\"\"\"\n",
        "\n",
        "\n",
        "results = distilbert_ner(article)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/transformers/pipelines/token_classification.py:218: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.identity instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "bXb3kWwIXMvw",
        "outputId": "def41cc7-5e05-4b83-f16e-166693132737"
      },
      "source": [
        "print(\"Predicted:\")\n",
        "for tag in results:\n",
        "    print(f\"{tag['entity_group']:<5} as {tag['word']}\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Output\n",
        "------\n",
        "Predicted:\n",
        "ORG   as KUALA LUMPUR\n",
        "MISC  as Malaysian\n",
        "PER   as Muhyiddin\n",
        "PER   as Yassin\n",
        "PER   as Umno\n",
        "PER   as Tan\n",
        "PER   as Sri\n",
        "PER   as Muhyiddin\n",
        "ORG   as Umno\n",
        "ORG   as Perikatan Nasional\n",
        "ORG   as PN\n",
        "ORG   as Parti Pribumi Bersatu Malaysia\n",
        "PER   as Umno\n",
        "\"\"\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted:\n",
            "ORG   as KUALA LUMPUR\n",
            "MISC  as Malaysian\n",
            "PER   as Muhyiddin\n",
            "PER   as Yassin\n",
            "PER   as Umno\n",
            "PER   as Tan\n",
            "PER   as Sri\n",
            "PER   as Muhyiddin\n",
            "ORG   as Umno\n",
            "ORG   as Perikatan Nasional\n",
            "ORG   as PN\n",
            "ORG   as Parti Pribumi Bersatu Malaysia\n",
            "PER   as Umno\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nOutput\\n------\\nPredicted:\\nORG   as KUALA LUMPUR\\nMISC  as Malaysian\\nPER   as Muhyiddin\\nPER   as Yassin\\nPER   as Umno\\nPER   as Tan\\nPER   as Sri\\nPER   as Muhyiddin\\nPER   as Umno\\nORG   as Perikatan Nasional\\nORG   as PN\\nORG   as Parti Pribumi Bersatu Malaysia\\nPER   as Umno\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkPQ0qTWvc_c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}