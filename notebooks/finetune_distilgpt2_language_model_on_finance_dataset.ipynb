{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82750bb0-f160-462c-9d02-4ccd78f50559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f6e9b-15d0-43e9-a58d-bbcc16ed2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround when you cant login using notebook_login\n",
    "\n",
    "#from huggingface_hub import interpreter_login\n",
    "#interpreter_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed2101-a822-497c-bf80-d5616eea153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df62579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16GB Tesla T4 is not enough to train EleutherAI/gpt-neo-1.3B\n",
    "# switch ti distilgpt2\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5b69a-456b-497b-b006-02d9f9eb6ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset_1 = load_dataset(\"gbharti/finance-alpaca\")\n",
    "dataset_2 = load_dataset(\"PaulAdversarial/all_news_finance_sm_1h2023\")\n",
    "dataset_3 = load_dataset(\"winddude/reddit_finance_43_250k\")\n",
    "dataset_4 = load_dataset(\"causal-lm/finance\")\n",
    "\n",
    "# create a column called text\n",
    "dataset_1 = dataset_1.map(\n",
    "    lambda example: {\"text\": example[\"instruction\"] + \" \" + example[\"output\"]},\n",
    "    num_proc=4,\n",
    ")\n",
    "dataset_1 = dataset_1.remove_columns([\"input\", \"instruction\", \"output\"])\n",
    "\n",
    "dataset_2 = dataset_2.map(\n",
    "    lambda example: {\"text\": example[\"title\"] + \" \" + example[\"description\"]},\n",
    "    num_proc=4,\n",
    ")\n",
    "dataset_2 = dataset_2.remove_columns(\n",
    "    [\"_id\", \"main_domain\", \"title\", \"description\", \"created_at\"]\n",
    ")\n",
    "\n",
    "dataset_3 = dataset_3.map(\n",
    "    lambda example: {\n",
    "        \"text\": example[\"title\"] + \" \" + example[\"selftext\"] + \" \" + example[\"body\"]\n",
    "    },\n",
    "    num_proc=4,\n",
    ")\n",
    "dataset_3 = dataset_3.remove_columns(\n",
    "    [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"selftext\",\n",
    "        \"z_score\",\n",
    "        \"normalized_score\",\n",
    "        \"subreddit\",\n",
    "        \"body\",\n",
    "        \"comment_normalized_score\",\n",
    "        \"combined_score\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_4 = dataset_4.map(\n",
    "    lambda example: {\"text\": example[\"instruction\"] + \" \" + example[\"output\"]},\n",
    "    num_proc=4,\n",
    ")\n",
    "dataset_4 = dataset_4.remove_columns([\"input\", \"instruction\", \"output\"])\n",
    "\n",
    "# combine and split train test sets\n",
    "combined_dataset = concatenate_datasets(\n",
    "    [\n",
    "        dataset_1[\"train\"],\n",
    "        dataset_2[\"train\"],\n",
    "        dataset_3[\"train\"],\n",
    "        dataset_4[\"train\"],\n",
    "        dataset_4[\"validation\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "datasets = combined_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f22c4c-ab4a-4c54-96e1-087b7affb299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3899988-0f04-4b2e-810c-f12644c8161e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3a918-86af-44f1-b8dd-209398715d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-finance\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=64,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7a1d2-f224-4bc4-a818-95578d99b856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994c947-ec3d-4a85-8e28-38d0d6c20223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b16a80-063c-45ad-9870-08284a2cf42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32943a23-753a-4a5a-844a-4762c1d48ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(f\"{model_name}-finetuned-finance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e215f7d-707d-415a-9395-dd3858797d63",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f3683-f83f-47e7-81ac-bac7b6f41767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model=\"lxyuan/distilgpt2-finetuned-finance\", tokenizer=tokenizer)\n",
    "\n",
    "generator(\"Tesla is\",\n",
    "  pad_token_id=generator.tokenizer.eos_token_id,\n",
    "  max_new_tokens=200,\n",
    "  num_return_sequences=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
