{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvamz55G6sk+gLfvVXOexr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/nlp/blob/main/notebooks/Experimenting_with_TorchChat_in_Colab_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BNCxOeBALy",
        "outputId": "b6797a52-43eb-4373-d7bd-a3e24829a9e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "ECD2pzUjBIMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v9LoZ48IBh-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lmdvRrH4yl0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pytorch/torchchat.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd torchchat\n",
        "#!bash install_requirements.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAzuOyFa5Nq6",
        "outputId": "d1d07adb-ffcf-4bdb-83f1-59b60e4f3020"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/torchchat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade torchvision"
      ],
      "metadata": {
        "id": "i6tKuUTjBjE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview"
      ],
      "metadata": {
        "id": "Qhf83vDlBK-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python torchchat.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtQoSaPw5YTR",
        "outputId": "9488a5d7-34d3-470d-9254-b06d91e8c340"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: torchchat [-h] {chat,generate,browser,export,download,list,remove,where,server,eval} ...\n",
            "\n",
            "positional arguments:\n",
            "  {chat,generate,browser,export,download,list,remove,where,server,eval}\n",
            "                        The specific command to run\n",
            "    chat                Chat interactively with a model via the CLI\n",
            "    generate            Generate responses from a model given a prompt\n",
            "    browser             Chat interactively with a model in a locally hosted browser\n",
            "    export              Export a model artifact to AOT Inductor or ExecuTorch\n",
            "    download            Download model artifacts\n",
            "    list                List all supported models\n",
            "    remove              Remove downloaded model artifacts\n",
            "    where               Return directory containing downloaded model artifacts\n",
            "    server              [WIP] Starts a locally hosted REST server for model interaction\n",
            "    eval                Evaluate a model via lm-eval\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### List"
      ],
      "metadata": {
        "id": "ftsnhFABBMiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python torchchat.py list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qanuhhvA6BRf",
        "outputId": "9633bd95-b6e6-4fad-ea94-09192d75f3bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model                                   Aliases                                     Downloaded \n",
            "--------------------------------------- ------------------------------------------- -----------\n",
            "meta-llama/llama-2-7b-hf                llama2-base, llama2-7b                                 \n",
            "meta-llama/llama-2-7b-chat-hf           llama2, llama2-chat, llama2-7b-chat                    \n",
            "meta-llama/llama-2-13b-chat-hf          llama2-13b-chat                                        \n",
            "meta-llama/llama-2-70b-chat-hf          llama2-70b-chat                                        \n",
            "meta-llama/meta-llama-3-8b              llama3-base                                            \n",
            "meta-llama/meta-llama-3-8b-instruct     llama3, llama3-chat, llama3-instruct                   \n",
            "meta-llama/meta-llama-3-70b-instruct    llama3-70b                                             \n",
            "meta-llama/meta-llama-3.1-8b            llama3.1-base                                          \n",
            "meta-llama/meta-llama-3.1-8b-instruct   llama3.1, llama3.1-chat, llama3.1-instruct             \n",
            "meta-llama/meta-llama-3.1-70b-instruct  llama3.1-70b                                           \n",
            "meta-llama/codellama-7b-python-hf       codellama, codellama-7b                                \n",
            "meta-llama/codellama-34b-python-hf      codellama-34b                                          \n",
            "mistralai/mistral-7b-v0.1               mistral-7b-v01-base                                    \n",
            "mistralai/mistral-7b-instruct-v0.1      mistral-7b-v01-instruct                                \n",
            "mistralai/mistral-7b-instruct-v0.2      mistral, mistral-7b, mistral-7b-instruct               \n",
            "openlm-research/open_llama_7b           open-llama, open-llama-7b                              \n",
            "stories15m                                                                          Yes        \n",
            "stories42m                                                                                     \n",
            "stories110m                                                                                    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download"
      ],
      "metadata": {
        "id": "rV2aXXfOBPWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python torchchat.py download stories15m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkOVCzv66DN-",
        "outputId": "37f2f1ec-35f3-4c51-cda9-1d15a2018837"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.pt...\n",
            "Downloading https://github.com/karpathy/llama2.c/raw/master/tokenizer.model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Where"
      ],
      "metadata": {
        "id": "NXvPjbQxBRUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python torchchat.py where stories15m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvF7C2q8AwW8",
        "outputId": "16a85a2b-4cc3-49b7-8865-60519cacb66f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.torchchat/model-cache/stories15M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python torchchat.py list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWpvWGej6tWH",
        "outputId": "9f054cdf-21db-4660-fc3e-138af917b928"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model                                   Aliases                                     Downloaded \n",
            "--------------------------------------- ------------------------------------------- -----------\n",
            "meta-llama/llama-2-7b-hf                llama2-base, llama2-7b                                 \n",
            "meta-llama/llama-2-7b-chat-hf           llama2, llama2-chat, llama2-7b-chat                    \n",
            "meta-llama/llama-2-13b-chat-hf          llama2-13b-chat                                        \n",
            "meta-llama/llama-2-70b-chat-hf          llama2-70b-chat                                        \n",
            "meta-llama/meta-llama-3-8b              llama3-base                                            \n",
            "meta-llama/meta-llama-3-8b-instruct     llama3, llama3-chat, llama3-instruct                   \n",
            "meta-llama/meta-llama-3-70b-instruct    llama3-70b                                             \n",
            "meta-llama/meta-llama-3.1-8b            llama3.1-base                                          \n",
            "meta-llama/meta-llama-3.1-8b-instruct   llama3.1, llama3.1-chat, llama3.1-instruct             \n",
            "meta-llama/meta-llama-3.1-70b-instruct  llama3.1-70b                                           \n",
            "meta-llama/codellama-7b-python-hf       codellama, codellama-7b                                \n",
            "meta-llama/codellama-34b-python-hf      codellama-34b                                          \n",
            "mistralai/mistral-7b-v0.1               mistral-7b-v01-base                                    \n",
            "mistralai/mistral-7b-instruct-v0.1      mistral-7b-v01-instruct                                \n",
            "mistralai/mistral-7b-instruct-v0.2      mistral, mistral-7b, mistral-7b-instruct               \n",
            "openlm-research/open_llama_7b           open-llama, open-llama-7b                              \n",
            "stories15m                                                                          Yes        \n",
            "stories42m                                                                                     \n",
            "stories110m                                                                                    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate"
      ],
      "metadata": {
        "id": "RI2Ic5_tBTBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 torchchat.py generate stories15m --prompt \"Write me story about ants and spiders\" --num-samples 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcd7kc0u68h3",
        "outputId": "dd944cc4-70f1-4994-d66b-fa2dcaf6ac66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchao/ops.py:12: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  return torch.library.impl_abstract(f\"{name}\")(func)\n",
            "NumExpr defaulting to 2 threads.\n",
            "PyTorch version 2.4.0 available.\n",
            "Polars version 0.20.2 available.\n",
            "TensorFlow version 2.17.0 available.\n",
            "JAX version 0.4.26 available.\n",
            "2024-08-03 07:01:17.661756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-03 07:01:18.045144: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-03 07:01:18.155355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-03 07:01:18.803306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-03 07:01:21.781177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 17.2MB/s]\n",
            "Using device=cpu Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Loading model...\n",
            "Time to load model: 0.11 seconds\n",
            "-----------------------------------------------------------\n",
            "Write me story about ants and spiders. He loved spiders and their webs. Every day for school, he told his friends about the network and the spiders.\n",
            "One day, his friends asked him what he was going to do with the network. \n",
            "He said, \"I'm going to explore outside and discover what it is all about!\"\n",
            "The friends were excited. They all decided to go and explore the network. They crawled through the grass and flew over the trees.\n",
            "When they came back, they saw lots of different kinds of bugs. Everyone was amazed by their weird discovery and they all started to explore the network.\n",
            "They spent the whole day exploring it. They found lots of exciting creatures and discovered so many amazing places.\n",
            "At the end of the day, the friends said goodbye and went home. They dreamed of the network and all the amazing things they knew! Once upon a time, there was a little girl named L\n",
            "Time for inference 1: 6.77 sec total, time to first token 0.16 sec with parallel prefill, 199 tokens, 29.38 tokens/sec, 34.04 ms/token\n",
            "Bandwidth achieved: 1.43 GB/s\n",
            "*** This first iteration will include cold start effects for dynamic import, hardware caches. ***\n",
            "Write me story about ants and spiders. He loved his story and felt very safe with it in his hand. He loved spending time with his family and having adventures. \n",
            "One day, he came across a spider in his garden. He was scared and wanted to try and fight it, but he didn't know how. He decided to remain calm and thought of a plan.\n",
            "He got a jar of bugs and told the spider to bite them. The spider was angry and tried to fight the spider. But the spider was too strong and he was too weak. \n",
            "Suddenly, a magical fairy appeared. She smiled at Oak and said: “I can help you defeat the spider.” Oak was so excited and he jumped up and down.\n",
            "The fairy said: “I know how to help you. I can use my magic to make the spider go away.” \n",
            "The fairy waved her wand and the sp\n",
            "Time for inference 2: 5.38 sec total, time to first token 0.15 sec with parallel prefill, 199 tokens, 36.97 tokens/sec, 27.05 ms/token\n",
            "Bandwidth achieved: 1.80 GB/s\n",
            "Write me story about ants and spiders and all the things that ants have, like the lily, but the lily is very white.\n",
            "One day, my friend Billy comes to visit.\n",
            "\"What is it, stories!\" mumbled Billy.\n",
            "\"The lily is white and the lily has eight legs,\" Sara said.\n",
            "\"It is white in a green and yellow,\" Billy said.\n",
            "Sarah smiled. \"Let's have a story about it. I'm going to tell it to the lily and the bugs!\"\n",
            "Sarah and Sara sat down and sat in the green, white lily.\n",
            "\"Once upon a time, there was a little girl named Jane who lived in a white house. She loved to visit her grandma and her grandma's garden.\n",
            "One day, Jane asked her grandma if she wanted to visit the white lily in the garden.\n",
            "Grandma said yes.\n",
            "So, Jane and her\n",
            "Time for inference 3: 6.35 sec total, time to first token 0.14 sec with parallel prefill, 199 tokens, 31.33 tokens/sec, 31.92 ms/token\n",
            "Bandwidth achieved: 1.53 GB/s\n",
            "Write me story about ants and spiders. He was a smart boy, but he always wanted to explore the ancient underground world. He had a special mission: visiting the underground world of bugs and spiders.\n",
            "One day, he slowly made his way up to the top of the underground world. As he stepped through, he heard a strange noise. He knew the underground world must have been closed!\n",
            "The next day, he got closer and saw a very old, dusty creature inside the underground. He was excited to find out what it was. He slowly walked closer, and the creature spoke to him.\n",
            "\"Hi there, little one,\" said the creature. \"I'm the ancient, guardian of this underground world. Would you like to come aboard and explore?\"\n",
            "The boy nodded eagerly. He followed the creature through the underground, discovering the secrets he had heard from. What a wonderful adventure! Once upon a time,\n",
            "Time for inference 4: 5.49 sec total, time to first token 0.25 sec with parallel prefill, 199 tokens, 36.24 tokens/sec, 27.59 ms/token\n",
            "Bandwidth achieved: 1.77 GB/s\n",
            "Write me story about ants and spiders. One day, he decided to go for a walk in the woods. He felt a bit tired, but he was brave. He took a deep breath and continued walking. After a while, he came to an old, ancient forest. He saw a big, old tree and decided to rest under it.\n",
            "When he closed his eyes, he heard a voice. \"Hello, Little Roxy! Where are you going?\" It was the voice of the old tree. He was so surprised as he opened his eyes and saw a small alien in a hole.\n",
            "The alien asked him, \"What are you doing here, Little Roxy?\".\n",
            "\"I'm just taking a walk,\" he said. \"I wanted to explore this ancient forest and have a wonderful time.\"\n",
            "The alien smiled. He gave Little Roxy a big hug, and said, \"Thank you for the adventure, Little Roxy! Let's go explore together\".\n",
            "\n",
            "Time for inference 5: 5.18 sec total, time to first token 0.16 sec with parallel prefill, 199 tokens, 38.41 tokens/sec, 26.04 ms/token\n",
            "Bandwidth achieved: 1.87 GB/s\n",
            "\n",
            "========================================\n",
            "\n",
            "Average tokens/sec: 34.47\n",
            "Memory used: 0.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Server"
      ],
      "metadata": {
        "id": "4zAYgCA5BVYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python torchchat.py server stories15m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw7pwIlx7rUH",
        "outputId": "32870fcb-3917-47e1-a45d-46284b5660b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchao/ops.py:12: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  return torch.library.impl_abstract(f\"{name}\")(func)\n",
            "NumExpr defaulting to 2 threads.\n",
            "PyTorch version 2.4.0 available.\n",
            "Polars version 0.20.2 available.\n",
            "TensorFlow version 2.17.0 available.\n",
            "JAX version 0.4.26 available.\n",
            "2024-08-03 07:02:05.405923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-03 07:02:05.432518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-03 07:02:05.440003: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-03 07:02:05.456612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-03 07:02:06.785222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/torchchat/torchchat.py\", line 83, in <module>\n",
            "    server_main(args)\n",
            "  File \"/content/torchchat/server.py\", line 85, in main\n",
            "    gen = initialize_generator(args)\n",
            "  File \"/content/torchchat/server.py\", line 69, in initialize_generator\n",
            "    generator_args = GeneratorArgs.from_args(args)\n",
            "  File \"/content/torchchat/generate.py\", line 113, in from_args\n",
            "    prompt=args.prompt,\n",
            "AttributeError: 'Namespace' object has no attribute 'prompt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strangely, I couldn't connect to my server running on port 5000 in my Colab notebook."
      ],
      "metadata": {
        "id": "XokDBawP_iOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "#subprocess.Popen([\"python\", \"/content/torchchat/torchchat.py\", \"server\", \"stories15m\"])\n",
        "server = subprocess.Popen([\"python\", \"/content/torchchat/torchchat.py\", \"server\", \"stories15m\"])\n"
      ],
      "metadata": {
        "id": "vEzYzCf77JGq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_VAk1PtAF-_",
        "outputId": "1ff33f7d-51dd-486e-e5bd-e929c9b41494"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['python', '/content/torchchat/torchchat.py',...>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://127.0.0.1:5000/chat -H \"Content-Type: application/json\" -d '{\"model\": \"stories15m\", \"messages\": [{\"role\": \"user\",\"content\": \"The sky is\"}]}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZJiXjV97ZVA",
        "outputId": "cef43855-1652-4ac9-d793-e9c64194b800"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to 127.0.0.1 port 5000 after 0 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -I http://127.0.0.1:5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM1Rp50H8HWo",
        "outputId": "ec8e488a-82a8-44d3-96f0-a29d9565373a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to 127.0.0.1 port 5000 after 3 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "args = {'model': 'stories15M', \"messages\":[{\"role\":\"user\", \"content\":\"It was a dark and stormy night and \"}]}\n",
        "\n",
        "r = requests.post(\"http://127.0.0.1:5000/chat\", json=args)\n",
        "\n",
        "print(r.json()['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "Do2bdii08qaK",
        "outputId": "1d9e8531-4414-4256-f84b-6de7b0b35d44"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b229feebcd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Failed to establish a new connection: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7b229feebcd0>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b229feebcd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2783aab22c25>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'stories15M'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"It was a dark and stormy night and \"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://127.0.0.1:5000/chat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b229feebcd0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval"
      ],
      "metadata": {
        "id": "6kLIFwCkBYBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running eval on wikitext dataset for 10 iterations\n",
        "# --limit (Optional[int]): The maximum number of samples to evaluate (None for all available).\n",
        "!python torchchat.py eval stories15M --tasks wikitext --limit 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8bwLhC49OAl",
        "outputId": "b48bee0b-8b35-4cd3-fcfe-44d1b38a16d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchao/ops.py:12: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  return torch.library.impl_abstract(f\"{name}\")(func)\n",
            "NumExpr defaulting to 2 threads.\n",
            "PyTorch version 2.4.0 available.\n",
            "Polars version 0.20.2 available.\n",
            "TensorFlow version 2.17.0 available.\n",
            "JAX version 0.4.26 available.\n",
            "2024-08-03 07:02:31.623243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-03 07:02:31.667288: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-03 07:02:31.680711: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-03 07:02:31.709203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-03 07:02:33.245467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using device=cpu\n",
            "Loading model...\n",
            "Time to load model: 0.09 seconds\n",
            "-----------------------------------------------------------\n",
            "Using device 'cpu'\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.65MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:03<00:00, 170MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 613kB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 147kB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 12.6MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 43.1MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 35.1MB/s]\n",
            "[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
            "[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "Downloading builder script: 100% 10.7k/10.7k [00:00<00:00, 25.6MB/s]\n",
            "Downloading readme: 100% 7.78k/7.78k [00:00<00:00, 22.4MB/s]\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Downloading data: 100% 4.72M/4.72M [00:00<00:00, 42.0MB/s]\n",
            "Generating test split: 62 examples [00:00, 724.68 examples/s]\n",
            "Generating train split: 629 examples [00:00, 2191.97 examples/s]\n",
            "Generating validation split: 60 examples [00:00, 3801.08 examples/s]\n",
            "Building contexts for wikitext on rank 0...\n",
            "100% 10/10 [00:00<00:00, 451.37it/s]\n",
            "Running loglikelihood_rolling requests\n",
            "100% 10/10 [19:43<00:00, 118.33s/it]\n",
            "Time to run eval: 1199.10s.\n",
            "Time in model.forward: 1164.45s, over 33 model evaluations\n",
            "forward run time stats - Median: 35.64s Min: 25.54s Max: 43.03s\n",
            "For model /root/.torchchat/model-cache/stories15M/stories15M.pt\n",
            "wikitext:\n",
            " word_perplexity,none: 47346.6580\n",
            " byte_perplexity,none: 7.7810\n",
            " bits_per_byte,none: 2.9600\n",
            " alias: wikitext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEzam7hBAVAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}