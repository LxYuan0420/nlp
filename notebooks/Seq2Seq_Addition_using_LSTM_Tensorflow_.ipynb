{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq Addition using LSTM Tensorflow .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "faXBQrpi_3lV"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JN0h1tVEspA"
      },
      "source": [
        "##### Generating dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkf7yHggGXTP"
      },
      "source": [
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = False\n",
        "\n",
        "MAX_QUESTION_LEN = DIGITS + 1 + DIGITS\n",
        "MAX_ANSWER_LEN = DIGITS + 1"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIOy941ZEv97"
      },
      "source": [
        "class CharacterTable:\n",
        "    def __init__(self, chars):\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.c2i = dict((c, idx) for idx, c in enumerate(self.chars))\n",
        "        self.i2c = dict((idx, c) for idx, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, math_str, seq_len):\n",
        "        x = np.zeros((seq_len, len(self.chars)))\n",
        "        for i, c in enumerate(math_str):\n",
        "            x[i, self.c2i[c]] = 1\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "\n",
        "        return \"\".join(self.i2c[i] for i in x)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM9yPnnmHrok"
      },
      "source": [
        "chars = \"0123456789+ \"\n",
        "ctable = CharacterTable(chars)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ94MzETIHUE",
        "outputId": "8f1d723e-62c4-4f40-b901-f002f40b2f1f"
      },
      "source": [
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(\n",
        "        \"\".join(\n",
        "            np.random.choice(list(\"0123456789\"))\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))\n",
        "        )\n",
        "    )\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    query = q + \" \" * (MAX_QUESTION_LEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print(\"Total questions:\", len(questions))\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQWWSxoin0wC",
        "outputId": "cdcf501f-c6a8-4a11-b234-70e03b9f056d"
      },
      "source": [
        "questions[:5]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7+125  ', '7+662  ', '684+7  ', '29+69  ', '32+33  ']"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhngEP3Wn9Xi",
        "outputId": "b56dae4d-432c-478a-90ee-22f3da118c86"
      },
      "source": [
        "expected[:5]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['132 ', '669 ', '691 ', '98  ', '65  ']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljsX4zm3J17w",
        "outputId": "90e11982-bc96-420c-9fd7-9da643d37288"
      },
      "source": [
        "x = np.zeros((len(questions), MAX_QUESTION_LEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), MAX_ANSWER_LEN, len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(math_str=sentence, seq_len=MAX_QUESTION_LEN)\n",
        "\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(math_str=sentence, seq_len=MAX_ANSWER_LEN)\n",
        "\n",
        "# shuffle\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_val.shape, y_val.shape)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(45000, 7, 12) (45000, 4, 12)\n",
            "(5000, 7, 12) (5000, 4, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbLa5bMCcelY"
      },
      "source": [
        "##### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwyJEQ3Bch3g",
        "outputId": "424df6cf-4c12-4f36-b5a9-e0776d4e3d52"
      },
      "source": [
        "num_layers = 3\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(MAX_QUESTION_LEN, len(chars))))\n",
        "model.add(layers.RepeatVector(MAX_ANSWER_LEN))\n",
        "for _ in range(num_layers):\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_10 (LSTM)               (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 468,492\n",
            "Trainable params: 468,492\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDwVVwTigwmj",
        "outputId": "749f2300-ad86-4a08-b91b-b997f47308a5"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1048\n",
        "\n",
        "for epoch in range(1, EPOCHS):\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "    if epoch % 25 == 0:\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        for i in range(10):\n",
        "            ind = np.random.randint(0, len(x_val))\n",
        "            rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "            preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "    \n",
        "            q = ctable.decode(rowx[0])\n",
        "            correct = ctable.decode(rowy[0])\n",
        "            guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "    \n",
        "            print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "            print(\"T\", correct, end=\" \")\n",
        "            if correct == guess:\n",
        "                print(\"☑ \" + guess)\n",
        "            else:\n",
        "                print(\"☒ \" + guess)\n",
        "    "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 1s 27ms/step - loss: 0.6779 - accuracy: 0.7676 - val_loss: 0.6711 - val_accuracy: 0.7653\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.8370 - accuracy: 0.6912 - val_loss: 0.6822 - val_accuracy: 0.7681\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.6749 - accuracy: 0.7689 - val_loss: 0.6247 - val_accuracy: 0.7943\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.6138 - accuracy: 0.7973 - val_loss: 0.5944 - val_accuracy: 0.8080\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5837 - accuracy: 0.8120 - val_loss: 0.5690 - val_accuracy: 0.8172\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5632 - accuracy: 0.8176 - val_loss: 0.5475 - val_accuracy: 0.8269\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5522 - accuracy: 0.8169 - val_loss: 0.5790 - val_accuracy: 0.7828\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.5232 - accuracy: 0.8356 - val_loss: 0.5041 - val_accuracy: 0.8522\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4981 - accuracy: 0.8502 - val_loss: 0.4909 - val_accuracy: 0.8508\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4781 - accuracy: 0.8592 - val_loss: 0.4696 - val_accuracy: 0.8624\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4581 - accuracy: 0.8675 - val_loss: 0.4496 - val_accuracy: 0.8749\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.4416 - accuracy: 0.8740 - val_loss: 0.4374 - val_accuracy: 0.8717\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.4417 - accuracy: 0.8650 - val_loss: 0.4162 - val_accuracy: 0.8878\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.4115 - accuracy: 0.8832 - val_loss: 0.3993 - val_accuracy: 0.8903\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.4133 - accuracy: 0.8718 - val_loss: 0.3847 - val_accuracy: 0.8970\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3777 - accuracy: 0.8986 - val_loss: 0.3758 - val_accuracy: 0.8935\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3606 - accuracy: 0.9068 - val_loss: 0.3603 - val_accuracy: 0.9032\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3478 - accuracy: 0.9110 - val_loss: 0.3369 - val_accuracy: 0.9161\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3409 - accuracy: 0.9091 - val_loss: 0.3238 - val_accuracy: 0.9230\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3203 - accuracy: 0.9219 - val_loss: 0.3239 - val_accuracy: 0.9158\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.3199 - accuracy: 0.9144 - val_loss: 0.3098 - val_accuracy: 0.9176\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.3034 - accuracy: 0.9227 - val_loss: 0.2881 - val_accuracy: 0.9323\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2866 - accuracy: 0.9329 - val_loss: 0.2856 - val_accuracy: 0.9301\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2757 - accuracy: 0.9356 - val_loss: 0.3080 - val_accuracy: 0.9067\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2659 - accuracy: 0.9385 - val_loss: 0.2647 - val_accuracy: 0.9360\n",
            "Epoch: 25\n",
            "Q 750+395 T 1145 ☒ 1135\n",
            "Q 124+139 T 263  ☑ 263 \n",
            "Q 881+7   T 888  ☑ 888 \n",
            "Q 56+91   T 147  ☑ 147 \n",
            "Q 765+13  T 778  ☑ 778 \n",
            "Q 888+524 T 1412 ☑ 1412\n",
            "Q 167+983 T 1150 ☒ 1240\n",
            "Q 628+422 T 1050 ☑ 1050\n",
            "Q 73+49   T 122  ☑ 122 \n",
            "Q 454+504 T 958  ☑ 958 \n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2580 - accuracy: 0.9402 - val_loss: 0.2960 - val_accuracy: 0.9038\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2531 - accuracy: 0.9397 - val_loss: 0.2451 - val_accuracy: 0.9427\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.5939 - accuracy: 0.8389 - val_loss: 0.7550 - val_accuracy: 0.7019\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.4158 - accuracy: 0.8598 - val_loss: 0.2650 - val_accuracy: 0.9423\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2430 - accuracy: 0.9517 - val_loss: 0.2326 - val_accuracy: 0.9534\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2257 - accuracy: 0.9563 - val_loss: 0.2222 - val_accuracy: 0.9541\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2156 - accuracy: 0.9596 - val_loss: 0.2126 - val_accuracy: 0.9588\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.2080 - accuracy: 0.9611 - val_loss: 0.2052 - val_accuracy: 0.9596\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1989 - accuracy: 0.9635 - val_loss: 0.1978 - val_accuracy: 0.9617\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1921 - accuracy: 0.9646 - val_loss: 0.2049 - val_accuracy: 0.9524\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1902 - accuracy: 0.9629 - val_loss: 0.1945 - val_accuracy: 0.9566\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1803 - accuracy: 0.9673 - val_loss: 0.1893 - val_accuracy: 0.9610\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1793 - accuracy: 0.9647 - val_loss: 0.1730 - val_accuracy: 0.9686\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1716 - accuracy: 0.9673 - val_loss: 0.1668 - val_accuracy: 0.9689\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1628 - accuracy: 0.9711 - val_loss: 0.1670 - val_accuracy: 0.9668\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1579 - accuracy: 0.9721 - val_loss: 0.1551 - val_accuracy: 0.9737\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1633 - accuracy: 0.9659 - val_loss: 0.2589 - val_accuracy: 0.9082\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1629 - accuracy: 0.9649 - val_loss: 0.1430 - val_accuracy: 0.9764\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1437 - accuracy: 0.9747 - val_loss: 0.1416 - val_accuracy: 0.9744\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1493 - accuracy: 0.9692 - val_loss: 0.1357 - val_accuracy: 0.9773\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1380 - accuracy: 0.9747 - val_loss: 0.1307 - val_accuracy: 0.9782\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1290 - accuracy: 0.9789 - val_loss: 0.1273 - val_accuracy: 0.9786\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1264 - accuracy: 0.9782 - val_loss: 0.1206 - val_accuracy: 0.9812\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1197 - accuracy: 0.9807 - val_loss: 0.1322 - val_accuracy: 0.9727\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.1188 - accuracy: 0.9799 - val_loss: 0.1142 - val_accuracy: 0.9808\n",
            "Epoch: 50\n",
            "Q 795+95  T 890  ☒ 880 \n",
            "Q 73+498  T 571  ☑ 571 \n",
            "Q 380+474 T 854  ☑ 854 \n",
            "Q 43+347  T 390  ☑ 390 \n",
            "Q 6+286   T 292  ☑ 292 \n",
            "Q 277+200 T 477  ☑ 477 \n",
            "Q 835+59  T 894  ☒ 994 \n",
            "Q 831+55  T 886  ☑ 886 \n",
            "Q 861+69  T 930  ☑ 930 \n",
            "Q 58+855  T 913  ☑ 913 \n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2022 - accuracy: 0.9497 - val_loss: 0.7990 - val_accuracy: 0.7030\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.5939 - accuracy: 0.8227 - val_loss: 0.1563 - val_accuracy: 0.9691\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1265 - accuracy: 0.9798 - val_loss: 0.1146 - val_accuracy: 0.9832\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1101 - accuracy: 0.9846 - val_loss: 0.1085 - val_accuracy: 0.9830\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.1040 - accuracy: 0.9857 - val_loss: 0.1041 - val_accuracy: 0.9844\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0996 - accuracy: 0.9865 - val_loss: 0.0992 - val_accuracy: 0.9851\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.0968 - accuracy: 0.9868 - val_loss: 0.0970 - val_accuracy: 0.9849\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0933 - accuracy: 0.9878 - val_loss: 0.0955 - val_accuracy: 0.9844\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0910 - accuracy: 0.9875 - val_loss: 0.0900 - val_accuracy: 0.9867\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0882 - accuracy: 0.9881 - val_loss: 0.0866 - val_accuracy: 0.9884\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0859 - accuracy: 0.9883 - val_loss: 0.0861 - val_accuracy: 0.9855\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0825 - accuracy: 0.9890 - val_loss: 0.0829 - val_accuracy: 0.9873\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0807 - accuracy: 0.9891 - val_loss: 0.0833 - val_accuracy: 0.9865\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0794 - accuracy: 0.9886 - val_loss: 0.0763 - val_accuracy: 0.9894\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0747 - accuracy: 0.9906 - val_loss: 0.0749 - val_accuracy: 0.9890\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0732 - accuracy: 0.9903 - val_loss: 0.0722 - val_accuracy: 0.9887\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0704 - accuracy: 0.9907 - val_loss: 0.0704 - val_accuracy: 0.9908\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0679 - accuracy: 0.9917 - val_loss: 0.0689 - val_accuracy: 0.9895\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0672 - accuracy: 0.9907 - val_loss: 0.0679 - val_accuracy: 0.9899\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0654 - accuracy: 0.9914 - val_loss: 0.0689 - val_accuracy: 0.9889\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0652 - accuracy: 0.9902 - val_loss: 0.0836 - val_accuracy: 0.9793\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0705 - accuracy: 0.9865 - val_loss: 0.0844 - val_accuracy: 0.9789\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0628 - accuracy: 0.9903 - val_loss: 0.0593 - val_accuracy: 0.9912\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0578 - accuracy: 0.9924 - val_loss: 0.0579 - val_accuracy: 0.9916\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0582 - accuracy: 0.9909 - val_loss: 0.0549 - val_accuracy: 0.9925\n",
            "Epoch: 75\n",
            "Q 280+7   T 287  ☑ 287 \n",
            "Q 408+66  T 474  ☑ 474 \n",
            "Q 307+355 T 662  ☑ 662 \n",
            "Q 590+44  T 634  ☑ 634 \n",
            "Q 31+242  T 273  ☑ 273 \n",
            "Q 317+72  T 389  ☑ 389 \n",
            "Q 476+546 T 1022 ☑ 1022\n",
            "Q 33+979  T 1012 ☑ 1012\n",
            "Q 1+14    T 15   ☑ 15  \n",
            "Q 14+40   T 54   ☑ 54  \n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0540 - accuracy: 0.9930 - val_loss: 0.0540 - val_accuracy: 0.9915\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0521 - accuracy: 0.9934 - val_loss: 0.0537 - val_accuracy: 0.9919\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0524 - accuracy: 0.9925 - val_loss: 0.0522 - val_accuracy: 0.9911\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.9458 - accuracy: 0.7858 - val_loss: 0.4304 - val_accuracy: 0.8284\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.2297 - accuracy: 0.9314 - val_loss: 0.1048 - val_accuracy: 0.9829\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0850 - accuracy: 0.9897 - val_loss: 0.0762 - val_accuracy: 0.9901\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0693 - accuracy: 0.9928 - val_loss: 0.0675 - val_accuracy: 0.9918\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0633 - accuracy: 0.9938 - val_loss: 0.0628 - val_accuracy: 0.9923\n",
            "43/43 [==============================] - 1s 27ms/step - loss: 0.0596 - accuracy: 0.9938 - val_loss: 0.0590 - val_accuracy: 0.9929\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0560 - accuracy: 0.9946 - val_loss: 0.0562 - val_accuracy: 0.9931\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0535 - accuracy: 0.9948 - val_loss: 0.0543 - val_accuracy: 0.9928\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0514 - accuracy: 0.9951 - val_loss: 0.0520 - val_accuracy: 0.9931\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0493 - accuracy: 0.9952 - val_loss: 0.0498 - val_accuracy: 0.9935\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0473 - accuracy: 0.9955 - val_loss: 0.0476 - val_accuracy: 0.9945\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0453 - accuracy: 0.9958 - val_loss: 0.0458 - val_accuracy: 0.9944\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0440 - accuracy: 0.9957 - val_loss: 0.0454 - val_accuracy: 0.9937\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0424 - accuracy: 0.9957 - val_loss: 0.0431 - val_accuracy: 0.9944\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0407 - accuracy: 0.9962 - val_loss: 0.0426 - val_accuracy: 0.9939\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0399 - accuracy: 0.9960 - val_loss: 0.0404 - val_accuracy: 0.9945\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0390 - accuracy: 0.9958 - val_loss: 0.0386 - val_accuracy: 0.9953\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0378 - accuracy: 0.9960 - val_loss: 0.0384 - val_accuracy: 0.9948\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0367 - accuracy: 0.9962 - val_loss: 0.0372 - val_accuracy: 0.9951\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0355 - accuracy: 0.9966 - val_loss: 0.0376 - val_accuracy: 0.9943\n",
            "43/43 [==============================] - 1s 28ms/step - loss: 0.0352 - accuracy: 0.9961 - val_loss: 0.0384 - val_accuracy: 0.9941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzopkX3Sor3h"
      },
      "source": [
        "#### Reference:\n",
        "\n",
        "1. https://keras.io/examples/nlp/addition_rnn/\n"
      ]
    }
  ]
}