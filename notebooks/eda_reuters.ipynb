{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718068d4-2784-48b4-835f-c53927d8fae8",
   "metadata": {},
   "source": [
    "### EDA on Reuters-21578 dataset\n",
    "\n",
    "This notebook aims to conduct an in-depth Exploratory Data Analysis (EDA) on the Reuters-21578 text classification dataset. The Reuters dataset is a collection of news documents that are categorized into various topics. Understanding the characteristics and nuances of this dataset is crucial for building efficient and effective text classification models.\n",
    "\n",
    "Note that we are using `ModApte` split in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756b961-10b0-476e-9750-38452690618e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca340039-b313-45ba-a18a-927dcc46910e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"reuters21578\", \"ModApte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074591dc-ee44-409d-8c4f-e3535b278dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 3299\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 9603\n",
       "    })\n",
       "    unused: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 722\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f298eff-b422-4e18-9819-bc1345e35275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Showers continued throughout the week in\\nthe Bahia cocoa zone, alleviating the drought since early\\nJanuary and improving prospects for the coming temporao,\\nalthough normal humidity levels have not been restored,\\nComissaria Smith said in its weekly review.\\n    The dry period means the temporao will be late this year.\\n    Arrivals for the week ended February 22 were 155,221 bags\\nof 60 kilos making a cumulative total for the season of 5.93\\nmln against 5.81 at the same stage last year. Again it seems\\nthat cocoa delivered earlier on consignment was included in the\\narrivals figures.\\n    Comissaria Smith said there is still some doubt as to how\\nmuch old crop cocoa is still available as harvesting has\\npractically come to an end. With total Bahia crop estimates\\naround 6.4 mln bags and sales standing at almost 6.2 mln there\\nare a few hundred thousand bags still in the hands of farmers,\\nmiddlemen, exporters and processors.\\n    There are doubts as to how much of this cocoa would be fit\\nfor export as shippers are now experiencing dificulties in\\nobtaining +Bahia superior+ certificates.\\n    In view of the lower quality over recent weeks farmers have\\nsold a good part of their cocoa held on consignment.\\n    Comissaria Smith said spot bean prices rose to 340 to 350\\ncruzados per arroba of 15 kilos.\\n    Bean shippers were reluctant to offer nearby shipment and\\nonly limited sales were booked for March shipment at 1,750 to\\n1,780 dlrs per tonne to ports to be named.\\n    New crop sales were also light and all to open ports with\\nJune/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\\nunder New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\\nper tonne FOB.\\n    Routine sales of butter were made. March/April sold at\\n4,340, 4,345 and 4,350 dlrs.\\n    April/May butter went at 2.27 times New York May, June/July\\nat 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\\n2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\\n2.27 times New York Dec, Comissaria Smith said.\\n    Destinations were the U.S., Covertible currency areas,\\nUruguay and open ports.\\n    Cake sales were registered at 785 to 995 dlrs for\\nMarch/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\\nNew York Dec for Oct/Dec.\\n    Buyers were the U.S., Argentina, Uruguay and convertible\\ncurrency areas.\\n    Liquor sales were limited with March/April selling at 2,325\\nand 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\\nYork July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\\nSept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\\nsaid.\\n    Total Bahia sales are currently estimated at 6.13 mln bags\\nagainst the 1986/87 crop and 1.06 mln bags against the 1987/88\\ncrop.\\n    Final figures for the period to February 28 are expected to\\nbe published by the Brazilian Cocoa Trade Commission after\\ncarnival which ends midday on February 27.\\n Reuter\\n',\n",
       " 'text_type': '\"NORM\"',\n",
       " 'topics': ['cocoa'],\n",
       " 'lewis_split': '\"TRAIN\"',\n",
       " 'cgis_split': '\"TRAINING-SET\"',\n",
       " 'old_id': '\"5544\"',\n",
       " 'new_id': '\"1\"',\n",
       " 'places': ['el-salvador', 'usa', 'uruguay'],\n",
       " 'people': [],\n",
       " 'orgs': [],\n",
       " 'exchanges': [],\n",
       " 'date': '26-FEB-1987 15:01:01.79',\n",
       " 'title': 'BAHIA COCOA REVIEW'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244461e-f585-4741-a2d6-ae438944c8bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2. Basic Stat\n",
    "\n",
    "- Count the number of samples.\n",
    "- Calculate the average length of the text.\n",
    "- Estimate the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1e4c97-840f-40b0-a0be-76ee9e805876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab03b81f-32e3-45bb-823f-2303d81dee6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "      <th>topics</th>\n",
       "      <th>lewis_split</th>\n",
       "      <th>cgis_split</th>\n",
       "      <th>old_id</th>\n",
       "      <th>new_id</th>\n",
       "      <th>places</th>\n",
       "      <th>people</th>\n",
       "      <th>orgs</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Showers continued throughout the week in\\nthe ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[cocoa]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5544\"</td>\n",
       "      <td>\"1\"</td>\n",
       "      <td>[el-salvador, usa, uruguay]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:01:01.79</td>\n",
       "      <td>BAHIA COCOA REVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Agriculture Department\\nreported the ...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[grain, wheat, corn, barley, oat, sorghum]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5548\"</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:10:44.60</td>\n",
       "      <td>NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Argentine grain board figures show\\ncrop regis...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[veg-oil, linseed, lin-oil, soy-oil, sun-oil, ...</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5549\"</td>\n",
       "      <td>\"6\"</td>\n",
       "      <td>[argentina]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:14:36.41</td>\n",
       "      <td>ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moody's Investors Service Inc said it\\nlowered...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5551\"</td>\n",
       "      <td>\"8\"</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:15:40.12</td>\n",
       "      <td>USX &amp;lt;X&gt; DEBT DOWGRADED BY MOODY'S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Champion Products Inc said its\\nboard of direc...</td>\n",
       "      <td>\"NORM\"</td>\n",
       "      <td>[earn]</td>\n",
       "      <td>\"TRAIN\"</td>\n",
       "      <td>\"TRAINING-SET\"</td>\n",
       "      <td>\"5552\"</td>\n",
       "      <td>\"9\"</td>\n",
       "      <td>[usa]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>26-FEB-1987 15:17:11.20</td>\n",
       "      <td>CHAMPION PRODUCTS &amp;lt;CH&gt; APPROVES STOCK SPLIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_type  \\\n",
       "0  Showers continued throughout the week in\\nthe ...    \"NORM\"   \n",
       "1  The U.S. Agriculture Department\\nreported the ...    \"NORM\"   \n",
       "2  Argentine grain board figures show\\ncrop regis...    \"NORM\"   \n",
       "3  Moody's Investors Service Inc said it\\nlowered...    \"NORM\"   \n",
       "4  Champion Products Inc said its\\nboard of direc...    \"NORM\"   \n",
       "\n",
       "                                              topics lewis_split  \\\n",
       "0                                            [cocoa]     \"TRAIN\"   \n",
       "1         [grain, wheat, corn, barley, oat, sorghum]     \"TRAIN\"   \n",
       "2  [veg-oil, linseed, lin-oil, soy-oil, sun-oil, ...     \"TRAIN\"   \n",
       "3                                                 []     \"TRAIN\"   \n",
       "4                                             [earn]     \"TRAIN\"   \n",
       "\n",
       "       cgis_split  old_id new_id                       places people orgs  \\\n",
       "0  \"TRAINING-SET\"  \"5544\"    \"1\"  [el-salvador, usa, uruguay]     []   []   \n",
       "1  \"TRAINING-SET\"  \"5548\"    \"5\"                        [usa]     []   []   \n",
       "2  \"TRAINING-SET\"  \"5549\"    \"6\"                  [argentina]     []   []   \n",
       "3  \"TRAINING-SET\"  \"5551\"    \"8\"                        [usa]     []   []   \n",
       "4  \"TRAINING-SET\"  \"5552\"    \"9\"                        [usa]     []   []   \n",
       "\n",
       "  exchanges                     date  \\\n",
       "0        []  26-FEB-1987 15:01:01.79   \n",
       "1        []  26-FEB-1987 15:10:44.60   \n",
       "2        []  26-FEB-1987 15:14:36.41   \n",
       "3        []  26-FEB-1987 15:15:40.12   \n",
       "4        []  26-FEB-1987 15:17:11.20   \n",
       "\n",
       "                                              title  \n",
       "0                                BAHIA COCOA REVIEW  \n",
       "1  NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE  \n",
       "2     ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS  \n",
       "3              USX &lt;X> DEBT DOWGRADED BY MOODY'S  \n",
       "4    CHAMPION PRODUCTS &lt;CH> APPROVES STOCK SPLIT  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08525ddf-ade6-491f-8e27-3f7df8e41466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of Samples': 9603,\n",
       " 'Average Text Length': 772.6746849942726,\n",
       " 'Vocabulary Size': 66778}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def basic_statistics(text_column: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate basic statistics about the text data in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        text_column (pd.Series): The text column in the DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing the basic statistics.\n",
    "    \"\"\"\n",
    "    # Count the number of samples\n",
    "    num_samples = len(text_column)\n",
    "    \n",
    "    # Calculate the average length of the text\n",
    "    avg_length = text_column.apply(len).mean()\n",
    "    \n",
    "    # Estimate the vocabulary size\n",
    "    all_words = ' '.join(text_column).split()\n",
    "    vocab_size = len(set(all_words))\n",
    "    \n",
    "    return {\n",
    "        'Number of Samples': num_samples,\n",
    "        'Average Text Length': avg_length,\n",
    "        'Vocabulary Size': vocab_size\n",
    "    }\n",
    "\n",
    "# Perform basic statistics on the 'text' column\n",
    "basic_stats = basic_statistics(df['text'])\n",
    "basic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6bec5-3c0c-4f0a-a950-66a25236037e",
   "metadata": {},
   "source": [
    "##### 3. Word Freq Analysis\n",
    "\n",
    "- Identify the most frequent words.\n",
    "- Identify the least frequent words.\n",
    "- Count the number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c803e135-7b88-4224-bd4f-81d0d7577511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Most Frequent Words': [('the', 64571),\n",
       "  ('of', 34532),\n",
       "  ('to', 31973),\n",
       "  ('in', 25032),\n",
       "  ('and', 24970),\n",
       "  ('a', 23336),\n",
       "  ('said', 15608),\n",
       "  ('mln', 14266),\n",
       "  ('for', 11835),\n",
       "  ('it', 9755)],\n",
       " 'Least Frequent Words': [('unsworth', 1),\n",
       "  ('barrie', 1),\n",
       "  ('ratification.', 1),\n",
       "  ('260,000,', 1),\n",
       "  ('845.50', 1),\n",
       "  ('843.90.', 1),\n",
       "  ('844.30', 1),\n",
       "  ('midrate', 1),\n",
       "  ('4,044', 1),\n",
       "  ('3,978', 1)],\n",
       " 'Unique Words': 61882}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequency_analysis(text_column: pd.Series, n_most_frequent: int = 10, n_least_frequent: int = 10) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Analyze the frequency of words in the text column.\n",
    "    \n",
    "    Args:\n",
    "        text_column (pd.Series): The text column in the DataFrame.\n",
    "        n_most_frequent (int): Number of most frequent words to return.\n",
    "        n_least_frequent (int): Number of least frequent words to return.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, List[str]]: A dictionary containing lists of most and least frequent words.\n",
    "    \"\"\"\n",
    "    # Tokenize the text and count word frequencies\n",
    "    all_words = ' '.join(text_column).lower().split()\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    # Get the n most frequent words\n",
    "    most_frequent_words = word_freq.most_common(n_most_frequent)\n",
    "    \n",
    "    # Get the n least frequent words\n",
    "    least_frequent_words = word_freq.most_common()[:-n_least_frequent-1:-1]\n",
    "    \n",
    "    return {\n",
    "        'Most Frequent Words': most_frequent_words,\n",
    "        'Least Frequent Words': least_frequent_words,\n",
    "        'Unique Words': len(word_freq)\n",
    "    }\n",
    "\n",
    "# Perform word frequency analysis on the 'text' column\n",
    "word_freq_stats = word_frequency_analysis(df['text'])\n",
    "word_freq_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42edce-1c4b-43ee-9f6f-2d906fcd6488",
   "metadata": {},
   "source": [
    "- Most Frequent Words: The most common words are mostly stop words like 'the', 'of', 'to', etc., which are generally expected in natural language text.\n",
    "- Least Frequent Words: Words like 'unsworth', 'barrie', 'ratification.', etc., appear only once in the dataset.\n",
    "- Unique Words: There are 61,882 unique words after lowercasing the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db40e60-59b4-441a-af65-472c55b95a79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 4. N-gram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a31dc4b-36bf-46a0-8c22-f828cebaa62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6a67f9-5b8d-4c11-914f-6fc0bbb04456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def ngram_analysis(text_column: pd.Series, n: int, top_n: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Analyze the frequency of n-grams in the text column.\n",
    "    \n",
    "    Args:\n",
    "        text_column (pd.Series): The text column in the DataFrame.\n",
    "        n (int): The size of the n-gram (bi-gram: 2, tri-gram: 3, etc.)\n",
    "        top_n (int): Number of most frequent n-grams to return.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of the most frequent n-grams.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    all_words = ' '.join(text_column).lower().split()\n",
    "    \n",
    "    # Generate n-grams\n",
    "    n_grams = ngrams(all_words, n)\n",
    "    \n",
    "    # Count the frequency of each n-gram\n",
    "    ngram_freq = Counter(n_grams)\n",
    "    \n",
    "    # Get the top_n most frequent n-grams\n",
    "    most_frequent_ngrams = ngram_freq.most_common(top_n)\n",
    "    \n",
    "    return most_frequent_ngrams\n",
    "\n",
    "# Perform n-gram analysis for bi-grams and tri-grams on the 'text' column\n",
    "bigram_stats = ngram_analysis(df['text'], 2)\n",
    "trigram_stats = ngram_analysis(df['text'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1b913d-c964-44d0-afc2-f3cd7a77eb95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 6474),\n",
       " (('in', 'the'), 5918),\n",
       " (('said', 'it'), 3918),\n",
       " (('said', 'the'), 3305),\n",
       " (('mln', 'dlrs'), 2997),\n",
       " (('for', 'the'), 2570),\n",
       " (('mln', 'vs'), 2476),\n",
       " (('to', 'the'), 2328),\n",
       " (('will', 'be'), 2316),\n",
       " (('cts', 'vs'), 2175)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bi-gram\n",
    "bigram_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75b47bd-b701-4fe3-aa03-852b05b68eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'company', 'said'), 773),\n",
       " (('mln', 'dlrs', 'in'), 635),\n",
       " (('mln', 'dlrs', 'of'), 585),\n",
       " (('pct', 'of', 'the'), 555),\n",
       " (('said', 'it', 'has'), 554),\n",
       " (('inc', 'said', 'it'), 525),\n",
       " (('cts', 'vs', 'loss'), 495),\n",
       " (('corp', 'said', 'it'), 460),\n",
       " (('the', 'end', 'of'), 449),\n",
       " (('the', 'bank', 'of'), 406)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tri-gram\n",
    "trigram_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bfd65-1bf8-4d08-8097-5dda1a23176e",
   "metadata": {},
   "source": [
    "- Most Frequent Bi-grams: The pair ('of', 'the') appears most frequently, followed by ('in', 'the'), ('said', 'it'), etc.\n",
    "- Most Frequent Tri-grams: The sequence ('the', 'company', 'said') is the most common tri-gram, followed by ('mln', 'dlrs', 'in'), ('mln', 'dlrs', 'of'), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72a103-2bad-4a03-80e3-2c2337fec3bf",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### 5. Label Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e32bf69-7af6-40f2-af12-c36baee94a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "def label_analysis(topics_column: pd.Series) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Perform label analysis on the topics column.\n",
    "    \n",
    "    Args:\n",
    "        topics_column (pd.Series): The topics column containing the labels.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Dict]: A dictionary containing individual and combined label counts, cardinality, and density.\n",
    "    \"\"\"\n",
    "    # Count individual labels\n",
    "    individual_label_count = Counter(chain.from_iterable(topics_column))\n",
    "    \n",
    "    # Calculate label cardinality (average number of labels per instance)\n",
    "    label_cardinality = topics_column.apply(len).mean()\n",
    "    \n",
    "    # Calculate label density (label cardinality divided by the number of unique labels)\n",
    "    label_density = label_cardinality / len(individual_label_count) if individual_label_count else 0\n",
    "    \n",
    "    return {\n",
    "        'unique_label_count': individual_label_count,\n",
    "        'label_cardinality': label_cardinality,\n",
    "        'label_density': label_density\n",
    "    }\n",
    "\n",
    "# Perform label analysis on the 'topics' column\n",
    "label_stats = label_analysis(df['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "552aac6f-48b0-45da-a97a-8dbe7537c694",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_label_count:\n",
      "\tCounter({'earn': 2877, 'acq': 1650, 'money-fx': 538, 'grain': 433, 'crude': 389, 'trade': 369, 'interest': 347, 'wheat': 212, 'ship': 197, 'corn': 182, 'money-supply': 140, 'dlr': 131, 'sugar': 126, 'oilseed': 124, 'coffee': 111, 'gnp': 101, 'gold': 94, 'veg-oil': 87, 'soybean': 78, 'livestock': 75, 'nat-gas': 75, 'bop': 75, 'cpi': 69, 'cocoa': 55, 'reserves': 55, 'carcass': 50, 'copper': 47, 'jobs': 46, 'yen': 45, 'ipi': 41, 'iron-steel': 40, 'cotton': 39, 'barley': 37, 'rubber': 37, 'gas': 37, 'rice': 35, 'alum': 35, 'meal-feed': 30, 'palm-oil': 30, 'sorghum': 24, 'retail': 23, 'silver': 21, 'zinc': 21, 'pet-chem': 20, 'wpi': 19, 'tin': 18, 'rapeseed': 18, 'stg': 17, 'housing': 16, 'strategic-metal': 16, 'hog': 16, 'orange': 16, 'lead': 15, 'soy-oil': 14, 'heat': 14, 'soy-meal': 13, 'fuel': 13, 'lei': 12, 'sunseed': 11, 'lumber': 10, 'dmk': 10, 'tea': 9, 'income': 9, 'oat': 8, 'nickel': 8, 'l-cattle': 6, 'sun-oil': 5, 'platinum': 5, 'rape-oil': 5, 'groundnut': 5, 'instal-debt': 5, 'inventories': 5, 'plywood': 4, 'jet': 4, 'coconut-oil': 4, 'austdlr': 4, 'coconut': 4, 'tapioca': 3, 'propane': 3, 'saudriyal': 3, 'potato': 3, 'can': 3, 'cpu': 3, 'pork-belly': 3, 'linseed': 2, 'copra-cake': 2, 'palmkernel': 2, 'cornglutenfeed': 2, 'wool': 2, 'fishmeal': 2, 'palladium': 2, 'dfl': 2, 'naphtha': 2, 'nzdlr': 2, 'rand': 2, 'lin-oil': 1, 'rye': 1, 'red-bean': 1, 'groundnut-oil': 1, 'citruspulp': 1, 'rape-meal': 1, 'corn-oil': 1, 'peseta': 1, 'cotton-oil': 1, 'ringgit': 1, 'castorseed': 1, 'castor-oil': 1, 'lit': 1, 'rupiah': 1, 'skr': 1, 'nkr': 1, 'dkr': 1, 'sun-meal': 1, 'lin-meal': 1, 'cruzado': 1})\n",
      "label_cardinality:\n",
      "\t1.0047901697386235\n",
      "label_density:\n",
      "\t0.008737305823814117\n"
     ]
    }
   ],
   "source": [
    "for k, v in label_stats.items():\n",
    "    print(f\"{k}:\\n\\t{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596d14dc-cd2f-43a3-aac1-9e19009629f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_labels_once = 20\n",
      "labels_once = ['lin-oil', 'rye', 'red-bean', 'groundnut-oil', 'citruspulp', 'rape-meal', 'corn-oil', 'peseta', 'cotton-oil', 'ringgit', 'castorseed', 'castor-oil', 'lit', 'rupiah', 'skr', 'nkr', 'dkr', 'sun-meal', 'lin-meal', 'cruzado']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the overall topic frequency\n",
    "overall_topic_freq = Counter(chain.from_iterable(df['topics']))\n",
    "\n",
    "# Identify and count labels that appear only once\n",
    "labels_once = [label for label, count in overall_topic_freq.items() if count == 1]\n",
    "count_labels_once = len(labels_once)\n",
    "\n",
    "print(f\"{count_labels_once = }\")\n",
    "print(f\"{labels_once = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e9fda-69cf-4170-b15a-6894244c0680",
   "metadata": {},
   "source": [
    "- Unique Label Count: Notice that there some labels appear only once in the training set.\n",
    "- Label Cardinality: The average number of labels per instance is approximately 1.005.\n",
    "- Label Density: The label density, calculated as the label cardinality divided by the number of unique labels, is approximately 0.0087. A label density of 0.0087 indicates that, on average, each article is associated with a very small fraction of the total unique labels available. In simpler terms, this means that the labels are quite diverse across the dataset, and each article is usually related to a very specific topic among many possible topics.\n",
    "- Top 5 Most Common Topics: The most common topics across all documents are 'earn', 'acq', 'money-fx', 'grain', and 'crude'.\n",
    "- Least 5 Common Topics: The least common topics, appearing only once in the dataset, are 'cruzado', 'lin-meal', 'sun-meal', 'dkr', and 'nkr'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad578f1-3dbf-4839-a7d4-886846e87b9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 6. Document Length Analysis (median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7cdf54d-03ce-4e04-811c-9658c34047f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_doc_length = 511.0 \n",
      "count_above_median = 4801 \n",
      "most_common_above_median = [('acq', 848), ('earn', 683), ('money-fx', 316), ('trade', 315), ('grain', 313)] \n",
      "count_below_median = 4796 \n",
      "most_common_below_median = [('earn', 2192), ('acq', 799), ('money-fx', 222), ('interest', 187), ('grain', 120)] \n"
     ]
    }
   ],
   "source": [
    "# Calculate the median document length\n",
    "median_doc_length = df['text'].apply(len).median()\n",
    "\n",
    "# Filter documents above and below the median length\n",
    "df_above_median = df[df['text'].apply(len) > median_doc_length]\n",
    "df_below_median = df[df['text'].apply(len) < median_doc_length]\n",
    "\n",
    "# Count of documents above and below the median length\n",
    "count_above_median = len(df_above_median)\n",
    "count_below_median = len(df_below_median)\n",
    "\n",
    "# Most common topics in documents above the median length\n",
    "most_common_above_median = Counter(chain.from_iterable(df_above_median['topics'])).most_common(5)\n",
    "\n",
    "# Most common topics in documents below the median length\n",
    "most_common_below_median = Counter(chain.from_iterable(df_below_median['topics'])).most_common(5)\n",
    "\n",
    "print(f\"{median_doc_length = } \")\n",
    "print(f\"{count_above_median = } \")\n",
    "print(f\"{most_common_above_median = } \")\n",
    "print(f\"{count_below_median = } \")\n",
    "print(f\"{most_common_below_median = } \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e10b8f-12a1-4073-91c1-847cafafb614",
   "metadata": {},
   "source": [
    "- Median Document Length: The median length of the documents is 511 characters.\n",
    "\n",
    "For documents above the median length:\n",
    "\n",
    "- Number of Documents: There are 4,801 documents that are longer than the median length.\n",
    "- Most Common Topics: The top 5 most common topics in these documents are 'acq', 'earn', 'money-fx', 'trade', and 'grain'.\n",
    "\n",
    "For documents below the median length:\n",
    "\n",
    "- Number of Documents: There are 4,796 documents that are shorter than the median length.\n",
    "- Most Common Topics: The top 5 most common topics in these documents are 'earn', 'acq', 'money-fx', 'interest', and 'grain'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3652e-164f-401e-add5-c2605bf80f16",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "7. Check single_appearance_label in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6ab7c6-5c70-458c-be2e-25708dfbe20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_appearance_labels = ['lin-oil',\n",
    " 'rye',\n",
    " 'red-bean',\n",
    " 'groundnut-oil',\n",
    " 'citruspulp',\n",
    " 'rape-meal',\n",
    " 'dfl',\n",
    " 'corn-oil',\n",
    " 'peseta',\n",
    " 'cotton-oil',\n",
    " 'ringgit',\n",
    " 'lit',\n",
    " 'rupiah',\n",
    " 'skr',\n",
    " 'nkr',\n",
    " 'dkr',\n",
    " 'sun-meal',\n",
    " 'lin-meal',\n",
    " 'cruzado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbdaa3ab-b98a-41d9-924a-f35c1ccbb674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store samples with single-appearance labels\n",
    "samples_with_single_appearance_labels = []\n",
    "\n",
    "for sample in dataset[\"test\"]:\n",
    "    if any(label in single_appearance_labels for label in sample['topics']):\n",
    "        samples_with_single_appearance_labels.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b4b3232-e69d-4344-a945-c5201029c9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples_with_single_appearance_labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eece3fe-1f2d-427e-8182-f0b4b0c22584",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before we remove labels that appear only once in the training set, it's a good idea to check how often these single-occurrence labels show up in the test set. Labels that appear only once can be problematic for classification tasks, as machine learning models may not have enough data to learn from them effectively.\n",
    "\n",
    "After checking, we found that there are only seven samples that contain one of those single-appearance labels. In other words, it is safe to drop them and reduce the size of the label space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9a100-61dc-4521-8b1e-5231f33fe7d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Key takeway:\n",
    "\n",
    "- It is a multilabel text dataset.\n",
    "- Preprocess train/test split by removing those single-appearance-labels.\n",
    "- We will be focusing on title, text, topics for this multilabel text classification. \n",
    "- A good starting point of defining the max_vocab_size is about 20000 since it is stated by PapersWithCode that: The Reuters-21578 dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words.\n",
    "- We are using the dataset hosted on HF hub (check the first code cell) and we will be only the train and test split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
