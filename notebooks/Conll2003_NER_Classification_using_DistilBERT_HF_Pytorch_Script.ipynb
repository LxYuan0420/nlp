{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conll2003 NER Classification using DistilBERT HF Pytorch Script.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O21tI9CjAXIc"
      },
      "source": [
        "### 0. Install and load library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyZGa3XuFX96"
      },
      "source": [
        "#Restart kernel after installation: Runtime -> Restart runtime\n",
        "\n",
        "#!pip install -U sentencepiece datasets\n",
        "#!pip install -U transformers sentencepiece datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKy8hYBNFooc"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQvadG34F1EU"
      },
      "source": [
        "%cd \"/content/transformers/examples/pytorch/token-classification\"\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_HQ2yqlCDA"
      },
      "source": [
        "### 1. Fine-tuning DistillBERT model on CONLL2003 NER dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP_E2Z9FDkcR",
        "outputId": "3f04bc1f-6f93-4d42-cff6-f0f15cb3c4f9"
      },
      "source": [
        "!python /content/transformers/examples/pytorch/token-classification/run_ner.py \\\n",
        "  --model_name_or_path distilbert-base-cased \\\n",
        "  --label_all_tokens True \\\n",
        "  --return_entity_level_metrics True \\\n",
        "  --dataset_name conll2003 \\\n",
        "  --output_dir /tmp/distilbert-base-cased-finetuned-conll03-english \\\n",
        "  --do_train \\\n",
        "  --do_eval"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-09 08:00:02.827115: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/09/2021 08:00:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/09/2021 08:00:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/distilbert-base-cased-finetuned-conll03-english/runs/Jul09_08-00-04_bcfb927c1c5d,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/distilbert-base-cased-finetuned-conll03-english,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=distilbert-base-cased-finetuned-conll03-english,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/distilbert-base-cased-finetuned-conll03-english,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/09/2021 08:00:05 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpiyew1s4f\n",
            "Downloading: 9.52kB [00:00, 9.61MB/s]       \n",
            "07/09/2021 08:00:05 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py in cache at /root/.cache/huggingface/datasets/downloads/5f32acc3044dafd3982cb6aed17f2d2c0a48cdd28a9451a4963c3754c892844b.69ec5a311ec36783373a7d3f79dee5165184196efbc42b7a7a8c2e29fb060f36.py\n",
            "07/09/2021 08:00:05 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5f32acc3044dafd3982cb6aed17f2d2c0a48cdd28a9451a4963c3754c892844b.69ec5a311ec36783373a7d3f79dee5165184196efbc42b7a7a8c2e29fb060f36.py\n",
            "07/09/2021 08:00:05 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp1g4iq935\n",
            "Downloading: 4.18kB [00:00, 4.33MB/s]       \n",
            "07/09/2021 08:00:05 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/01f81f8d006b5c9ead430e489d9be47711828caa560cca6530b96f16b8c63bdd.6f0fff71b78999f12af13d55f74d5ac6c69bf9a9b243d3a410ec8bd474e3e077\n",
            "07/09/2021 08:00:05 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/01f81f8d006b5c9ead430e489d9be47711828caa560cca6530b96f16b8c63bdd.6f0fff71b78999f12af13d55f74d5ac6c69bf9a9b243d3a410ec8bd474e3e077\n",
            "07/09/2021 08:00:05 - INFO - datasets.load - Creating main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003\n",
            "07/09/2021 08:00:05 - INFO - datasets.load - Creating specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\n",
            "07/09/2021 08:00:05 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py to /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/conll2003.py\n",
            "07/09/2021 08:00:05 - INFO - datasets.load - Copying dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/dataset_infos.json\n",
            "07/09/2021 08:00:05 - INFO - datasets.load - Creating metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/conll2003.json\n",
            "07/09/2021 08:00:06 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003\n",
            "07/09/2021 08:00:06 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\n",
            "07/09/2021 08:00:06 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py to /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/conll2003.py\n",
            "07/09/2021 08:00:06 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/dataset_infos.json\n",
            "07/09/2021 08:00:06 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/conll2003/conll2003.py at /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/conll2003.json\n",
            "07/09/2021 08:00:06 - INFO - datasets.builder - No config specified, defaulting to first: conll2003/conll2003\n",
            "07/09/2021 08:00:06 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6\n",
            "07/09/2021 08:00:06 - INFO - datasets.builder - Generating dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
            "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n",
            "07/09/2021 08:00:06 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "  0% 0/3 [00:00<?, ?it/s]07/09/2021 08:00:07 - INFO - datasets.utils.file_utils - https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpbi85a25a\n",
            "\n",
            "Downloading: 3.28MB [00:00, 61.2MB/s]      \n",
            "07/09/2021 08:00:07 - INFO - datasets.utils.file_utils - storing https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt in cache at /root/.cache/huggingface/datasets/downloads/07144cf16b7a7a911e3a0944fafe476248258d83a4d36783d968885f345571ee\n",
            "07/09/2021 08:00:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/07144cf16b7a7a911e3a0944fafe476248258d83a4d36783d968885f345571ee\n",
            " 33% 1/3 [00:01<00:02,  1.18s/it]07/09/2021 08:00:08 - INFO - datasets.utils.file_utils - https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/valid.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp932c74e4\n",
            "\n",
            "Downloading: 827kB [00:00, 48.7MB/s]       \n",
            "07/09/2021 08:00:08 - INFO - datasets.utils.file_utils - storing https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/valid.txt in cache at /root/.cache/huggingface/datasets/downloads/82da4e00f7d3318b5628fc5f95ab0e5b314bd92a8ee939be387a64ea68da06c2\n",
            "07/09/2021 08:00:08 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/82da4e00f7d3318b5628fc5f95ab0e5b314bd92a8ee939be387a64ea68da06c2\n",
            " 67% 2/3 [00:01<00:01,  1.05s/it]07/09/2021 08:00:08 - INFO - datasets.utils.file_utils - https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/test.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpgyyj_9m9\n",
            "\n",
            "Downloading: 748kB [00:00, 43.8MB/s]       \n",
            "07/09/2021 08:00:09 - INFO - datasets.utils.file_utils - storing https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/test.txt in cache at /root/.cache/huggingface/datasets/downloads/4d7b6be967231c656a4ce471ef1de9e3b7804499daac36a131618cf206527529\n",
            "07/09/2021 08:00:09 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/4d7b6be967231c656a4ce471ef1de9e3b7804499daac36a131618cf206527529\n",
            "100% 3/3 [00:02<00:00,  1.13it/s]\n",
            "07/09/2021 08:00:09 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "07/09/2021 08:00:09 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 3/3 [00:00<00:00, 1629.91it/s]\n",
            "07/09/2021 08:00:09 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "07/09/2021 08:00:09 - INFO - datasets.builder - Generating split train\n",
            "07/09/2021 08:00:11 - INFO - datasets.builder - Generating split validation\n",
            "07/09/2021 08:00:12 - INFO - datasets.builder - Generating split test\n",
            "07/09/2021 08:00:12 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 301.69it/s]\n",
            "[INFO|file_utils.py:1623] 2021-07-09 08:00:12,940 >> https://huggingface.co/distilbert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsjltu182\n",
            "Downloading: 100% 411/411 [00:00<00:00, 343kB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-09 08:00:13,236 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|file_utils.py:1635] 2021-07-09 08:00:13,236 >> creating metadata file for /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:537] 2021-07-09 08:00:13,237 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:573] 2021-07-09 08:00:13,237 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2,\n",
            "    \"3\": 3,\n",
            "    \"4\": 4,\n",
            "    \"5\": 5,\n",
            "    \"6\": 6,\n",
            "    \"7\": 7,\n",
            "    \"8\": 8\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2,\n",
            "    \"3\": 3,\n",
            "    \"4\": 4,\n",
            "    \"5\": 5,\n",
            "    \"6\": 6,\n",
            "    \"7\": 7,\n",
            "    \"8\": 8\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1623] 2021-07-09 08:00:13,533 >> https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8qrdo5v3\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 22.4kB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-09 08:00:13,831 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1635] 2021-07-09 08:00:13,831 >> creating metadata file for /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:537] 2021-07-09 08:00:14,126 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:573] 2021-07-09 08:00:14,126 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1623] 2021-07-09 08:00:14,418 >> https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqhv4zfxi\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 1.62MB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-09 08:00:14,845 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1635] 2021-07-09 08:00:14,845 >> creating metadata file for /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1623] 2021-07-09 08:00:15,141 >> https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp00sxmqiw\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 3.12MB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-09 08:00:15,581 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1635] 2021-07-09 08:00:15,581 >> creating metadata file for /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 08:00:16,478 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 08:00:16,478 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 08:00:16,478 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 08:00:16,478 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-09 08:00:16,478 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1623] 2021-07-09 08:00:16,931 >> https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj_vcisko\n",
            "Downloading: 100% 263M/263M [00:04<00:00, 58.1MB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-09 08:00:21,604 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[INFO|file_utils.py:1635] 2021-07-09 08:00:21,605 >> creating metadata file for /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[INFO|modeling_utils.py:1237] 2021-07-09 08:00:21,605 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[WARNING|modeling_utils.py:1440] 2021-07-09 08:00:22,175 >> Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1451] 2021-07-09 08:00:22,175 >> Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/15 [00:00<?, ?ba/s]07/09/2021 08:00:22 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/cache-15e8d403fb27c910.arrow\n",
            "Running tokenizer on train dataset: 100% 15/15 [00:01<00:00,  9.04ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]07/09/2021 08:00:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/cache-4bc5e752fa4fcf00.arrow\n",
            "Running tokenizer on validation dataset: 100% 4/4 [00:00<00:00,  8.37ba/s]\n",
            "07/09/2021 08:00:24 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/seqeval/seqeval.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8xps9ifo\n",
            "Downloading: 6.34kB [00:00, 5.93MB/s]       \n",
            "07/09/2021 08:00:24 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/seqeval/seqeval.py in cache at /root/.cache/huggingface/datasets/downloads/fde5b46e0899600ac4f22bc0294a8d3d06e121455c06dca9aff078b577aea922.a09051c6235c8b054e473ffcc08da34f95b4a1800e1818bce66eed11f4e3833d.py\n",
            "07/09/2021 08:00:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/fde5b46e0899600ac4f22bc0294a8d3d06e121455c06dca9aff078b577aea922.a09051c6235c8b054e473ffcc08da34f95b4a1800e1818bce66eed11f4e3833d.py\n",
            "07/09/2021 08:00:25 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval\n",
            "07/09/2021 08:00:25 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e\n",
            "07/09/2021 08:00:25 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/seqeval/seqeval.py to /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.py\n",
            "07/09/2021 08:00:25 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/seqeval/dataset_infos.json\n",
            "07/09/2021 08:00:25 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.json\n",
            "[INFO|trainer.py:521] 2021-07-09 08:00:36,991 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, pos_tags, id, ner_tags.\n",
            "[INFO|trainer.py:1158] 2021-07-09 08:00:37,008 >> ***** Running training *****\n",
            "[INFO|trainer.py:1159] 2021-07-09 08:00:37,008 >>   Num examples = 14041\n",
            "[INFO|trainer.py:1160] 2021-07-09 08:00:37,008 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1161] 2021-07-09 08:00:37,008 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1162] 2021-07-09 08:00:37,008 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1163] 2021-07-09 08:00:37,009 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1164] 2021-07-09 08:00:37,009 >>   Total optimization steps = 5268\n",
            "{'loss': 0.2495, 'learning_rate': 4.525436598329537e-05, 'epoch': 0.28}\n",
            "  9% 500/5268 [00:26<04:00, 19.79it/s][INFO|trainer.py:1913] 2021-07-09 08:01:03,325 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-500\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:01:03,326 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:01:03,869 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:01:03,870 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:01:03,870 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.1185, 'learning_rate': 4.050873196659074e-05, 'epoch': 0.57}\n",
            " 19% 1000/5268 [00:55<04:02, 17.60it/s][INFO|trainer.py:1913] 2021-07-09 08:01:32,224 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1000\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:01:32,224 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:01:32,756 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:01:32,757 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:01:32,757 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.0976, 'learning_rate': 3.5763097949886106e-05, 'epoch': 0.85}\n",
            " 28% 1500/5268 [01:24<03:33, 17.61it/s][INFO|trainer.py:1913] 2021-07-09 08:02:01,763 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1500\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:02:01,764 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:02:02,326 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:02:02,327 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:02:02,327 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.0734, 'learning_rate': 3.1017463933181475e-05, 'epoch': 1.14}\n",
            " 38% 2000/5268 [01:54<02:59, 18.21it/s][INFO|trainer.py:1913] 2021-07-09 08:02:31,796 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2000\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:02:31,797 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:02:32,448 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:02:32,449 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:02:32,449 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.0434, 'learning_rate': 2.6271829916476843e-05, 'epoch': 1.42}\n",
            " 47% 2500/5268 [02:24<02:31, 18.28it/s][INFO|trainer.py:1913] 2021-07-09 08:03:01,925 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2500\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:03:01,926 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:03:02,576 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:03:02,577 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:03:02,577 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.05, 'learning_rate': 2.152619589977221e-05, 'epoch': 1.71}\n",
            " 57% 3000/5268 [02:54<02:07, 17.79it/s][INFO|trainer.py:1913] 2021-07-09 08:03:31,849 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3000\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:03:31,850 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:03:32,504 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:03:32,505 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:03:32,505 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.0426, 'learning_rate': 1.678056188306758e-05, 'epoch': 1.99}\n",
            " 66% 3500/5268 [03:24<01:29, 19.72it/s][INFO|trainer.py:1913] 2021-07-09 08:04:01,576 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3500\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:04:01,577 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:04:02,195 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:04:02,195 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:04:02,195 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.0223, 'learning_rate': 1.2034927866362947e-05, 'epoch': 2.28}\n",
            " 76% 4000/5268 [03:54<01:05, 19.27it/s][INFO|trainer.py:1913] 2021-07-09 08:04:31,048 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4000\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:04:31,049 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:04:31,664 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:04:31,665 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:04:31,665 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.0198, 'learning_rate': 7.289293849658315e-06, 'epoch': 2.56}\n",
            " 85% 4500/5268 [04:23<00:40, 18.79it/s][INFO|trainer.py:1913] 2021-07-09 08:05:00,744 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4500\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:05:00,745 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:05:01,381 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:05:01,382 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:05:01,382 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.0185, 'learning_rate': 2.5436598329536827e-06, 'epoch': 2.85}\n",
            " 95% 5000/5268 [04:53<00:15, 17.85it/s][INFO|trainer.py:1913] 2021-07-09 08:05:30,271 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-5000\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:05:30,272 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:05:30,957 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:05:30,957 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:05:30,958 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/checkpoint-5000/special_tokens_map.json\n",
            "100% 5268/5268 [05:10<00:00, 19.47it/s][INFO|trainer.py:1354] 2021-07-09 08:05:47,330 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 310.3341, 'train_samples_per_second': 135.734, 'train_steps_per_second': 16.975, 'train_loss': 0.0706053676257644, 'epoch': 3.0}\n",
            "100% 5268/5268 [05:10<00:00, 16.98it/s]\n",
            "[INFO|trainer.py:1913] 2021-07-09 08:05:47,345 >> Saving model checkpoint to /tmp/distilbert-base-cased-finetuned-conll03-english\n",
            "[INFO|configuration_utils.py:371] 2021-07-09 08:05:47,346 >> Configuration saved in /tmp/distilbert-base-cased-finetuned-conll03-english/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-09 08:05:47,970 >> Model weights saved in /tmp/distilbert-base-cased-finetuned-conll03-english/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-09 08:05:47,971 >> tokenizer config file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:05:47,971 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.0706\n",
            "  train_runtime            = 0:05:10.33\n",
            "  train_samples            =      14041\n",
            "  train_samples_per_second =    135.734\n",
            "  train_steps_per_second   =     16.975\n",
            "07/09/2021 08:05:48 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:521] 2021-07-09 08:05:48,010 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, pos_tags, id, ner_tags.\n",
            "[INFO|trainer.py:2159] 2021-07-09 08:05:48,030 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2161] 2021-07-09 08:05:48,031 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2164] 2021-07-09 08:05:48,031 >>   Batch size = 8\n",
            "100% 407/407 [00:06<00:00, 60.22it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_LOC_f1             =     0.9592\n",
            "  eval_LOC_number         =       3635\n",
            "  eval_LOC_precision      =     0.9639\n",
            "  eval_LOC_recall         =     0.9546\n",
            "  eval_MISC_f1            =     0.8856\n",
            "  eval_MISC_number        =       1480\n",
            "  eval_MISC_precision     =     0.8896\n",
            "  eval_MISC_recall        =     0.8818\n",
            "  eval_ORG_f1             =     0.9053\n",
            "  eval_ORG_number         =       2702\n",
            "  eval_ORG_precision      =     0.8931\n",
            "  eval_ORG_recall         =     0.9178\n",
            "  eval_PER_f1             =       0.96\n",
            "  eval_PER_number         =       3329\n",
            "  eval_PER_precision      =     0.9584\n",
            "  eval_PER_recall         =     0.9616\n",
            "  eval_loss               =     0.0782\n",
            "  eval_overall_accuracy   =     0.9838\n",
            "  eval_overall_f1         =     0.9365\n",
            "  eval_overall_precision  =     0.9349\n",
            "  eval_overall_recall     =     0.9381\n",
            "  eval_runtime            = 0:00:06.78\n",
            "  eval_samples            =       3250\n",
            "  eval_samples_per_second =    479.029\n",
            "  eval_steps_per_second   =     59.989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kye944GmmZd"
      },
      "source": [
        "\"\"\"\n",
        "[INFO|tokenization_utils_base.py:1954] 2021-07-09 08:05:47,971 >> Special tokens file saved in /tmp/distilbert-base-cased-finetuned-conll03-english/special_tokens_map.json\n",
        "***** train metrics *****\n",
        "  epoch                    =        3.0\n",
        "  train_loss               =     0.0706\n",
        "  train_runtime            = 0:05:10.33\n",
        "  train_samples            =      14041\n",
        "  train_samples_per_second =    135.734\n",
        "  train_steps_per_second   =     16.975\n",
        "07/09/2021 08:05:48 - INFO - __main__ - *** Evaluate ***\n",
        "[INFO|trainer.py:521] 2021-07-09 08:05:48,010 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: chunk_tags, tokens, pos_tags, id, ner_tags.\n",
        "[INFO|trainer.py:2159] 2021-07-09 08:05:48,030 >> ***** Running Evaluation *****\n",
        "[INFO|trainer.py:2161] 2021-07-09 08:05:48,031 >>   Num examples = 3250\n",
        "[INFO|trainer.py:2164] 2021-07-09 08:05:48,031 >>   Batch size = 8\n",
        "100% 407/407 [00:06<00:00, 60.22it/s]\n",
        "***** eval metrics *****\n",
        "  epoch                   =        3.0\n",
        "  eval_LOC_f1             =     0.9592\n",
        "  eval_LOC_number         =       3635\n",
        "  eval_LOC_precision      =     0.9639\n",
        "  eval_LOC_recall         =     0.9546\n",
        "  eval_MISC_f1            =     0.8856\n",
        "  eval_MISC_number        =       1480\n",
        "  eval_MISC_precision     =     0.8896\n",
        "  eval_MISC_recall        =     0.8818\n",
        "  eval_ORG_f1             =     0.9053\n",
        "  eval_ORG_number         =       2702\n",
        "  eval_ORG_precision      =     0.8931\n",
        "  eval_ORG_recall         =     0.9178\n",
        "  eval_PER_f1             =       0.96\n",
        "  eval_PER_number         =       3329\n",
        "  eval_PER_precision      =     0.9584\n",
        "  eval_PER_recall         =     0.9616\n",
        "  eval_loss               =     0.0782\n",
        "  eval_overall_accuracy   =     0.9838\n",
        "  eval_overall_f1         =     0.9365\n",
        "  eval_overall_precision  =     0.9349\n",
        "  eval_overall_recall     =     0.9381\n",
        "  eval_runtime            = 0:00:06.78\n",
        "  eval_samples            =       3250\n",
        "  eval_samples_per_second =    479.029\n",
        "  eval_steps_per_second   =     59.989\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np6obh3fGfGw"
      },
      "source": [
        "### 2. Inference using pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoNgwOCTV6YL",
        "outputId": "84fefe70-93eb-4f66-f590-b10dbc55bc57"
      },
      "source": [
        "!ls tmp/distilbert-base-cased-finetuned-conll03-english/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_results.json  checkpoint-3500  eval_results.json\t    trainer_state.json\n",
            "checkpoint-1000   checkpoint-4000  pytorch_model.bin\t    training_args.bin\n",
            "checkpoint-1500   checkpoint-4500  runs\t\t\t    train_results.json\n",
            "checkpoint-2000   checkpoint-500   special_tokens_map.json  vocab.txt\n",
            "checkpoint-2500   checkpoint-5000  tokenizer_config.json\n",
            "checkpoint-3000   config.json\t   tokenizer.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kg3EH1Iav3v"
      },
      "source": [
        "# Update NER label2id and id2label in model config file\n",
        "\"\"\"\n",
        "  \"id2label\": {\n",
        "    \"0\": \"O\",\n",
        "    \"1\": \"B-PER\",\n",
        "    \"2\": \"I-PER\",\n",
        "    \"3\": \"B-ORG\",\n",
        "    \"4\": \"I-ORG\",\n",
        "    \"5\": \"B-LOC\",\n",
        "    \"6\": \"I-LOC\",\n",
        "    \"7\": \"B-MISC\",\n",
        "    \"8\": \"I-MISC\"\n",
        "  },\n",
        "\n",
        "\n",
        "  \"label2id\": {\n",
        "    \"B-LOC\": 5,\n",
        "    \"B-MISC\": 7,\n",
        "    \"B-ORG\": 3,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-MISC\": 8,\n",
        "    \"I-ORG\": 4,\n",
        "    \"I-PER\": 2,\n",
        "    \"O\": 0\n",
        "  },\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUeIUVJGSMc"
      },
      "source": [
        "from transformers import pipeline\n",
        "distilbert_ner = pipeline('ner', model=\"/tmp/distilbert-base-cased-finetuned-conll03-english/\", tokenizer=\"distilbert-base-cased\", aggregation_strategy='first')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8mDhsXIWol2"
      },
      "source": [
        "article = \"\"\"\n",
        "KUALA LUMPUR - Malaysian Prime Minister Muhyiddin Yassin's party said on Thursday (July 8) that his government would continue to function despite Umno withdrawing its backing. \n",
        "Amid uncertainty over whether Tan Sri Muhyiddin continues to command majority support without Umno, the largest party in the Perikatan Nasional (PN) ruling pact, \n",
        "his Parti Pribumi Bersatu Malaysia said Umno's decision \"had no effect on the workings of government\".\"\"\"\n",
        "\n",
        "\n",
        "results = distilbert_ner(article)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXb3kWwIXMvw",
        "outputId": "fbfa1e28-2662-4fdc-df2e-3c66cdb3266c"
      },
      "source": [
        "print(\"Predicted:\")\n",
        "for tag in results:\n",
        "    print(f\"{tag['entity_group']:<5} as {tag['word']}\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Output\n",
        "------\n",
        "\n",
        "Predicted:\n",
        "ORG   as KUALA LUMPUR\n",
        "MISC  as Malaysian\n",
        "PER   as Muhyiddin Yassin\n",
        "PER   as Umno\n",
        "PER   as Tan Sri Muhyiddin\n",
        "PER   as Umno\n",
        "ORG   as Perikatan Nasional\n",
        "ORG   as PN\n",
        "ORG   as Parti Pribumi\n",
        "PER   as Bersatu\n",
        "ORG   as Malaysia\n",
        "PER   as Umno\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted:\n",
            "ORG   as KUALA LUMPUR\n",
            "MISC  as Malaysian\n",
            "PER   as Muhyiddin Yassin\n",
            "PER   as Umno\n",
            "PER   as Tan Sri Muhyiddin\n",
            "PER   as Umno\n",
            "ORG   as Perikatan Nasional\n",
            "ORG   as PN\n",
            "ORG   as Parti Pribumi\n",
            "PER   as Bersatu\n",
            "ORG   as Malaysia\n",
            "PER   as Umno\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}