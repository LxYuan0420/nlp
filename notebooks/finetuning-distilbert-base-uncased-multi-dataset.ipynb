{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q datasets sentence-transformers accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    SentenceTransformerModelCardData,\n)\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers\nfrom sentence_transformers.evaluation import TripletEvaluator\nfrom sentence_transformers.losses import CoSENTLoss, MultipleNegativesRankingLoss, SoftmaxLoss\n\n\n# 1. Load a model to finetune with 2. (Optional) model card data\nmodel = SentenceTransformer(\n    \"distilbert/distilbert-base-uncased\",\n    model_card_data=SentenceTransformerModelCardData(\n        language=\"en\",\n        license=\"apache-2.0\",\n        model_name=\"Distillbert base uncased model trained on AllNLI triplets\",\n    )\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-13T10:07:18.133603Z","iopub.execute_input":"2024-07-13T10:07:18.133899Z","iopub.status.idle":"2024-07-13T10:07:26.572905Z","shell.execute_reply.started":"2024-07-13T10:07:18.133874Z","shell.execute_reply":"2024-07-13T10:07:26.571912Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-13 10:07:22.246310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-13 10:07:22.246365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-13 10:07:22.247833: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. Load several Datasets to train with\n# (anchor, positive)\nall_nli_pair_train = load_dataset(\"sentence-transformers/all-nli\", \"pair\", split=\"train[:10000]\")\n\n# (sentence1, sentence2) + score\nall_nli_pair_score_train = load_dataset(\"sentence-transformers/all-nli\", \"pair-score\", split=\"train[:10000]\")\n\n# (anchor, positive, negative)\nall_nli_triplet_train = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train[:10000]\")\n\n# (sentence1, sentence2) + score\nstsb_pair_score_train = load_dataset(\"sentence-transformers/stsb\", split=\"train[:10000]\")\n\n# (anchor, positive)\nquora_pair_train = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair\", split=\"train[:10000]\")\n\n# (query, answer)\nnatural_questions_train = load_dataset(\"sentence-transformers/natural-questions\", split=\"train[:10000]\")\n\n# We can combine all datasets into a dictionary with dataset names to datasets\ntrain_dataset = {\n    \"all-nli-pair\": all_nli_pair_train,\n    \"all-nli-pair-score\": all_nli_pair_score_train,\n    \"all-nli-triplet\": all_nli_triplet_train,\n    \"stsb\": stsb_pair_score_train,\n    \"quora\": quora_pair_train,\n    \"natural-questions\": natural_questions_train,\n}\n\n# 3. Load several Datasets to evaluate with\n# (anchor, positive, negative)\nall_nli_triplet_dev = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev\")\n# (sentence1, sentence2, score)\nstsb_pair_score_dev = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n# (anchor, positive)\nquora_pair_dev = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair\", split=\"train[10000:11000]\")\n# (query, answer)\nnatural_questions_dev = load_dataset(\"sentence-transformers/natural-questions\", split=\"train[10000:11000]\")\n\n# We can use a dictionary for the evaluation dataset too, but we don't have to. We could also just use\n# no evaluation dataset, or one dataset.\neval_dataset = {\n    \"all-nli-triplet\": all_nli_triplet_dev,\n    \"stsb\": stsb_pair_score_dev,\n    \"quora\": quora_pair_dev,\n    \"natural-questions\": natural_questions_dev,\n}\n\n# 4. Load several loss functions to train with\n# (anchor, positive), (anchor, positive, negative)\nmnrl_loss = MultipleNegativesRankingLoss(model)\n# (sentence_A, sentence_B) + score\ncosent_loss = CoSENTLoss(model)\n\n# Create a mapping with dataset names to loss functions, so the trainer knows which loss to apply where.\n# Note that you can also just use one loss if all of your training/evaluation datasets use the same loss\nlosses = {\n    \"all-nli-pair\": mnrl_loss,\n    \"all-nli-pair-score\": cosent_loss,\n    \"all-nli-triplet\": mnrl_loss,\n    \"stsb\": cosent_loss,\n    \"quora\": mnrl_loss,\n    \"natural-questions\": mnrl_loss,\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:07:26.574087Z","iopub.execute_input":"2024-07-13T10:07:26.574860Z","iopub.status.idle":"2024-07-13T10:07:43.400877Z","shell.execute_reply.started":"2024-07-13T10:07:26.574822Z","shell.execute_reply":"2024-07-13T10:07:43.400076Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 4. Specifc training arguments\nargs = SentenceTransformerTrainingArguments(\n    # Required parameter:\n    output_dir=\"models/distilbert-base-uncased-all-nli\",\n    # Optional training parameters:\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    auto_find_batch_size=False,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n    # Optional tracking/debugging parameters:\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    logging_strategy=\"epoch\",\n    #run_name=\"distilbert-base-uncased-nli-triplet\",  # Will be used in W&B if `wandb` is installed\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:07:43.401956Z","iopub.execute_input":"2024-07-13T10:07:43.402238Z","iopub.status.idle":"2024-07-13T10:07:43.436957Z","shell.execute_reply.started":"2024-07-13T10:07:43.402214Z","shell.execute_reply":"2024-07-13T10:07:43.436194Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:07:43.438744Z","iopub.execute_input":"2024-07-13T10:07:43.439025Z","iopub.status.idle":"2024-07-13T10:07:44.723014Z","shell.execute_reply.started":"2024-07-13T10:07:43.439001Z","shell.execute_reply":"2024-07-13T10:07:44.722119Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]},{"cell_type":"code","source":"# 6. Create a trainer & train\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=losses,\n    #evaluator=dev_evaluator,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:07:44.724210Z","iopub.execute_input":"2024-07-13T10:07:44.724873Z","iopub.status.idle":"2024-07-13T10:24:24.422590Z","shell.execute_reply.started":"2024-07-13T10:07:44.724846Z","shell.execute_reply":"2024-07-13T10:24:24.421655Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3485' max='3485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3485/3485 16:33, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>All-nli-triplet Loss</th>\n      <th>Stsb Loss</th>\n      <th>Quora Loss</th>\n      <th>Natural-questions Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.787300</td>\n      <td>No log</td>\n      <td>0.775467</td>\n      <td>6.950783</td>\n      <td>0.038007</td>\n      <td>0.058657</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing widget examples:   0%|          | 0/3 [00:00<?, ?example/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3485, training_loss=1.787275824964132, metrics={'train_runtime': 995.4569, 'train_samples_per_second': 56.003, 'train_steps_per_second': 3.501, 'total_flos': 0.0, 'train_loss': 1.787275824964132, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n\n\n# Load the STSB dataset (https://huggingface.co/datasets/sentence-transformers/stsb)\ntest_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n\n# Initialize the evaluator\ntest_evaluator = EmbeddingSimilarityEvaluator(\n    sentences1=test_dataset[\"sentence1\"],\n    sentences2=test_dataset[\"sentence2\"],\n    scores=test_dataset[\"score\"],\n    main_similarity=SimilarityFunction.COSINE,\n    name=\"distilbert-base-uncased-all-nli-sts-test\",\n)\n\ntest_evaluator(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T10:26:59.929439Z","iopub.execute_input":"2024-07-13T10:26:59.929938Z","iopub.status.idle":"2024-07-13T10:27:03.826238Z","shell.execute_reply.started":"2024-07-13T10:26:59.929904Z","shell.execute_reply":"2024-07-13T10:27:03.825392Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'distilbert-base-uncased-all-nli-sts-test_pearson_cosine': 0.7774674252504533,\n 'distilbert-base-uncased-all-nli-sts-test_spearman_cosine': 0.7978656569955651,\n 'distilbert-base-uncased-all-nli-sts-test_pearson_manhattan': 0.8002165724113339,\n 'distilbert-base-uncased-all-nli-sts-test_spearman_manhattan': 0.7983410609487319,\n 'distilbert-base-uncased-all-nli-sts-test_pearson_euclidean': 0.8007164914288766,\n 'distilbert-base-uncased-all-nli-sts-test_spearman_euclidean': 0.7991835974428595,\n 'distilbert-base-uncased-all-nli-sts-test_pearson_dot': 0.4716203511085343,\n 'distilbert-base-uncased-all-nli-sts-test_spearman_dot': 0.4535374154660375,\n 'distilbert-base-uncased-all-nli-sts-test_pearson_max': 0.8007164914288766,\n 'distilbert-base-uncased-all-nli-sts-test_spearman_max': 0.7991835974428595}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}