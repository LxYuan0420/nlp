{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d3d700-80a4-4e69-a548-ca39cb68ce9e",
   "metadata": {},
   "source": [
    "### Stratified Splitting & Hyperparameter Search\n",
    "\n",
    "In this notebook, we aim to demonstrate two advanced machine learning techniques using the Hugging Face Trainer API. Specifically, we'll focus on:\n",
    "\n",
    "- Multilabel Iterative Stratified Splitting: This method is used for more equitable division of imbalanced datasets across multiple labels, making sure that each fold in a k-fold cross-validation retains the same (similar) multilabel distribution as the complete dataset.\n",
    "\n",
    "- Hyperparameter Search: We will walk through how to conduct a systematic hyperparameter search to fine-tune models for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e3355e-cdaa-4a86-a2b3-2a1b526b6fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab640f9f-e39a-4484-ade3-b865ea413010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97850683-313b-4aac-b673-70c15f04aa93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Remove numbers, newlines, and special characters from text.\"\"\"\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Find Single Appearance Labels\n",
    "def find_single_appearance_labels(y):\n",
    "    \"\"\"Find labels that appear only once in the dataset.\"\"\"\n",
    "    all_labels = list(chain.from_iterable(y))\n",
    "    label_count = Counter(all_labels)\n",
    "    single_appearance_labels = [label for label, count in label_count.items() if count == 1]\n",
    "    return single_appearance_labels\n",
    "\n",
    "# Remove Single Appearance Labels from Dataset\n",
    "def remove_single_appearance_labels(dataset, single_appearance_labels):\n",
    "    \"\"\"Remove samples with single-appearance labels from both train and test sets.\"\"\"\n",
    "    for split in ['train', 'test']:\n",
    "        dataset[split] = dataset[split].filter(lambda x: all(label not in single_appearance_labels for label in x['topics']))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e10cde9-d5fc-4a45-847e-4da1818698c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    \n",
    "    y_true = labels\n",
    "    \n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea2626-5e64-449c-bc8b-f27efcad281c",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Note that we are using `ModApte` split in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6e06bc-254b-4ada-beb9-0ac922dcc924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 17.9k/17.9k [00:00<00:00, 24.9MB/s]\n",
      "Downloading readme: 100%|██████████| 16.0k/16.0k [00:00<00:00, 3.76MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\"reuters21578\", \"ModApte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007268b-4fe5-4400-b078-c2724a151c9a",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "- Find out single appearance labels and remove them from train and test split\n",
    "- Combine title and text together as `text` column\n",
    "- Transform topics into multihot encoding as `labels` column\n",
    "- Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db6f5f3-6177-45d9-ada9-bd16674f07a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding single appearance labels...\n",
      "Single appearance labels: ['lin-oil', 'rye', 'red-bean', 'groundnut-oil', 'citruspulp', 'rape-meal', 'corn-oil', 'peseta', 'cotton-oil', 'ringgit', 'castorseed', 'castor-oil', 'lit', 'rupiah', 'skr', 'nkr', 'dkr', 'sun-meal', 'lin-meal', 'cruzado']\n",
      "Removing samples with single-appearance labels...\n"
     ]
    }
   ],
   "source": [
    "# Find and Remove Single Appearance Labels\n",
    "print(\"Finding single appearance labels...\")\n",
    "y_train = [item['topics'] for item in dataset['train']]\n",
    "single_appearance_labels = find_single_appearance_labels(y_train)\n",
    "print(f\"Single appearance labels: {single_appearance_labels}\")\n",
    "\n",
    "print(\"Removing samples with single-appearance labels...\")\n",
    "dataset = remove_single_appearance_labels(dataset, single_appearance_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b386af6-017c-4fdf-8ca5-9ed25736fbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine title and text together\n"
     ]
    }
   ],
   "source": [
    "print(\"Combine title and text together\")\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text\": x[\"title\"] + \" \" + x[\"text\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922bad4d-99c6-4fa3-b308-967b66e37073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 3292\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 9588\n",
       "    })\n",
       "    unused: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 722\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47347011-0140-4d7c-bd66-5f347327e198",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create train/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1577d38-53e3-456f-97ed-eb9a412f02b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X = np.array(dataset[\"train\"][\"text\"]).reshape(-1, 1)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(dataset[\"train\"][\"topics\"])\n",
    "y_sparse = csr_matrix(y)\n",
    "\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split(X, y_sparse, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7c1047-1008-401e-937a-fb692c86e6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = mlb.inverse_transform(y_train)\n",
    "y_val = mlb.inverse_transform(y_val)\n",
    "\n",
    "y_train = [list(tup) for tup in y_train]\n",
    "y_val = [list(tup) for tup in y_val]\n",
    "\n",
    "# Convert to Python list of strings\n",
    "X_train = [item[0] for item in X_train.tolist()]\n",
    "X_val = [item[0] for item in X_val.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b787635-995f-4328-8b2f-8a2b19dea66c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"USX &lt;X> DEBT DOWGRADED BY MOODY'S Moody's Investors Service Inc said it\\nlowered the debt and preferred stock ratings of USX Corp and\\nits units. About seven billion dlrs of securities is affected.\\n    Moody's said Marathon Oil Co's recent establishment of up\\nto one billion dlrs in production payment facilities on its\\nprolific Yates Field has significant negative implications for\\nUSX's unsecured creditors.\\n    The company appears to have positioned its steel segment\\nfor a return to profit by late 1987, Moody's added.\\n    Ratings lowered include those on USX's senior debt to BA-1\\nfrom BAA-3.\\n Reuter\\n\",\n",
       "  'CHAMPION PRODUCTS &lt;CH> APPROVES STOCK SPLIT Champion Products Inc said its\\nboard of directors approved a two-for-one stock split of its\\ncommon shares for shareholders of record as of April 1, 1987.\\n    The company also said its board voted to recommend to\\nshareholders at the annual meeting April 23 an increase in the\\nauthorized capital stock from five mln to 25 mln shares.\\n Reuter\\n'],\n",
       " [[], ['earn']])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2], y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd89e993-7f96-4517-bd82-d020596b8f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": X_train, \"topics\": y_train})\n",
    "val_dataset = Dataset.from_dict({\"text\": X_val, \"topics\": y_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae962a38-7d21-49de-b0c0-3dda05960517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"] = train_dataset\n",
    "dataset[\"validation\"] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ef94dbe-81aa-463f-abd3-3a5b2faf3dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 3292\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'topics'],\n",
       "        num_rows: 4794\n",
       "    })\n",
       "    unused: Dataset({\n",
       "        features: ['text', 'text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title'],\n",
       "        num_rows: 722\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'topics'],\n",
       "        num_rows: 4794\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686113bd-18c7-417f-83da-f0f040cf00e4",
   "metadata": {},
   "source": [
    "### Sanity check on the label ratio in train/val set\n",
    "\n",
    "Looking good, the label splitting is done in a balanced way. There are only three cases where a label appears in the validation set but is missing in the training set. This is negligible because the label count in the validation set is very small and less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28232f56-48c7-4652-881e-55bf35b0615b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_count_train = Counter(list(chain.from_iterable(dataset[\"train\"][\"topics\"])))\n",
    "label_count_validation = Counter(list(chain.from_iterable(dataset[\"validation\"][\"topics\"])))\n",
    "\n",
    "unique_labels = set(label_count_validation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980fa60b-2e43-4c14-a7ab-4fba79d83908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label           -   TRAIN    :    VAL    \n",
      "**************************************************\n",
      "money-fx        -    267     :    266    \n",
      "l-cattle        -     3      :     3     \n",
      "wpi             -     9      :     10    \n",
      "gas             -     19     :     18    \n",
      "wheat           -    105     :    105    \n",
      "trade           -    184     :    185    \n",
      "gnp             -     50     :     50    \n",
      "palladium       -     1      :     1     \n",
      "linseed         -  MISSING   :     1     \n",
      "housing         -     8      :     8     \n",
      "tea             -     5      :     4     \n",
      "wool            -     1      :     1     \n",
      "heat            -     7      :     7     \n",
      "oat             -     4      :     3     \n",
      "nat-gas         -     38     :     37    \n",
      "soybean         -     37     :     37    \n",
      "pork-belly      -     1      :     2     \n",
      "interest        -    173     :    174    \n",
      "jet             -     2      :     2     \n",
      "cpu             -     1      :     2     \n",
      "naphtha         -     1      :     1     \n",
      "cornglutenfeed  -  MISSING   :     1     \n",
      "cocoa           -     27     :     28    \n",
      "plywood         -     2      :     2     \n",
      "propane         -     1      :     2     \n",
      "orange          -     8      :     8     \n",
      "retail          -     12     :     11    \n",
      "cotton          -     20     :     19    \n",
      "groundnut       -     2      :     3     \n",
      "sugar           -     63     :     63    \n",
      "zinc            -     10     :     11    \n",
      "palmkernel      -     1      :     1     \n",
      "tin             -     9      :     9     \n",
      "veg-oil         -     41     :     41    \n",
      "lumber          -     5      :     5     \n",
      "lei             -     6      :     6     \n",
      "grain           -    215     :    215    \n",
      "dmk             -     4      :     4     \n",
      "bop             -     37     :     38    \n",
      "yen             -     23     :     21    \n",
      "money-supply    -     70     :     70    \n",
      "dlr             -     65     :     65    \n",
      "earn            -    1439    :    1438   \n",
      "pet-chem        -     10     :     10    \n",
      "crude           -    194     :    195    \n",
      "nickel          -     4      :     4     \n",
      "soy-meal        -     6      :     5     \n",
      "platinum        -     2      :     3     \n",
      "saudriyal       -     2      :     1     \n",
      "tapioca         -     1      :     1     \n",
      "potato          -     2      :     1     \n",
      "rand            -     1      :     1     \n",
      "stg             -     8      :     8     \n",
      "nzdlr           -  MISSING   :     2     \n",
      "silver          -     10     :     11    \n",
      "rice            -     17     :     18    \n",
      "sun-oil         -     1      :     1     \n",
      "reserves        -     27     :     28    \n",
      "cpi             -     34     :     35    \n",
      "copra-cake      -     1      :     1     \n",
      "copper          -     23     :     24    \n",
      "livestock       -     38     :     37    \n",
      "rape-oil        -     1      :     3     \n",
      "strategic-metal -     8      :     8     \n",
      "corn            -     89     :     89    \n",
      "fishmeal        -     1      :     1     \n",
      "jobs            -     23     :     23    \n",
      "rapeseed        -     8      :     8     \n",
      "hog             -     8      :     8     \n",
      "acq             -    825     :    825    \n",
      "fuel            -     7      :     6     \n",
      "rubber          -     18     :     19    \n",
      "oilseed         -     59     :     59    \n",
      "sunseed         -     4      :     5     \n",
      "ipi             -     20     :     21    \n",
      "instal-debt     -     3      :     2     \n",
      "lead            -     7      :     8     \n",
      "alum            -     18     :     17    \n",
      "can             -     1      :     2     \n",
      "meal-feed       -     14     :     14    \n",
      "coconut         -     2      :     2     \n",
      "palm-oil        -     14     :     15    \n",
      "income          -     5      :     4     \n",
      "iron-steel      -     20     :     20    \n",
      "sorghum         -     11     :     11    \n",
      "ship            -     98     :     99    \n",
      "coconut-oil     -     2      :     2     \n",
      "barley          -     19     :     18    \n",
      "inventories     -     3      :     2     \n",
      "gold            -     47     :     47    \n",
      "austdlr         -     1      :     3     \n",
      "carcass         -     25     :     25    \n",
      "soy-oil         -     6      :     6     \n",
      "coffee          -     56     :     55    \n"
     ]
    }
   ],
   "source": [
    "print(f'{\"label\":<15} - {\"TRAIN \":^10} : {\"VAL\":^10}')\n",
    "print(\"*\"*50)\n",
    "for label in unique_labels:\n",
    "    print(f'{label:<15} - {label_count_train.get(label, \"MISSING\"):^10} : {label_count_validation.get(label, \"MISSING\"):^10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af3b06ad-a4a5-414f-a410-7e033a015eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 92 unique labels:\n",
      "{'money-fx', 'l-cattle', 'wpi', 'gas', 'wheat', 'gnp', 'trade', 'palladium', 'housing', 'tea', 'heat', 'wool', 'oat', 'nat-gas', 'soybean', 'pork-belly', 'interest', 'jet', 'cpu', 'naphtha', 'plywood', 'cocoa', 'propane', 'orange', 'retail', 'cotton', 'groundnut', 'sugar', 'zinc', 'palmkernel', 'tin', 'veg-oil', 'lumber', 'lei', 'grain', 'dmk', 'bop', 'yen', 'money-supply', 'dlr', 'earn', 'pet-chem', 'crude', 'nickel', 'soy-meal', 'platinum', 'saudriyal', 'tapioca', 'potato', 'rand', 'stg', 'dfl', 'silver', 'rice', 'sun-oil', 'reserves', 'cpi', 'copra-cake', 'copper', 'livestock', 'rape-oil', 'strategic-metal', 'corn', 'fishmeal', 'jobs', 'rapeseed', 'hog', 'acq', 'fuel', 'rubber', 'oilseed', 'sunseed', 'ipi', 'instal-debt', 'lead', 'alum', 'can', 'meal-feed', 'coconut', 'palm-oil', 'income', 'iron-steel', 'sorghum', 'ship', 'coconut-oil', 'barley', 'inventories', 'gold', 'austdlr', 'carcass', 'soy-oil', 'coffee'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3292 [00:00<?, ? examples/s]/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['cottonseed', 'f-cattle'] will be ignored\n",
      "  warnings.warn(\n",
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nzdlr', 'sfr'] will be ignored\n",
      "  warnings.warn(\n",
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nzdlr'] will be ignored\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 3292/3292 [00:00<00:00, 54491.90 examples/s]\n",
      "Map: 100%|██████████| 4794/4794 [00:00<00:00, 102420.47 examples/s]\n",
      "Map:   0%|          | 0/722 [00:00<?, ? examples/s]/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['bfr', 'f-cattle', 'hk', 'lit', 'sfr'] will be ignored\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 722/722 [00:00<00:00, 39493.56 examples/s]\n",
      "Map:   0%|          | 0/4794 [00:00<?, ? examples/s]/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['cornglutenfeed'] will be ignored\n",
      "  warnings.warn(\n",
      "/home/likxun/mynotebooks/env38/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['linseed'] will be ignored\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 4794/4794 [00:00<00:00, 104085.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Check number of unique labels \n",
    "unique_labels = set(chain.from_iterable(dataset['train'][\"topics\"]))\n",
    "print(f\"We have {len(unique_labels)} unique labels:\\n{unique_labels}\")\n",
    "\n",
    "# Transform topics into multi-hot encoding format\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(dataset['train']['topics'])\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"labels\": torch.from_numpy(mlb.transform(x[\"topics\"])).float()}, batched=True)\n",
    "\n",
    "labels = mlb.classes_\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "num_labels = len(id2label)\n",
    "\n",
    "assert num_labels == len(unique_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04250da0-b8a9-4a24-bb73-5ebeb6aa6901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: acq\n",
      "1: alum\n",
      "2: austdlr\n",
      "3: barley\n",
      "4: bop\n",
      "5: can\n",
      "6: carcass\n",
      "7: cocoa\n",
      "8: coconut\n",
      "9: coconut-oil\n"
     ]
    }
   ],
   "source": [
    "# sanity check:\n",
    "for idx, label in id2label.items():\n",
    "    if idx>=10:\n",
    "        break\n",
    "    \n",
    "    print(f\"{idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a97865f-6270-4d86-aa89-c0b9b45135b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3292/3292 [00:01<00:00, 3289.53 examples/s]\n",
      "Map: 100%|██████████| 4794/4794 [00:01<00:00, 4302.52 examples/s]\n",
      "Map: 100%|██████████| 722/722 [00:00<00:00, 3308.04 examples/s]\n",
      "Map: 100%|██████████| 4794/4794 [00:01<00:00, 4175.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "# Tokenize and remove unwanted columns\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "columns = dataset[\"train\"].column_names\n",
    "columns.remove(\"text\")\n",
    "columns.remove(\"labels\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbbdebb6-518f-4e44-a821-45c1397c6579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'text_type', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3292\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4794\n",
       "    })\n",
       "    unused: Dataset({\n",
       "        features: ['text', 'text_type', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 722\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4794\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "638074ca-e405-4b39-81a6-3e945407a889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'labels', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "example = tokenized_dataset['train'][1]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15d09099-3c4a-4c7b-8c6e-a3024b27fd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] CHAMPION PRODUCTS & lt ; CH > APPROVES STOCK SPLIT Champion Products Inc said its board of directors approved a two - for - one stock split of its common shares for shareholders of record as of April 1, 1987. The company also said its board voted to recommend to shareholders at the annual meeting April 23 an increase in the authorized capital stock from five mln to 25 mln shares. Reuter [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08a95e1f-9b3d-4be6-be80-d94c1f5c6e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(example['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa77d4b-f64b-4572-b4b2-c9fd065197c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earn']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1f33873-4453-4523-9f6e-cb465ad8c58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32db0a68-4877-4b08-b4f1-a499103f72c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-cased\", \n",
    "        num_labels=num_labels, \n",
    "        problem_type=\"multi_label_classification\",\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae188986-ba6e-4f66-b8ab-69949d8c4f86",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "\n",
    "In this example, we focus on optimizing the learning rate for our machine learning model. Using Optuna for hyperparameter optimization, we will search for the best learning rate in the range of 2e-5 to 5e-5. The goal is to identify the learning rate that yields the best model performance within this specified range. For demonstration purposes, we limit the training epochs to 5 and set n_trial to 2 as well. Feel free to increase these numbers to a larger range for better results. Additionally, you are encouraged to experiment with other parameters to fine-tune the model further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ece0e85-15e1-4295-a24b-1771c6579698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 5e-5, log=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18d525c6-5265-416f-a302-9cdf20880e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"hyperparameter-search-distilbert-reuters21578\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3afa72a4-758c-4e0e-a05b-352451cbc077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7319a47-f6c3-4f28-a5f5-5f017f092c75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-08 19:56:18,700] A new study created in memory with name: no-name-b48b9cc9-5c4c-4b5b-b438-a4e3b69b0729\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 09:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.132036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.189612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.055436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.189612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.045973</td>\n",
       "      <td>0.061551</td>\n",
       "      <td>0.515876</td>\n",
       "      <td>0.221318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.042393</td>\n",
       "      <td>0.329145</td>\n",
       "      <td>0.598496</td>\n",
       "      <td>0.386316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.041176</td>\n",
       "      <td>0.341244</td>\n",
       "      <td>0.602882</td>\n",
       "      <td>0.395077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-08 20:06:17,398] Trial 0 finished with value: 0.34124372076909754 and parameters: {'learning_rate': 3.262125845083782e-05}. Best is trial 0 with value: 0.34124372076909754.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 09:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.327400</td>\n",
       "      <td>0.102568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.189612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.049549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.189612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.042749</td>\n",
       "      <td>0.300976</td>\n",
       "      <td>0.588573</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.039329</td>\n",
       "      <td>0.367416</td>\n",
       "      <td>0.612593</td>\n",
       "      <td>0.414059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.038065</td>\n",
       "      <td>0.375805</td>\n",
       "      <td>0.615828</td>\n",
       "      <td>0.420108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-08 20:16:15,006] Trial 1 finished with value: 0.37580481192815995 and parameters: {'learning_rate': 3.77934555883044e-05}. Best is trial 1 with value: 0.37580481192815995.\n"
     ]
    }
   ],
   "source": [
    "# Optuna hyperparameter search\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    compute_objective=lambda x: x[\"eval_f1\"],\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c309f8d5-5936-4321-ab99-f10dd5a0b9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='1', objective=0.37580481192815995, hyperparameters={'learning_rate': 3.77934555883044e-05}, run_summary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
