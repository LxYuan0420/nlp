# -*- coding: utf-8 -*-
"""AG News Classification using DistilBERT Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12HihnIx6a6I3C7__ikEfTq5iemA9zh98

### 0. Install and load library
"""

#Restart kernel after installation: Runtime -> Restart runtime

#!pip install -U transformers sentencepiece datasets

from datasets import load_dataset
from datasets import DatasetDict
from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification, pipeline
import tensorflow as tf
from tqdm import tqdm

"""### 1. Load dataset"""

datasets = load_dataset("ag_news")

"""### 2. Split dataset"""

datasets.keys() #dict_keys(['train', 'test'])

# split original train set into train/val with 8/2 ratio
# gather together to process splits at once
train_valid = datasets['train'].train_test_split(test_size=.2)
train_valid_test_datasets = DatasetDict({
    'train': train_valid['train'],
    'valid': train_valid['test'],
    'test': datasets['test']
})

# ['World', 'Sports', 'Business', 'Sci/Tech']
dataset_labels = datasets['train'].features['label'].names
id2label = {id: label for id, label in enumerate(dataset_labels)}
label2id = {label: id for id, label in id2label.items()}

"""### 3. Load model and tokenizer with correct configs """

model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=len(dataset_labels), label2id=label2id, id2label=id2label)
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

"""### 4. Tokenize and convert dataset to DataLoader"""

encoded_datasets = train_valid_test_datasets.map(lambda examples: tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True)
encoded_datasets = encoded_datasets.map(lambda examples: {'labels': examples['label']}, batched=True)
encoded_datasets.set_format(type='tensorflow', columns=['input_ids', 'attention_mask', 'labels'])

train_features = {x: encoded_datasets['train'][x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids', 'attention_mask']}
val_features = {x: encoded_datasets['valid'][x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids', 'attention_mask']}
test_features = {x: encoded_datasets['test'][x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids','attention_mask']}

train_tfdataset = tf.data.Dataset.from_tensor_slices((train_features, encoded_datasets['train']['labels'])).batch(8).prefetch(tf.data.AUTOTUNE)
val_tfdataset = tf.data.Dataset.from_tensor_slices((val_features, encoded_datasets['valid']['labels'])).batch(8).prefetch(tf.data.AUTOTUNE)
test_tfdataset = tf.data.Dataset.from_tensor_slices((test_features, encoded_datasets['test']['labels'])).batch(8).prefetch(tf.data.AUTOTUNE)

"""### 5. Prepare training arguments"""

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
metrics = ['accuracy']
epochs = 3

"""### 6. Training"""

model.compile(loss=loss, optimizer=optimizer, metrics=metrics)

history = model.fit(train_tfdataset,
                    epochs=epochs,
                    verbose=1,
                    validation_data=val_tfdataset
                    )

"""### 7. Predicts on Test set"""

from sklearn.metrics import classification_report
import numpy as np

y_pred = model.predict(test_tfdataset, verbose=1)
y_pred = np.argmax(y_pred.logits, axis=1)

y_true = []
for element in test_samples.as_numpy_iterator(): 
  y_true.append(element[1])
y_true = np.concatenate(y_true)


print(classification_report(y_true, y_pred))

"""### 8. Inference using pipeline"""

ag_news_pipeline = pipeline('text-classification', model=model, tokenizer=tokenizer)

"""
datasets['test'][0]

>>> {
    'label': 2,
    'text': "Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul."
    }

"""

test_input = datasets['test'][0]['text']
outputs = ag_news_pipeline(test_input)
print(outputs)

"""
>>> [{'label': 'LABEL_2', 'score': 0.9857332110404968}]
"""